<!DOCTYPE html><HTML lang="en"><head><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Exercises · Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><script data-outdated-warner="">function maybeAddWarning () {
    const head = document.getElementsByTagName('head')[0];

    // Add a noindex meta tag (unless one exists) so that search engines don't index this version of the docs.
    if (document.body.querySelector('meta[name="robots"]') === null) {
        const meta = document.createElement('meta');
        meta.name = 'robots';
        meta.content = 'noindex';

        head.appendChild(meta);
    };

    // Add a stylesheet to avoid inline styling
    const style = document.createElement('style');
    style.type = 'text/css';
    style.appendChild(document.createTextNode('.outdated-warning-overlay {  position: fixed;  top: 0;  left: 0;  right: 0;  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);  z-index: 999;  background-color: #ffaba7;  color: rgba(0, 0, 0, 0.7);  border-bottom: 3px solid #da0b00;  padding: 10px 35px;  text-align: center;  font-size: 15px; }  .outdated-warning-overlay .outdated-warning-closer {    position: absolute;    top: calc(50% - 10px);    right: 18px;    cursor: pointer;    width: 12px; }  .outdated-warning-overlay a {    color: #2e63b8; }    .outdated-warning-overlay a:hover {      color: #363636; }'));
    head.appendChild(style);

    const div = document.createElement('div');
    div.classList.add('outdated-warning-overlay');
    const closer = document.createElement('div');
    closer.classList.add('outdated-warning-closer');

    // Icon by font-awesome (license: https://fontawesome.com/license, link: https://fontawesome.com/icons/times?style=solid)
    closer.innerHTML = '<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>';
    closer.addEventListener('click', function () {
        document.body.removeChild(div);
    });
    let href = '/stable';
    if (window.documenterBaseURL) {
        href = window.documenterBaseURL + '/../stable';
    }
    div.innerHTML = 'This documentation is not for the latest version. <br> <a href="' + href + '">Go to the latest documentation</a>.';
    div.appendChild(closer);
    document.body.appendChild(div);
};

if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', maybeAddWarning);
} else {
    maybeAddWarning();
};
</script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="Julia for Machine Learning logo" class="docs-light-only" src="../../assets/logo.svg"/><img alt="Julia for Machine Learning logo" class="docs-dark-only" src="../../assets/logo-dark.svg"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input checked="" class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../constrained/">Constrained optimization</a></li><li class="is-active"><a class="tocitem" href="">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/iris/">Introduction to Flux</a></li><li><a class="tocitem" href="../../lecture_10/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Statistics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">7: Optimization</a></li><li class="is-active"><a href="">Exercises</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Exercises</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_07/exercises.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="l7-exercises"><a class="docs-heading-anchor" href="#l7-exercises">Exercises</a><a id="l7-exercises-1"></a><a class="docs-heading-anchor-permalink" href="#l7-exercises" title="Permalink"></a></h1><div class="homework-body"><header class="homework-header">Homework: Newton's method</header><p></p><p>Newton's method for solving equation <span>$g(x)=0$</span> is an iterative procedure which at every iteration <span>$x^k$</span> approximates the function <span>$g(x)$</span> by its first-order (linear) expansion <span>$g(x) \approx g(x^k) + \nabla g(x^k)(x-x^k)$</span> and finds the zero point of this approximation.</p><p>Newton's method for unconstrained optimization replaces the optimization problem by its optimality condition and solves the resulting equation.</p><p>Implement Newton's method to minimize</p><p class="math-container">\[f(x) = e^{x_1^2 + x_2^2 - 1} + (x_1-1)^2\]</p><p>with the starting point <span>$x^0=(0,0)$</span>.</p><p></p></div><div class="exercise-body"><header class="exercise-header">Exercise 1: Solving a system of linear equations</header><p></p><p>The update of Newton's method computes <span>$A^{-1}b$</span>. The most intuitive way of writing this is to use <code>inv(A) * b</code>, which first computes the inverse of <code>A</code> and then multiplies it with a vector. However, this approach has several disadvantages:</p><ul><li>Specialized algorithms for solving the linear system <span>$Ax=b$</span> cannot be used.</li><li>When <code>A</code> is sparse, this inverse is dense and additional memory is needed to store the dense matrix.</li></ul><p>For these reasons, the linear system of equations is solved by <code>A \ b</code>, which calls specialized algorithms.</p><p>Use the package <code>BenchmarkTools</code> to benchmark both possibilities.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>We first create a random matrix <code>A</code> and a random vector <code>b</code>.</p><pre><code class="language-julia">using BenchmarkTools

n = 1000
A = randn(n,n)
b = randn(n)</code></pre><p>We first verify that both possibilities result in the same number.</p><pre><code class="language-julia">using LinearAlgebra

norm(inv(A)*b - A \ b)</code></pre><pre><code class="language-julia">9.321906736594836e-12</code></pre><p>We benchmark the first possibility.</p><pre><code class="language-julia">@btime inv($A)*($b)</code></pre><pre><code class="language-julia">  71.855 ms (6 allocations: 8.13 MiB)</code></pre><p>We benchmark the second possibility.</p><pre><code class="language-julia">@btime ($A) \ ($b)</code></pre><pre><code class="language-julia">  31.126 ms (4 allocations: 7.64 MiB)</code></pre><p>The second possibility is faster and has lower memory requirements.</p><p></p></details><div class="exercise-body"><header class="exercise-header">Exercise 2: Bisection method</header><p></p><p>Similarly to Newton's method, the bisection method is primarily designed to solve equations by finding their zero points. It is only able to solve equations <span>$f(x)=0$</span> where <span>$f:\mathbb{R}\to\mathbb{R}$</span>. It starts with an interval <span>$[a,b]$</span> where <span>$f$</span> has opposite values <span>$f(a)f(b)&lt;0$</span>. Then it selects the middle point on <span>$[a,b]$</span> and halves the interval so that the new interval again satisfies the constraint on opposite signs <span>$f(a)f(b)&lt;0$</span>. This is repeated until the function value is small or until the interval has a small length.</p><p>Implement the bisection method and use it to minimize <span>$f(x) = x^2 - x$</span> on <span>$[-1,1]$</span>. During the implementation, do not evaluate <span>$f$</span> unless necessary.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>First, we write the bisection method. We initialize it with arguments <span>$f$</span> and the initial interval <span>$[a,b]$</span>. We also specify the optional tolerance. First, we save the function value <code>fa = f(a)</code> to not need to recompute it every time. The syntax <code>fa == 0 &amp;&amp; return a</code> is a bit complex. Since <code>&amp;&amp;</code> is the "and" operator, this first checks whether <code>fa == 0</code> is satisfied, and if so, it evaluates the second part. However, the second part exits the function and returns <code>a</code>. Since we need to have <span>$f(a)f(b)&lt;0$</span>, we check this condition, and if it is not satisfied, we return an error message. Finally, we run the while loop, where every iteration halves the interval. The condition on opposite signs is enforced in the if condition inside the loop.</p><pre><code class="language-julia">function bisection(f, a, b; tol=1e-6)
    fa = f(a)
    fb = f(b)
    fa == 0 &amp;&amp; return a
    fb == 0 &amp;&amp; return b
    fa*fb &gt; 0 &amp;&amp; error("Wrong initial values for bisection")
    while b-a &gt; tol
        c = (a+b)/2
        fc = f(c)
        fc == 0 &amp;&amp; return c
        if fa*fc &gt; 0
            a = c
            fa = fc
        else
            b = c
            fb = fc
        end
    end
    return (a+b)/2
end</code></pre><p>This implementation is efficient in the way that only one function evaluation is needed per iteration. The price to pay are additional variables <code>fa</code>, <code>fb</code> and <code>fc</code>.</p><p>To use the bisection method to minimize a function <span>$f(x)$</span>, we use it find the solution of the optimality condition <span>$f'(x)=0$</span>.</p><pre><code class="language-julia">f(x) = x^2 - x
g(x) = 2*x - 1
x_opt = bisection(g, -1, 1)</code></pre><p></p></details><p>The correct solution is</p><pre class="documenter-example-output">0.5</pre><div class="exercise-body"><header class="exercise-header">Exercise 3: JuMP</header><p></p><p>The library to perform optimization is called <code>JuMP</code>. Install it, go briefly through its documentation, and use it to solve the linear optimization problem</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;x_1 + x_2 + x_5 \\
\text{subject to}\qquad &amp;x_1+2x_2+3x_3+4x_4+5x_5 = 8, \\
&amp;x_3+x_4+x_5 = 2, \\
&amp;x_1+x_2 = 2.
\end{aligned}\]</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>The best start is the official documentation of the <a href="https://jump.dev/JuMP.jl/stable/quickstart/">JuMP package</a>. Since <code>JuMP</code> is only an interface for solvers, we need to include an actual solver as well. For linear programs, we can use <code>using GLPK</code>, for non-linear ones, we would need to use <code>using Ipopt</code>. We specify the constraints in a matrix form. It is possible to write them directly via <code>@constraint(model, x[1] + x[2] == 2)</code>. This second way is more pleasant for complex constraints. Since <code>x</code> is a vector, we need to use <code>value.(x)</code> instead of the wrong <code>value(x)</code>.</p><pre><code class="language-julia">using JuMP
using GLPK

A = [1 2 3 4 5; 0 0 1 1 1; 1 1 0 0 0]
b = [8; 2; 2]
c = [1; 1; 0; 0; 1]
n = size(A, 2)

model = Model(GLPK.Optimizer)

@variable(model, x[1:n] &gt;= 0)

@objective(model, Min, c'*x)
@constraint(model, A*x .== b)
optimize!(model)

x_val = value.(x)</code></pre><p></p></details><p>The correct solution is</p><pre class="documenter-example-output">[2.0, 0.0, 2.0, 0.0, 0.0]</pre><div class="exercise-body"><header class="exercise-header">Exercise 4: SQP method</header><p></p><p>Derive the SQP method for optimization problem with only equality constraints</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;f(x) \\
\text{subject to}\qquad &amp;h_j(x) = 0, j=1,\dots,J.
\end{aligned}\]</p><p>SQP writes the <a href="../constrained/#lagrangian">Karush-Kuhn-Tucker</a> optimality conditions and then applies Newton's method to solve the resulting system of equations. </p><p>Apply the obtained algorithm to</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;\sum_{i=1}^{10} ix_i^4 \\
\text{subject to}\qquad &amp;\sum_{i=1}^{10} x_i = 1.
\end{aligned}\]</p><p>Verify that the numerically obtained solution is correct.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>The Lagrangian reads</p><p class="math-container">\[L(x,\mu) = f(x) + \sum_{j=1}^J\mu_j h_j(x).\]</p><p>Since there are no inequality constraints, the optimality conditions contain no complementarity and read</p><p class="math-container">\[\begin{aligned}
\nabla f(x) + \sum_{j=1}^J\mu_j \nabla h_j(x) &amp;= 0,\\
h_j(x) &amp;= 0,
\end{aligned}\]</p><p>The Newton method's at iteration <span>$k$</span> has some pair <span>$(x^k,\mu^k)$</span> and performs the update</p><p class="math-container">\[\begin{pmatrix} x^{k+1} \\ \mu^{k+1} \end{pmatrix} = \begin{pmatrix} x^{k} \\ \mu^{k} \end{pmatrix} - \begin{pmatrix} \nabla^2 f(x^k) + \sum_{j=1}^J \mu_j^k \nabla^2 h_j(x^k) &amp; \nabla h(x^k) \\ \nabla h(x^k)^\top &amp; 0 \end{pmatrix}^{-1} \begin{pmatrix} \nabla f(x^k) + \sum_{j=1}^J\mu_j^k \nabla h_j(x^k) \\ h(x^k) \end{pmatrix}. \]</p><p>We define functions <span>$f$</span> and <span>$h$</span> and their derivates and Hessians for the numerical implementation. The simplest way to create a diagonal matrix is <code>Diagonal</code> from the <code>LinearAlgebra</code> package. It can be, of course, done manually as well. </p><pre><code class="language-julia">using LinearAlgebra

n = 10
f(x) = sum((1:n) .* x.^4)
f_grad(x) = 4*(1:n).*x.^3
f_hess(x) = 12*Diagonal((1:n).*x.^2)
h(x) = sum(x) - 1
h_grad(x) = ones(n)
h_hess(x) = zeros(n,n)</code></pre><p>To implement SQP, we first randomly generate initial <span>$x$</span> and <span>$\mu$</span> and then write the procedure derived above. Since we update <span>$x$</span> in a for loop, we need to define it as a <code>global</code> variables; otherwise, it will be a local variable, and the global (outside of the loop) will not update. We can write <code>inv(A)*b</code> or the more efficient <code>A\b</code>. To subtract from <span>$x$</span>, we use the shortened notation <code>x -= ?</code>, which is the same as <code>x = x - ?</code>.</p><pre><code class="language-julia">x = randn(n)
μ = randn()
for i in 1:100
    global x, μ
    A = [f_hess(x) + μ*h_hess(x) h_grad(x); h_grad(x)' 0]
    b = [f_grad(x) + μ*h_grad(x); h(x)]
    step = A \ b
    x -= step[1:n]
    μ -= step[n+1]
end</code></pre><p>The need to differentiate global and local variables in scripts is one reason why functions should be used as much as possible.</p><p>To validate, we need to verify the optimality and the feasibility; both need to equal zero. These are the same as the <code>b</code> variable. However, we cannot call <code>b</code> directly, as it is inside the for loop and therefore local only.</p><pre><code class="language-julia-repl">julia&gt; f_grad(x) + μ*h_grad(x)
10-element Array{Float64,1}:
  0.0
  0.0
  3.469446951953614e-18
 -3.469446951953614e-18
  6.938893903907228e-18
  3.469446951953614e-18
  0.0
  0.0
 -3.469446951953614e-18
  0.0

julia&gt; h(x)
0.0</code></pre><p></p></details><p>The correct solution is</p><pre class="documenter-example-output">[0.1608, 0.1276, 0.1115, 0.1013, 0.094, 0.0885, 0.084, 0.0804, 0.0773, 0.0746]</pre><div class="exercise-body"><header class="exercise-header">Exercise 5 (theory)</header><p></p><p>Show that the primal formulation for a problem with no inequalities is equivalent to the min-max formulation.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>The primal problem with no inequalities reads</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;f(x) \\
\text{subject to}\qquad &amp;h_j(x) = 0,\ j=1,\dots,J.
\end{aligned}\]</p><p>The Lagrangian has form</p><p class="math-container">\[L(x;\lambda,\mu) = f(x) + \sum_{j=1}^J \mu_j h_j(x).\]</p><p>Now consider the min-max formulation</p><p class="math-container">\[\operatorname*{minimize}_x\quad \operatorname*{maximize}_{\mu}\quad f(x) + \sum_{j=1}^J \mu_j h_j(x).\]</p><p>If <span>$h_j(x)\neq 0$</span>, then it is simple to choose <span>$\mu_j$</span>so that the inner maximization problem has the optimal value <span>$+\infty$</span>. However, since the outer problem minimizes the objective, the value of <span>$+\infty$</span> is irrelevant. Therefore, we can ignore all points with <span>$h_j(x)\neq 0$</span> and prescribe <span>$h_j(x)=0$</span> as a hard constraint. That is precisely the primal formulation.</p><p></p></details><div class="exercise-body"><header class="exercise-header">Exercise 6 (theory)</header><p></p><p>Derive the dual formulation for the linear programming.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>The linear program</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;c^\top x \\
\text{subject to}\qquad &amp;Ax=b, \\
&amp;x\ge 0
\end{aligned}\]</p><p>has the Lagrangian</p><p class="math-container">\[L(x;\lambda,\mu) = c^\top x - \lambda^\top x + \mu^\top (b-Ax) = (c - \lambda - A^\top\mu)^\top x + b^\top \mu.\]</p><p>We need to have <span>$- \lambda^\top x$</span> because we require constraints <span>$g(x)\le 0$</span> or in other words <span>$-x\le 0$</span>. The dual problem from its definition reads</p><p class="math-container">\[\operatorname*{maximize}_{\lambda\ge0, \mu} \quad \operatorname*{minimize}_x \quad (c - \lambda - A^\top\mu)^\top x + b^\top \mu.\]</p><p>Since the minimization with respect to <span>$x$</span> is unconstrained, the same arguments as the previous exercise imply the hard constraint <span>$c - \lambda - A^\top\mu=0$</span>. Then we may simplify the dual problem into</p><p class="math-container">\[\begin{aligned}
\text{maximize}\qquad &amp;b^\top \mu \\
\text{subject to}\qquad &amp;c - \lambda - A^\top\mu = 0, \\
&amp;\lambda\ge 0.
\end{aligned}\]</p><p>From this formulation, we may remove <span>$\lambda$</span> and obtain <span>$A^\top \mu\le c$</span>. This is the desired dual formulation.</p><p></p></details></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../constrained/">« Constrained optimization</a><a class="docs-footer-nextpage" href="../../lecture_08/theory/">Introduction to regression and classification »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 26 April 2021 20:33">Monday 26 April 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>