<!DOCTYPE html><HTML lang="en"><head><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Neural networks Â· Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><script data-outdated-warner="">function maybeAddWarning () {
    const head = document.getElementsByTagName('head')[0];

    // Add a noindex meta tag (unless one exists) so that search engines don't index this version of the docs.
    if (document.body.querySelector('meta[name="robots"]') === null) {
        const meta = document.createElement('meta');
        meta.name = 'robots';
        meta.content = 'noindex';

        head.appendChild(meta);
    };

    // Add a stylesheet to avoid inline styling
    const style = document.createElement('style');
    style.type = 'text/css';
    style.appendChild(document.createTextNode('.outdated-warning-overlay {  position: fixed;  top: 0;  left: 0;  right: 0;  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);  z-index: 999;  background-color: #ffaba7;  color: rgba(0, 0, 0, 0.7);  border-bottom: 3px solid #da0b00;  padding: 10px 35px;  text-align: center;  font-size: 15px; }  .outdated-warning-overlay .outdated-warning-closer {    position: absolute;    top: calc(50% - 10px);    right: 18px;    cursor: pointer;    width: 12px; }  .outdated-warning-overlay a {    color: #2e63b8; }    .outdated-warning-overlay a:hover {      color: #363636; }'));
    head.appendChild(style);

    const div = document.createElement('div');
    div.classList.add('outdated-warning-overlay');
    const closer = document.createElement('div');
    closer.classList.add('outdated-warning-closer');

    // Icon by font-awesome (license: https://fontawesome.com/license, link: https://fontawesome.com/icons/times?style=solid)
    closer.innerHTML = '<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>';
    closer.addEventListener('click', function () {
        document.body.removeChild(div);
    });
    let href = '/stable';
    if (window.documenterBaseURL) {
        href = window.documenterBaseURL + '/../stable';
    }
    div.innerHTML = 'This documentation is not for the latest version. <br> <a href="' + href + '">Go to the latest documentation</a>.';
    div.appendChild(closer);
    document.body.appendChild(div);
};

if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', maybeAddWarning);
} else {
    maybeAddWarning();
};
</script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="Julia for Machine Learning logo" class="docs-light-only" src="../../assets/logo.svg"/><img alt="Julia for Machine Learning logo" class="docs-dark-only" src="../../assets/logo-dark.svg"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input checked="" class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li class="is-active"><a class="tocitem" href="">Neural networks</a><ul class="internal"><li><a class="tocitem" href="#Prepare-data"><span>Prepare data</span></a></li><li><a class="tocitem" href="#Create-the-network"><span>Create the network</span></a></li><li><a class="tocitem" href="#Train-the-network"><span>Train the network</span></a></li><li><a class="tocitem" href="#Prediction"><span>Prediction</span></a></li></ul></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/iris/">Introduction to Flux</a></li><li><a class="tocitem" href="../../lecture_10/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../../lecture_11/glm/">Linear regression revisited</a></li><li><a class="tocitem" href="../../lecture_11/sparse/">Linear regression with sparse constraints</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">9: Neural networks I.</a></li><li class="is-active"><a href="">Neural networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Neural networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_09/nn.md" title="Edit on GitHub"><span class="docs-icon fab">ï</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-networks"><a class="docs-heading-anchor" href="#Neural-networks">Neural networks</a><a id="Neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-networks" title="Permalink"></a></h1><p>During this lecture, we will train a better classifier for the iris dataset. From the previous lecture, it will differ in several points:</p><ul><li>It will use a neural network instead of the linear classifier.</li><li>It will use all features and not only two.</li><li>It will use all classes and not only two.</li></ul><h2 id="Prepare-data"><a class="docs-heading-anchor" href="#Prepare-data">Prepare data</a><a id="Prepare-data-1"></a><a class="docs-heading-anchor-permalink" href="#Prepare-data" title="Permalink"></a></h2><p>We start by loading the iris dataset in the same way as in the last lecture.</p><pre><code class="language-julia">using RDatasets

iris = dataset("datasets", "iris")

X = Matrix(iris[:, 1:4])
y = iris.Species</code></pre><p>The first exercise splits the dataset into the training and testing sets. Recall that the training set is used to train the classifier, while its performance is evaluated on the testing set. Since the classifier does not see the testing set samples during training, the same performance on the training and testing sets indicates no overfitting.</p><div class="exercise-body"><header class="exercise-header">Exercise:</header><p></p><p>Write the <code>split</code> function, which randomly splits the dataset and the labels into training and testing sets. Its input should be the dataset <code>X</code> and the labels <code>y</code>. It should have four outputs. Include 80% of data in the training set and 20% of data in the testing set by default.</p><p><strong>Hint</strong>: use the <code>randperm</code> function from the <code>Random</code> package.</p><p><strong>Hint</strong>: while <code>y</code> can be assumed to a vector, <code>X</code> is a matrix or a more-dimensional array. Then it is beneficial to use the <code>selectdim</code> function to select subindices along the correct dimension.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>The function <code>split</code> has two required arguments and two optional arguments. The first optional argument is the dimension <code>dims</code> along which the split is done. The second optional argument is the fraction of the training set. We first check whether the inputs have the same sizes along the correct dimension. Then we determine the number of samples <code>n_train</code> in the training set, create a random permutation <code>i_rand</code> and select the correct number of indices. Finally, we return the data and labels in the training and testing sets.</p><pre><code class="language-julia">using Random

function split(X, y::AbstractVector; dims=1, ratio_train=0.8, kwargs...)
    n = length(y)
    size(X, dims) == n || throw(DimensionMismatch("..."))

    n_train = round(Int, ratio_train*n)
    i_rand = randperm(n)
    i_train = i_rand[1:n_train]
    i_test = i_rand[n_train+1:end]

    return selectdim(X, dims, i_train), y[i_train], selectdim(X, dims, i_test), y[i_test]
end</code></pre><p>We can verify its functionality by calling this function.</p><pre><code class="language-julia">X_train, y_train, X_test, y_test = split(X, y)</code></pre><p></p></details><p>The following exercise normalizes the data. In the previous lecture, we have already normalized the training set. We compute the normalizing constants (mean and standard deviation) for each feature and then apply them to the data. Since the normalization needs to be done before training, and since the testing set is not available during training, the normalizing constants can be computed only from the training set. This also means that the features on the training set have zero mean and unit variance, but features on the testing set may have different mean and variance.</p><div class="exercise-body"><header class="exercise-header">Exercise:</header><p></p><p>Write the <code>normalize</code> functions as described above. It should have two inputs and two outputs. The keyword argument <code>dims</code> should also be included.</p><p><strong>Hint</strong>: check the help for the <code>mean</code> function.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>To compute the mean of <code>X</code> along dimension <code>dims</code>, we can check the help for the <code>mean</code> function to realize that the correct command is <code>mean(X; dims)</code>. This is equivalent to <code>mean(X; dims=dims)</code>. We do the same for the standard deviation. To normalize, we need to subtract the mean and divide by the standard deviation. Since <code>col_means</code> has the same number of dimensions as <code>X_train</code>, we can use <code>X_train .- col_mean</code> to broadcast <code>col_mean</code> along the dimension mean was computed. We need to use the same normalizing constant for the training and testing sets due to the reasons mentioned above.</p><pre><code class="language-julia">using Statistics

function normalize(X_train, X_test; dims=1, kwargs...)
    col_mean = mean(X_train; dims)
    col_std = std(X_train; dims)

    return (X_train .- col_mean) ./ col_std, (X_test .- col_mean) ./ col_std
end</code></pre><p>To obtain the normalized datasets, we run the <code>normalize</code> function.</p><pre><code class="language-julia">X_train, X_test = normalize(X_train, X_test)</code></pre><p></p></details><p>The following exercise modifies the labels into a standard form for machine learning.</p><div class="exercise-body"><header class="exercise-header">Exercise:</header><p></p><p>Write the <code>onehot</code> function that converts the labels <code>y</code> into their one-hot representation. The samples should be along the second dimension. Write the <code>onecold</code> function that converts the one-hot representation into the one-cold (original) representation. Both these functions need to have two arguments; the second one is <code>classes</code>, which equals <code>unique(y)</code>.</p><p>Write a check that both functions work correctly.</p><p><strong>Hint</strong>: the one-hot representation for a label has the size equalling to the number of classes. All entries besides one are zeros.</p><p><strong>Hint</strong>: since the one-hot representation represents probabilities, the prediction is the class with the highest probability.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>The <code>onehot</code> function first creates an array <code>y_onehot</code>, where the first dimension is the number of classes, and the second dimension the number of samples. Since all but one entries of each column will be zeros, we initialize it by zeros. Then we run a for loop to fill one into each column. We perform the for loop over all classes, but it is also possible to perform it over all columns.</p><pre><code class="language-julia">function onehot(y, classes)
    y_onehot = falses(length(classes), length(y))
    for (i, class) in enumerate(classes)
        y_onehot[i, y .== class] .= 1
    end
    return y_onehot
end</code></pre><p>The <code>onecold</code> function finds the index of its maximum value. We repeat this for every column  <code>y_col</code>.</p><pre><code class="language-julia">onecold(y, classes) = [classes[argmax(y_col)] for y_col in eachcol(y)]</code></pre><p>Functions <code>onehot</code> and <code>onecold</code> should be inverse to each other. That means that if we call them in succession, we obtain the original input.</p><pre><code class="language-julia">classes = unique(y)

isequal(onecold(onehot(y, classes), classes), y)</code></pre><pre class="documenter-example-output">true</pre><p></p></details><p>Preparing the data is spread over many lines. It is better to combine them into the function <code>prepare_data</code>.</p><pre><code class="language-julia">function prepare_data(X, y; do_normal=true, do_onehot=true, kwargs...)
    X_train, y_train, X_test, y_test = split(X, y; kwargs...)

    if do_normal
        X_train, X_test = normalize(X_train, X_test; kwargs...)
    end

    classes = unique(y)

    if do_onehot
        y_train = onehot(y_train, classes)
        y_test = onehot(y_test, classes)
    end

    return X_train, y_train, X_test, y_test, classes
end</code></pre><p>The standard data representation in linear and logistic regression is that each row (first dimension) is one sample. However, neural networks work with more-dimensional data (three dimensions represent each image). The convention changed, and the last dimension represents the samples. For this reason, we need to transpose the matrix <code>X</code> and use the keyword argument <code>dims=2</code> to split the dataset along the second dimension. Then the whole procedure for data preprocessing can be summarized in a few lines of code. We specify the seed to obtain the same train-test split.</p><pre><code class="language-julia">Random.seed!(666)

iris = dataset("datasets", "iris")

X = Matrix(iris[:, 1:4])
y = iris.Species

X_train, y_train, X_test, y_test, classes = prepare_data(X', y; dims=2)</code></pre><p>Writing function <code>prepare_data</code> as above has other advantages; we will show them in the exercises. The following example shows that specifying the dimension to split along works as intended.</p><pre><code class="language-julia">using LinearAlgebra

Random.seed!(666)
aux1 = prepare_data(X, y; dims=1)
Random.seed!(666)
aux2 = prepare_data(X', y; dims=2)

norm(aux1[1] - aux2[1]')</code></pre><pre class="documenter-example-output">1.6368977293341684e-14</pre><h2 id="Create-the-network"><a class="docs-heading-anchor" href="#Create-the-network">Create the network</a><a id="Create-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Create-the-network" title="Permalink"></a></h2><p>We will now construct a simple neural network <code>SimpleNet</code> with the following three layers:</p><ul><li>The first layer is a dense layer with the ReLU activation function.</li><li>The second layer is a dense layer with the identity activation function.</li><li>The third layer is the softmax.</li></ul><p>Its parameters will be stored in the following structure.</p><pre><code class="language-julia">struct SimpleNet{T&lt;:Real}
    W1::Matrix{T}
    b1::Vector{T}
    W2::Matrix{T}
    b2::Vector{T}
end</code></pre><p>We will start with initializing the weights stored in the <code>SimpleNet</code> structure.</p><div class="exercise-body"><header class="exercise-header">Exercise:</header><p></p><p>Write an outer constructor for <code>SimpleNet</code>. Its inputs should be three integers representing the input size of the three layers. All matrices should be initialized based on the normal distribution.</p><p><strong>Hint</strong>: think about the representation of the dense layer.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>Since a dense layer computes <span>$Wx+b$</span>, the size of <span>$W$</span> should be the layer output size times the layer input size. The bias <span>$b$</span> should be of the size of the layer output.</p><pre><code class="language-julia">SimpleNet(n1, n2, n3) = SimpleNet(randn(n2, n1), randn(n2), randn(n3, n2), randn(n3))</code></pre><p></p></details><p>Out neural network will have five hidden neurons. Therefore, we need to initialize it with the following code.</p><pre><code class="language-julia">Random.seed!(666)

m = SimpleNet(size(X_train,1), 5, size(y_train,1))</code></pre><p>The following exercise computes the network prediction for samples. For a calling simplicity, we will write it as a functor.</p><div class="exercise-body"><header class="exercise-header">Exercise:</header><p></p><p>Write a functor <code>function (m::SimpleNet)(x)</code> which computes the prediction (forward pass) of the neural network <code>SimpleNet</code>.</p><p><strong>Bonus</strong>: try to make the functor work for both vectors (one sample) and matrices (multiple samples) <code>x</code>.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>The dense layer is a linear function <code>z1 = W1*x .+ b1</code> followed by an activation function. If we assume that <code>x</code> is a vector, then <code>+</code> would work the same as <code>.+</code> because both <code>W1*x</code> and <code>b</code> are of the same dimension. However, if we want <code>x</code> to be a matrix (each column corresponds to one sample), we need to write <code>.+</code> because <code>W1*x</code> is a matrix and the vector <code>b</code> needs to be broadcasted to be of the same size. The activation function is the ReLU function which needs to be applied componentwise. The procedure for the second layer is the same, but we need to finish it with the softmax function. If <code>x</code> is a matrix, then <code>z2</code> is a matrix, and we specify that we want to normalize along the first dimension. If we assume only vector inputs, then specifying the dimension is not necessary.</p><pre><code class="language-julia">function (m::SimpleNet)(x)
    z1 = m.W1*x .+ m.b1
    a1 = max.(z1, 0)
    z2 = m.W2*a1 .+ m.b2
    return exp.(z2) ./ sum(exp.(z2), dims=1)
end</code></pre><p></p></details><p>It is simple now to evaluate the first two samples one the training set.</p><pre><code class="language-julia">m(X_train[:,1:2])</code></pre><pre class="documenter-example-output">3Ã2 Array{Float64,2}:
 0.81372    0.384107
 0.0205311  0.0324847
 0.165749   0.583408</pre><p>Due to the softmax layer, they sum to one and form a probability distribution describing the probability of each class.</p><h2 id="Train-the-network"><a class="docs-heading-anchor" href="#Train-the-network">Train the network</a><a id="Train-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-network" title="Permalink"></a></h2><p>To train the network, we need to compute the gradients. It is rather complicated. However, when going through the code, it becomes clear that it is just a different form of the chain rule derived in the theoretical part.</p><pre><code class="language-julia">function grad(m::SimpleNet, x::AbstractVector, y; Ïµ=1e-10)
    z1 = m.W1*x .+ m.b1
    a1 = max.(z1, 0)
    z2 = m.W2*a1 .+ m.b2
    a2 = exp.(z2) ./ sum(exp.(z2), dims=1)
    l = -sum(y .* log.(a2 .+ Ïµ))

    e_z2 = exp.(z2)
    l_part = (- e_z2 * e_z2' + Diagonal(e_z2 .* sum(e_z2))) / sum(e_z2)^2

    l_a2 = - y ./ (a2 .+ Ïµ)
    l_z2 = l_part * l_a2
    l_a1 = m.W2' * l_z2
    l_z1 = l_a1 .* (a1 .&gt; 0)
    l_x = m.W1' * l_z1

    l_W2 = l_z2 * a1'
    l_b2 = l_z2
    l_W1 = l_z1 * x'
    l_b1 = l_z1

    return l, l_W1, l_b1, l_W2, l_b2
end</code></pre><p>The function returns the function value <code>l</code> and derivatives with respect to all four variables.</p><div class="info-body"><header class="info-header">That's it? I thought neural networks are magic...</header><p></p><p>Well, for a network with two layers and a loss, we can compute the function value and its derivative in only 16 lines of code.</p><p></p></div><div class="info-body"><header class="info-header">Simple implementation</header><p></p><p>The previous function <code>grad</code> can compute the gradient for only one sample. Since the objective in training a neural network is a mean over all samples, this mean needs to be included externally. This is NOT the correct way of writing function. However, we decided to present it in the current way to keep the presentation (relatively) simple. When such a simplification is included in the code, we should include a check such as <code>x::AbstractVector</code> to prevent unexpected errors. </p><p>When we compute gradients of multiple samples, we obtain an array. Each element is a tuple with five elements from the <code>grad</code> function.</p><pre><code class="language-julia">g_all = [grad(m, X_train[:,k], y_train[:,k]) for k in 1:size(X_train,2)]

typeof(g_all)</code></pre><pre class="documenter-example-output">Array{Tuple{Float64,Array{Float64,2},Array{Float64,1},Array{Float64,2},Array{Float64,1}},1}</pre><p>To compute the mean over all samples, we need to use the following obscure function.</p><pre><code class="language-julia">mean_tuple(d::AbstractArray{&lt;:Tuple}) = Tuple([mean([d[k][i] for k in 1:length(d)]) for i in 1:length(d[1])])</code></pre><p>We see that it produces an averaged output of the <code>grad</code> function, where the average is taken with respect to all its inputs.</p><pre><code class="language-julia">g_mean = mean_tuple(g_all)

typeof(g_mean)</code></pre><pre class="documenter-example-output">Tuple{Float64,Array{Float64,2},Array{Float64,1},Array{Float64,2},Array{Float64,1}}</pre><p></p></div><p>Having the gradient at hand, we can finally train the network.</p><div class="exercise-body"><header class="exercise-header">Exercise:</header><p></p><p>Train the network with a gradient descent with stepsize <span>$\alpha=0.1$</span> for <span>$200$</span> iterations. Save the objective value at each iteration and plot the results.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>Now the process is simple. We compute the gradient <code>grad_all</code>, then its mean <code>grad_mean</code> via the already written function <code>mean_tuple</code>. The first value of the tuple <code>grad_mean</code> is the objective; the remaining are the gradients. Thus, we save the first value to an array and use the remaining one to update the weights.</p><pre><code class="language-julia">Î± = 1e-1
max_iter = 200
L = zeros(max_iter)
for iter in 1:max_iter
    grad_all = [grad(m, X_train[:,k], y_train[:,k]) for k in 1:size(X_train,2)]
    grad_mean = mean_tuple(grad_all)

    L[iter] = grad_mean[1]

    m.W1 .-= Î±*grad_mean[2]
    m.b1 .-= Î±*grad_mean[3]
    m.W2 .-= Î±*grad_mean[4]
    m.b2 .-= Î±*grad_mean[5]
end</code></pre><p></p></details><p><img alt="" src="../loss.svg"/></p><h2 id="Prediction"><a class="docs-heading-anchor" href="#Prediction">Prediction</a><a id="Prediction-1"></a><a class="docs-heading-anchor-permalink" href="#Prediction" title="Permalink"></a></h2><p>We have trained our first network. We saw that the loss function keeps decreasing, which indicates a good training procedure. Now we will evaluate the performance.</p><div class="exercise-body"><header class="exercise-header">Exercise:</header><p></p><p>Write a function which predict the labels for samples. Show the accuracy on both training and testing sets.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>The predicted probabilities are obtained by using the model <code>m</code>. The prediction (highest predicted probability) is obtained by converting the one-hot into the one-cold representation. Finally, the accuracy computes in how many cases the prediction equals to the label.</p><pre><code class="language-julia">predict(X) = m(X)
accuracy(X, y) = mean(onecold(predict(X), classes) .== onecold(y, classes))

println("Train accuracy = ", accuracy(X_train, y_train))
println("Test accuracy = ", accuracy(X_test, y_test))</code></pre><pre class="documenter-example-output">Train accuracy = 0.9666666666666667
Test accuracy = 0.9333333333333333</pre><p></p></details><p>The correct answer is</p><pre class="documenter-example-output">Train accuracy = 0.9666666666666667
Test accuracy = 0.9333333333333333</pre><p>We see that the testing accuracy is smaller than the training one. This is quite a common phenomenon which is named overfitting. The problem is that the algorithm sees only the data from the training set. If it fits this data "too perfectly", it cannot generalize into unseen samples (the testing set).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../theory/">Â« Theory of neural networks</a><a class="docs-footer-nextpage" href="../exercises/">Exercises Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 3 May 2021 22:39">Monday 3 May 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>