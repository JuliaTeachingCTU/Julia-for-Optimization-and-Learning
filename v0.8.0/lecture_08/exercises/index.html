<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Exercises · Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Machine Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Machine Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox" checked/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../linear/">Linear regression</a></li><li><a class="tocitem" href="../logistic/">Logistic regression</a></li><li class="is-active"><a class="tocitem" href>Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/iris/">Introduction to Flux</a></li><li><a class="tocitem" href="../../lecture_10/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../../lecture_11/glm/">Linear regression revisited</a></li><li><a class="tocitem" href="../../lecture_11/sparse/">Linear regression with sparse constraints</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">8: Regression and classification</a></li><li class="is-active"><a href>Exercises</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Exercises</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_08/exercises.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="l8-exercises"><a class="docs-heading-anchor" href="#l8-exercises">Exercises</a><a id="l8-exercises-1"></a><a class="docs-heading-anchor-permalink" href="#l8-exercises" title="Permalink"></a></h1><div class = "homework-body">
<header class = "homework-header">Homework: Data normalization</header><p><p>Data are often normalized. Each feature subtracts its mean and then divides the result by its standard deviation. The normalized features have zero mean and unit standard deviation. This may help in several cases:</p><ul><li>When each feature has a different order of magnitude (such as millimetres and kilometres). Then the gradient would ignore the feature with the smaller values.</li><li>When problems such as vanishing gradients are present (we will elaborate on this in Exercise 4).</li></ul><p>Write function <code>normalize</code> which takes as an input a dataset and normalizes it. Then train the same classifier as we did for <a href="../logistic/#log-reg">logistic regression</a>. Use the original and normalized dataset. Which differences did you observe when</p><ul><li>the logistic regression is optimized via gradient descent?</li><li>the logistic regression is optimized via Newton&#39;s method?</li></ul><p>Do you have any intuition as to why?</p><p>Write a short report (in LaTeX) summarizing your findings.</p></p></div><div class = "exercise-body">
<header class = "exercise-header">Exercise 1</header><p><p>The logistic regression on the iris dataset failed in 6 out of 100 samples. But the visualization shows the failure only in 5 cases. How is it possible?</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We use the <code>iris_reduced</code> dataframe and add the column <code>prediction</code> to it.</p><pre><code class="language-julia">df = iris_reduced
df.prediction = σ.(X*w) .&gt;= 0.5</code></pre><p>Now we show all misclassified samples.</p><pre><code class="language-julia">sort(df[df.label .!= df.prediction, :], [:PetalLength, :PetalWidth])</code></pre><table class="data-frame"><thead><tr><th></th><th>PetalLength</th><th>PetalWidth</th><th>intercept</th><th>Species</th><th>label</th><th>prediction</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Int64</th><th>Cat…</th><th>Bool</th><th>Bool</th></tr></thead><tbody><p>6 rows × 6 columns</p><tr><th>1</th><td>4.5</td><td>1.7</td><td>1</td><td>virginica</td><td>1</td><td>0</td></tr><tr><th>2</th><td>4.8</td><td>1.8</td><td>1</td><td>versicolor</td><td>0</td><td>1</td></tr><tr><th>3</th><td>5.0</td><td>1.5</td><td>1</td><td>virginica</td><td>1</td><td>0</td></tr><tr><th>4</th><td>5.0</td><td>1.7</td><td>1</td><td>versicolor</td><td>0</td><td>1</td></tr><tr><th>5</th><td>5.1</td><td>1.5</td><td>1</td><td>virginica</td><td>1</td><td>0</td></tr><tr><th>6</th><td>5.1</td><td>1.6</td><td>1</td><td>versicolor</td><td>0</td><td>1</td></tr></tbody></table><p>A quick look at the image shows that the point <span>$(4.8,1.8)$</span> is misclassified, but the image shows it correctly. Let us show all such points.</p><pre><code class="language-julia">df[(df.PetalLength .== 4.8) .&amp; (df.PetalWidth .== 1.8), :]</code></pre><table class="data-frame"><thead><tr><th></th><th>PetalLength</th><th>PetalWidth</th><th>intercept</th><th>Species</th><th>label</th><th>prediction</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Int64</th><th>Cat…</th><th>Bool</th><th>Bool</th></tr></thead><tbody><p>3 rows × 6 columns</p><tr><th>1</th><td>4.8</td><td>1.8</td><td>1</td><td>versicolor</td><td>0</td><td>1</td></tr><tr><th>2</th><td>4.8</td><td>1.8</td><td>1</td><td>virginica</td><td>1</td><td>1</td></tr><tr><th>3</th><td>4.8</td><td>1.8</td><td>1</td><td>virginica</td><td>1</td><td>1</td></tr></tbody></table><p>As we can see, there are three samples with the same data. Two of them have label 1 and one label 0. Since the incorrectly classified sample was redrawn, it was not possible to see it.</p></p></details><div class = "exercise-body">
<header class = "exercise-header">Exercise 2: Disadvantages of the sigmoid function</header><p><p>Show that Newton&#39;s method fails when started from the vector <span>$(1,2,3)$</span>. Can you guess why it happened? What are the consequences for optimization? Is gradient descent going to suffer from the same problems?</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>First, we run the logistic regression as before, only with a different starting point</p><pre><code class="language-julia">log_reg(X, y, [1;2;3])</code></pre><pre class="documenter-example-output">3-element Array{Float64,1}:
 NaN
 NaN
 NaN</pre><p>This resulted in NaNs. When something fails, it may be a good idea to run a step-by-step analysis. In this case, we will run the first iteration of Newton&#39;s method</p><pre><code class="language-julia-repl">julia&gt; w = [1;2;3];

julia&gt; X_mult = [row*row&#39; for row in eachrow(X)];

julia&gt; y_hat = 1 ./(1 .+exp.(-X*w))
100-element Array{Float64,1}:
 0.9999724643088853
 0.9999724643088853
 0.9999815421067044
 0.9999322758503804
 0.9999750846110607
 0.9999589221322353
 0.9999815421067044
 0.9997515449181605
 0.9999628310628971
 0.9999387202603833
 ⋮
 0.9999969488837513
 0.999993209641302
 0.999998629042793
 0.9999988775548947
 0.9999972392350497
 0.9999924954984029
 0.9999949695696981
 0.999997739675702
 0.9999917062496261

julia&gt; grad = X&#39;*(y_hat.-y) / size(X,1)
3-element Array{Float64,1}:
 2.129852089784904
 0.66295432173409
 0.49996260866776915

julia&gt; hess = y_hat.*(1 .-y_hat).*X_mult |&gt; mean
3×3 Array{Float64,2}:
 0.000598417  0.00018514  0.000147894
 0.00018514   5.80682e-5  4.56733e-5
 0.000147894  4.56733e-5  3.73868e-5

julia&gt; w -= hess \ grad
3-element Array{Float64,1}:
 -10764.590886852531
  -1801.6675865928162
  31420.070268736363</code></pre><p>Starting from the bottom, we can see that even though we started with relatively small <span>$w$</span>, the next iteration is four degrees of magnitude larger. This happened because the Hessian <code>hess</code> is much smaller than the gradient <code>grad</code>. This indicates that there is some kind of numerical instability. The prediction <code>y_hat</code> should lie in the interval <span>$[0,1]$</span> but it seems that it is almost always close to 1. Let us verify this by showing the extrema of <code>y_hat</code></p><pre><code class="language-julia">extrema(y_hat)</code></pre><pre class="documenter-example-output">(0.9997254218438986, 0.9999994956525918)</pre><p>They are indeed too large.</p><p>Now we explain the reason. We know that the prediction equals to</p><p class="math-container">\[\hat y_i = \sigma(w^\top x_i),\]</p><p>where <span>$\sigma$</span> is the sigmoid function. Since the mimimum from <span>$w^\top x_i$</span></p><pre><code class="language-julia">minimum(X*[1;2;3])</code></pre><pre class="documenter-example-output">8.2</pre><p>is large, all <span>$w^\top x_i$</span> are large. But plotting the sigmoid funtion</p><pre><code class="language-julia">xs = -10:0.01:10
plot(xs, σ, label=&quot;&quot;, ylabel=&quot;Sigmoid function&quot;)</code></pre><p><img src="../sigmoid.svg" alt/></p><p>it is clear that all <span>$w^\top x_i$</span> hit the part of the sigmoid which is flat. This means that the derivative is almost zero, and the Hessian is &quot;even smaller&quot; zero. Then the ratio of the gradient and Hessian is huge.</p><p>The gradient descent will probably run into the same difficulty. Since the gradient will be too small, it will take a huge number of iterations to escape the flat region of the sigmoid. This is a known problem of the sigmoid function. It is also the reason why it was replaced in neural networks by other activation functions.</p></p></details><div class = "exercise-body">
<header class = "exercise-header">Exercise 3 (theory)</header><p><p>Show the details for the derivation of the loss function of the logistic regression.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Since <span>$\hat y$</span> equals the probability of predicting <span>$1$</span>, we have</p><p class="math-container">\[\hat y = \frac{1}{1+e^{-w^\top x}}\]</p><p>Then the cross-entropy loss reduces to</p><p class="math-container">\[\begin{aligned}
\operatorname{loss}(y,\hat y) &amp;= - y\log \hat y - (1-y)\log(1-\hat y) \\
&amp;= y\log(1+e^{-w^\top x}) - (1-y)\log(e^{-w^\top x}) + (1-y)\log(1+e^{-w^\top x}) \\
&amp;= \log(1+e^{-w^\top x}) + (1-y)w^\top x.
\end{aligned}\]</p><p>Then it remains to sum this term over all samples.</p></p></details><div class = "exercise-body">
<header class = "exercise-header">Exercise 4 (theory)</header><p><p>Show that if the Newton&#39;s method converged for the logistic regression, then it found a point globally minimizing the logistic loss. </p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We derived that the Hessian of the objective function for logistic regression is</p><p class="math-container">\[\nabla^2 L(w) = \frac 1n \sum_{i=1}^n\hat y_i(1-\hat y_i)x_i x_i^\top.\]</p><p>For any vector <span>$a$</span>, we have</p><p class="math-container">\[a^\top x_i x_i^\top a = (x_i^\top a)^\top (x_i^\top a) = \|x_i^\top a\|^2 \ge 0,\]</p><p>which implies that <span>$x_i x_i^\top$</span> is a positive semidefinite matrix (it is known as rank-1 matrix as its rank is always 1 if <span>$x_i$</span> is a non-zero vector). Since <span>$y_i(1-\hat y_i)\ge 0$</span>, it follows that <span>$\nabla^2 L(w)$</span> is a positive semidefinite matrix. If a Hessian of a function is positive semidefinite everywhere, the function is immediately convex. Since Newton&#39;s method found a stationary point, this points is a global minimum.</p></p></details></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../logistic/">« Logistic regression</a><a class="docs-footer-nextpage" href="../../lecture_09/theory/">Theory of neural networks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 3 May 2021 22:39">Monday 3 May 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
