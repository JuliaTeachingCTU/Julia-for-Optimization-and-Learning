<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction to Flux · Julia for Optimization and Learning</title><meta name="title" content="Introduction to Flux · Julia for Optimization and Learning"/><meta property="og:title" content="Introduction to Flux · Julia for Optimization and Learning"/><meta property="twitter:title" content="Introduction to Flux · Julia for Optimization and Learning"/><meta name="description" content="Documentation for Julia for Optimization and Learning."/><meta property="og:description" content="Documentation for Julia for Optimization and Learning."/><meta property="twitter:description" content="Documentation for Julia for Optimization and Learning."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Julia for Optimization and Learning</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Setup guide</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/installation/">Install</a></li><li><a class="tocitem" href="../../installation/tutorial/">Project setup</a></li><li><a class="tocitem" href="../../installation/running/">Running Julia</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Basics I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Basics II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/functions/">Function basics</a></li><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_03/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_03/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_03/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_03/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_03/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_04/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_04/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_04/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/structure/">Package structure</a></li><li><a class="tocitem" href="../../lecture_06/workflow/">Development workflow</a></li><li><a class="tocitem" href="../../lecture_06/compatibility/">Package dependencies</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Code organization II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/tests/">Tests</a></li><li><a class="tocitem" href="../../lecture_07/documentation/">Documentation</a></li><li><a class="tocitem" href="../../lecture_07/extensions/">Extensions</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_08/gradients/">Gradients</a></li><li><a class="tocitem" href="../../lecture_08/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_09/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_09/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox" checked/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li class="is-active"><a class="tocitem" href>Introduction to Flux</a><ul class="internal"><li><a class="tocitem" href="#Creating-the-network"><span>Creating the network</span></a></li><li><a class="tocitem" href="#Training-the-network"><span>Training the network</span></a></li></ul></li><li><a class="tocitem" href="../nn/">More complex networks</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/sparse/">Linear regression with sparse constraints</a></li><li><a class="tocitem" href="../../lecture_12/monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../../lecture_12/glm/">Linear regression revisited</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">11: Neural networks II.</a></li><li class="is-active"><a href>Introduction to Flux</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Introduction to Flux</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_11/iris.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Introduction-to-Flux"><a class="docs-heading-anchor" href="#Introduction-to-Flux">Introduction to Flux</a><a id="Introduction-to-Flux-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction-to-Flux" title="Permalink"></a></h1><p>Flux is a library for using neural networks. This part will present the basics of Flux on the Iris dataset from the previous lecture. We include the auxiliary functions from the previous lesson into the <code>utilities.jl</code> file, which we include by</p><pre><code class="language-julia hljs">include(&quot;utilities.jl&quot;)</code></pre><p>We set the seed and load the data in the same way as during the last lecture.</p><pre><code class="language-julia hljs">using RDatasets
using Random

Random.seed!(666)

iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)

X = Matrix{Float32}(iris[:, 1:4])
y = iris.Species

X_train, y_train, X_test, y_test, classes = prepare_data(X&#39;, y; dims=2)</code></pre><h2 id="Creating-the-network"><a class="docs-heading-anchor" href="#Creating-the-network">Creating the network</a><a id="Creating-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-the-network" title="Permalink"></a></h2><p>We recall that machine learning minimizes the discrepancy between the predictions <span>$\operatorname{predict}(w; x_i)$</span> and labels <span>$y_i$</span>. Mathematically, this amounts to minimizing the following objective function:</p><p class="math-container">\[L(w) = \frac1n\sum_{i=1}^n \operatorname{loss}(y_i, \operatorname{predict}(w; x_i)).\]</p><p>To build the objective <span>$L$</span>, we first specify the prediction function <span>$\operatorname{predict}$</span>, which we denote by model <code>m</code>.  We start by creating the same network by the function <code>Chain</code>. Its inputs are the individual layers. Dense layers are created by <code>Dense</code> with the correct number of input and output neurons. We also need to specify the activation functions.</p><pre><code class="language-julia hljs">using Flux

n_hidden = 5
m = Chain(
    Dense(size(X_train,1) =&gt; n_hidden, relu),
    Dense(n_hidden =&gt; size(y_train,1), identity),
    softmax,
)</code></pre><p>Since <code>identity</code> is the default argument, it is possible to remove it in the second layer. However, we recommend keeping it for clarity.</p><p>We can evaluate the whole dataset.</p><pre><code class="language-julia hljs">m(X_train)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3×120 Matrix{Float32}:
 0.374425  0.394161  0.31563   0.275201  …  0.414601  0.361287  0.241313
 0.315142  0.133596  0.197784  0.544938     0.227725  0.313487  0.620158
 0.310433  0.472243  0.486586  0.179861     0.357673  0.325226  0.13853</code></pre><p>Because there are <span>$3$</span> classes and <span>$120$</span> samples in the training set, it returns an array of size <span>$3\times 120$</span>. Each column corresponds to one sample and forms a vector of probabilities due to the last layer of softmax.</p><p>We access the neural network parameters by using <code>params(m)</code>. We can select the second layer of <code>m</code> by <code>m[2]</code>. Since the second layer has <span>$5$</span> inputs and <span>$3$</span> output neurons, its parameters are a matrix of size <span>$3\times 5$</span> and a vector of length <span>$3$</span>. The parameters <code>params(m[2])</code> are a tuple of the matrix and the vector. This also implies that the parameters are initialized randomly, and we do not need to take care of it. We can also easily modify any parameters.</p><pre><code class="language-julia hljs">using Flux: params

params(m[2])[2] .= [-1;0;1]</code></pre><h2 id="Training-the-network"><a class="docs-heading-anchor" href="#Training-the-network">Training the network</a><a id="Training-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-network" title="Permalink"></a></h2><p>To train the network, we need to define the objective function <span>$L$</span>. Since we already defined <span>$\operatorname{predict}$</span>, it suffices to define the loss function <span>$\operatorname{loss}$</span>. Since we work with a multi-class problem, the loss function is usually the cross-entropy.</p><pre><code class="language-julia hljs">using Flux: crossentropy

L(ŷ, y) = crossentropy(ŷ, y)</code></pre><p>The <code>loss</code> function should be defined between predicted <span>$\hat{y}$</span> and true label <span>$y$</span>. Therefore, we can evaluate the objective function by</p><pre><code class="language-julia hljs">L(m(X_train), y_train)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.5389596f0</code></pre><p>where <code>ŷ = m(x)</code>.</p><p>This computes the objective function on the whole training set. Since Flux is (unlike our implementation from the last lecture) smart, there is no need to take care of individual samples.</p><div class="admonition is-info" id="Notation:-fc44596901d9667"><header class="admonition-header">Notation:<a class="admonition-anchor" href="#Notation:-fc44596901d9667" title="Permalink"></a></header><div class="admonition-body"><p>While the <a href="https://en.wikipedia.org/wiki/Cross_entropy">standard definition</a> of cross-entropy is <span>$\operatorname{loss}(y,\hat y)$</span>, <a href="https://fluxml.ai/Flux.jl/stable/models/losses/">Flux</a> uses <span>$\operatorname{loss}(\hat y,y)$</span>.</p></div></div><p>Since we have the model and the loss function, the only remaining thing is the gradient. Flux again provides a smart way to compute it.</p><pre><code class="language-julia hljs">grads = Flux.gradient(m -&gt; L(m(X_train), y_train), m)</code></pre><p>The function <code>gradient</code> takes as inputs a function to differentiate, and arguments that specify the parameters we want to differentiate with respect to. Since the argument is the model <code>m</code> itself, the gradient is taken with respect to the parameters of <code>m</code>. The <code>L</code> function needs to be evaluated at the correct points <code>m(X_train)</code> (predictions) and <code>y_train</code> (true labels).</p><p>The <code>grads</code> structure is a tuple holding a named tuple with the <code>:layers</code> key. Each layer then holds the parameters of the model, in this case, the weights <span>$W$</span>, bias <span>$b$</span>, and optionally parameters of the activation function <span>$\sigma$</span>.</p><pre><code class="language-julia hljs">julia&gt; grads[1][:layers][2]
(weight = Float32[0.30140522 0.007785671 … -0.070617765 0.014230583; 0.06814249 -0.07018863 … 0.17996183 -0.20995824; -0.36954764 0.062402964 … -0.10934405 0.19572766], bias = Float32[0.0154182855, 0.022615476, -0.03803377], σ = nothing)</code></pre><p>Now, we train the classifiers for 250 iterations. In each iteration, we compute the gradient with respect to all network parameters and perform the gradient descent with stepsize <span>$0.1$</span>. Since Flux@0.14, there&#39;s been a change from implicit definition to explicit definition of optimisers. Since now, we need to use <code>Flux.setup(optimiser, model)</code> to create an optimiser that would optimise over the model&#39;s parameters.</p><pre><code class="language-julia hljs">opt = Descent(0.1)
opt_state = Flux.setup(opt, m)
max_iter = 250

acc_train = zeros(max_iter)
acc_test = zeros(max_iter)
for i in 1:max_iter
    gs = Flux.gradient(m -&gt; L(m(X_train), y_train), m)
    Flux.update!(opt_state, m, gs[1])
    acc_train[i] = accuracy(X_train, y_train)
    acc_test[i] = accuracy(X_test, y_test)
end</code></pre><p>Both the accuracy on the training and testing set keeps increasing as the training progresses. This is a good check that we are not over-fitting.</p><pre><code class="language-julia hljs">using Plots

plot(acc_train, xlabel=&quot;Iteration&quot;, ylabel=&quot;Accuracy&quot;, label=&quot;train&quot;, ylim=(-0.01,1.01))
plot!(acc_test, xlabel=&quot;Iteration&quot;, label=&quot;test&quot;, ylim=(-0.01,1.01))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_11/Iris_train_test_acc.svg&quot;</code></pre><p><img src="../Iris_train_test_acc.svg" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../theory/">« Theory of neural networks</a><a class="docs-footer-nextpage" href="../nn/">More complex networks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 3 December 2025 16:23">Wednesday 3 December 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
