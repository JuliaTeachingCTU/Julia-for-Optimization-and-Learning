<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction to Flux Â· Julia for Optimization and Learning</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Julia for Optimization and Learning</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/vscode/">Julia + Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Data structures</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_02/tuples/">Tuples and named tuples</a></li><li><a class="tocitem" href="../../lecture_02/dictionaries/">Dictionaries</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_03/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_04/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_04/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_04/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_05/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_05/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_05/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_05/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_06/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_07/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_07/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_08/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_09/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_09/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox" checked/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li class="is-active"><a class="tocitem" href>Introduction to Flux</a><ul class="internal"><li><a class="tocitem" href="#Creating-the-network"><span>Creating the network</span></a></li><li><a class="tocitem" href="#Training-the-network"><span>Training the network</span></a></li></ul></li><li><a class="tocitem" href="../nn/">More complex networks</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/sparse/">Linear regression with sparse constraints</a></li><li><a class="tocitem" href="../../lecture_12/monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../../lecture_12/glm/">Linear regression revisited</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">13: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_13/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_13/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_13/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_13/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">11: Neural networks II.</a></li><li class="is-active"><a href>Introduction to Flux</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Introduction to Flux</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_11/iris.md" title="Edit on GitHub"><span class="docs-icon fab">ï</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Introduction-to-Flux"><a class="docs-heading-anchor" href="#Introduction-to-Flux">Introduction to Flux</a><a id="Introduction-to-Flux-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction-to-Flux" title="Permalink"></a></h1><p>Flux is a library for using neural networks. This part will present the basics of Flux on the Iris dataset from the previous lecture. We include the auxiliary functions from the previous lesson into the <code>utilities.jl</code> file, which we include by</p><pre><code class="language-julia hljs">include(&quot;utilities.jl&quot;)</code></pre><p>We set the seed and load the data in the same way as during the last lecture.</p><pre><code class="language-julia hljs">using RDatasets
using Random

Random.seed!(666)

iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)

X = Matrix(iris[:, 1:4])
y = iris.Species

X_train, y_train, X_test, y_test, classes = prepare_data(X&#39;, y; dims=2)</code></pre><h2 id="Creating-the-network"><a class="docs-heading-anchor" href="#Creating-the-network">Creating the network</a><a id="Creating-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-the-network" title="Permalink"></a></h2><p>We recall that machine learning minimizes the discrepancy between the predictions <span>$\operatorname{predict}(w; x_i)$</span> and labels <span>$y_i$</span>. Mathematically, this amount to minimizing the following objective function.  </p><p class="math-container">\[L(w) = \frac1n\sum_{i=1}^n \operatorname{loss}(y_i, \operatorname{predict}(w; x_i)).\]</p><p>To build the objective <span>$L$</span>, we first specify the prediction function <span>$\operatorname{predict}$</span>, which we denote by model <code>m</code>.  We start by creating the same network by the function <code>Chain</code>. Its inputs are the individual layers. Dense layers are created by <code>Dense</code> with the correct number of input and output neurons. We also need to specify the activation functions.</p><pre><code class="language-julia hljs">using Flux

n_hidden = 5
m = Chain(
    Dense(size(X_train,1), n_hidden, relu),
    Dense(n_hidden, size(y_train,1), identity),
    softmax,
)</code></pre><p>Since <code>identity</code> is the default argument, it is possible to remove it in the second layer. However, we recommend keeping it for clarity.</p><p>We can evaluate the whole dataset.</p><pre><code class="language-julia hljs">m(X_train)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3Ã120 Matrix{Float64}:
 0.374425  0.394161  0.31563   0.275201  â¦  0.414601  0.361286  0.241313
 0.315142  0.133596  0.197784  0.544938     0.227725  0.313487  0.620158
 0.310433  0.472243  0.486586  0.179861     0.357674  0.325227  0.13853</code></pre><p>Because there are <span>$3$</span> classes and <span>$120$</span> samples in the training set, it returns an array of size <span>$3\times 120$</span>. Each column corresponds to one sample and forms a vector of probabilities due to the last layer of softmax.</p><p>We access the neural network parameters by using <code>params(m)</code>. We can select the second layer of <code>m</code> by <code>m[2]</code>. Since the second layer has <span>$5$</span> input and <span>$3$</span> output neurons, its parameters are a matrix of size <span>$3\times 5$</span> and a vector of length <span>$3$</span>. The parameters <code>params(m[2])</code> are a tuple of the matrix and the vector. This also implies that the parameters are initialized randomly, and we do not need to take care of it. We can easily modify any parameters.</p><pre><code class="language-julia hljs">using Flux: params

params(m[2])[2] .= [-1;0;1]</code></pre><h2 id="Training-the-network"><a class="docs-heading-anchor" href="#Training-the-network">Training the network</a><a id="Training-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-network" title="Permalink"></a></h2><p>To train the network, we need to define the objective function <span>$L$</span>. Since we already defined <span>$\operatorname{predict}$</span>, it suffices to define the loss function <span>$\operatorname{loss}$</span>. Since we work with a multi-class problem, the loss function is usually the cross-entropy.</p><pre><code class="language-julia hljs">using Flux: crossentropy

L(x,y) = crossentropy(m(x), y)</code></pre><p>The <code>loss</code> function does not have <code>m</code> as input. Even though there could be an additional input parameter, it is customary to write it without it. We can evaluate the objective function by</p><pre><code class="language-julia hljs">L(X_train, y_train)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.5389607629762656</code></pre><p>This computes the objective function on the whole training set. Since Flux is (unlike our implementation from the last lecture) smart, there is no need to take care of individual samples.</p><div class="admonition is-info"><header class="admonition-header">Notation:</header><div class="admonition-body"><p>While the <a href="https://en.wikipedia.org/wiki/Cross_entropy">standard definition</a> of cross-entropy is <span>$\operatorname{loss}(y,\hat y)$</span>, <a href="https://fluxml.ai/Flux.jl/stable/models/losses/">Flux</a> uses <span>$\operatorname{loss}(\hat y,y)$</span>.</p></div></div><p>Since we have the model and the loss function, the only remaining thing is the gradient. Flux again provides a smart way to compute it.</p><pre><code class="language-julia hljs">ps = params(m)
grad = gradient(() -&gt; L(X_train, y_train), ps)</code></pre><p>The function <code>gradient</code> takes two inputs. The first one is the function we want to differentiate, and the second one are the parameters. The <code>L</code> function needs to be evaluated at the correct points <code>X_train</code> and <code>y_train</code>. In some applications, we may need to differentiate with respect to other parameters such as <code>X_train</code>. This can be achieved by changing the second parameters of the <code>gradient</code> function.</p><pre><code class="language-julia hljs">grad = gradient(() -&gt; L(X_train, y_train), params(X_train))

size(grad[X_train])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(4, 120)</code></pre><p>Since <code>X_train</code> has shape <span>$4\times 120$</span>, the gradient needs to have the same size.</p><p>We train the classifiers for 250 iterations. In each iteration, we compute the gradient with respect to all network parameters and perform the gradient descent with stepsize <span>$0.1$</span>.</p><pre><code class="language-julia hljs">opt = Descent(0.1)
max_iter = 250

acc_test = zeros(max_iter)
for i in 1:max_iter
    gs = gradient(() -&gt; L(X_train, y_train), ps)
    Flux.Optimise.update!(opt, ps, gs)
    acc_test[i] = accuracy(X_test, y_test)
end</code></pre><p>The accuracy on the testing set keeps increasing as the training progresses.</p><pre><code class="language-julia hljs">using Plots

plot(acc_test, xlabel=&quot;Iteration&quot;, ylabel=&quot;Test accuracy&quot;, label=&quot;&quot;, ylim=(-0.01,1.01))</code></pre><p><img src="../Iris_acc.svg" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../theory/">Â« Theory of neural networks</a><a class="docs-footer-nextpage" href="../nn/">More complex networks Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Friday 16 September 2022 12:30">Friday 16 September 2022</span>. Using Julia version 1.8.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
