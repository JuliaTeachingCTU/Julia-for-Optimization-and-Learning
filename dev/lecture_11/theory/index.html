<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theory of neural networks · Julia for Optimization and Learning</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/ctustyle.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Julia for Optimization and Learning</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/vscode/">Julia + Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Data structures</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_02/tuples/">Tuples and named tuples</a></li><li><a class="tocitem" href="../../lecture_02/dictionaries/">Dictionaries</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_03/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_04/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_04/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_04/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_05/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_05/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_05/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_05/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_06/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_07/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_07/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_08/gradients/">Gradients</a></li><li><a class="tocitem" href="../../lecture_08/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_09/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_09/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox" checked/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Theory of neural networks</a><ul class="internal"><li><a class="tocitem" href="#Convolutional-layers"><span>Convolutional layers</span></a></li><li><a class="tocitem" href="#Network-structure"><span>Network structure</span></a></li><li><a class="tocitem" href="#Stochastic-gradient-descent"><span>Stochastic gradient descent</span></a></li></ul></li><li><a class="tocitem" href="../iris/">Introduction to Flux</a></li><li><a class="tocitem" href="../nn/">More complex networks</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/sparse/">Linear regression with sparse constraints</a></li><li><a class="tocitem" href="../../lecture_12/monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../../lecture_12/glm/">Linear regression revisited</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">13: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_13/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_13/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_13/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_13/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">11: Neural networks II.</a></li><li class="is-active"><a href>Theory of neural networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theory of neural networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_11/theory.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Theory-of-neural-networks"><a class="docs-heading-anchor" href="#Theory-of-neural-networks">Theory of neural networks</a><a id="Theory-of-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Theory-of-neural-networks" title="Permalink"></a></h1><p>In the previous lecture, we presented an introduction to neural networks. We also showed how to train neural networks using gradient descent. This lecture is going to show more layers and a more sophisticated way of training.</p><h2 id="Convolutional-layers"><a class="docs-heading-anchor" href="#Convolutional-layers">Convolutional layers</a><a id="Convolutional-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional-layers" title="Permalink"></a></h2><p>The last lecture concentrated on the dense layer. Even though it is widely used due to its simplicity, it suffers from several disadvantages, especially in visual recognition. These disadvantages include:</p><ul><li><em>Large number of parameters</em>. For an image with <span>$500\times 500\times 3$</span> pixels and the output layer of only <span>$1000$</span> neurons, the dense layer would contain <span>$750$</span> million parameters. This is too much to optimize.</li><li><em>No structural information</em>. Dense layers assign a weight to every pixel and then add the weighted values. This means that information from the top-leftmost and bottom-rightmost pixels of the image will be combined. Since a combination of these two pixels should carry no meaningful information, redundant computation is performed.</li></ul><p>Convolutional layers were designed to alleviate these issues.</p><h4 id="Motivation"><a class="docs-heading-anchor" href="#Motivation">Motivation</a><a id="Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation" title="Permalink"></a></h4><p>To understand the convolutional layers, we need to go back to the definition of convolution. Having a function <span>$f$</span> and  a kernel <span>$g$</span>, their convolution is defined by</p><p class="math-container">\[(f\ast g)(x) = \int_{-\infty}^{\infty} f(x - t)g(t) dt.\]</p><p>Let us consider the simplest case when</p><p class="math-container">\[g(t) = \begin{cases} \frac{1}{2\varepsilon} &amp;\text{if }t\in[-\varepsilon,\varepsilon], \\ 0 &amp;\text{otherwise.} \end{cases}\]</p><p>Then </p><p class="math-container">\[(f\ast g)(x) = \int_{-\infty}^{\infty} f(x - t)g(t) dt = \frac{1}{2\varepsilon}\int_{-\varepsilon}^{\varepsilon}f(x - t)dt.\]</p><p>Then <span>$(f\ast g)(x)$</span> does not take the value of <span>$f$</span> at <span>$x$</span> but integrates <span>$f$</span> over a small neighbourhood of <span>$x$</span>. Applying this kernel results in a smoothening of <span>$f$</span>.  </p><p>In image processing, the image <span>$f$</span> is not represented by a function but by a collection of pixels. The kernel <span>$g$</span> is represented by a small matrix. For the commonly used <span>$3\times 3$</span> kernel matrix, the convolution has the form</p><p class="math-container">\[(f\ast g)(x,y) = \sum_{i=-1}^1\sum_{j=1}^1 f(x+i,y+j)g(i,j).\]</p><p>The following kernels</p><p class="math-container">\[K_1 = \begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}, \qquad
K_2 = \frac 19\begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 \end{pmatrix}, \qquad
K_3 = \begin{pmatrix} -1 &amp; -1 &amp; -1 \\ -1 &amp; 8 &amp; -1 \\ -1 &amp; -1 &amp; -1 \end{pmatrix}\]</p><p>perform identity, image smoothening and edge detection, respectively.</p><p><img src="../turtles.png" alt/></p><h4 id="Formulas"><a class="docs-heading-anchor" href="#Formulas">Formulas</a><a id="Formulas-1"></a><a class="docs-heading-anchor-permalink" href="#Formulas" title="Permalink"></a></h4><p>Traditional techniques for image processing use multiple fixed kernels and combine their results. The idea of convolutional layers is to remove all human-made assumptions about which kernels to choose and learn the kernels&#39; parameters based purely on data. Even though it gives superb results, it also removes any insight or interpretation humans may make. </p><p><img src="../nn.png" alt/></p><p>The input of a convolutional layer has dimension <span>$I_1\times J_1\times C_1$</span>, where <span>$I_1\times J_1$</span> is the size of the image and <span>$C_1$</span> is the number of channels (1 for grayscale, 3 for coloured, anything for hidden layers). Its input is also the kernel <span>$K$</span>. The output of the convolutional layer has dimension <span>$I_2\times J_2\times C_2$</span> and its value at some <span>$(i_0,j_0,c_0)$</span> equals to</p><p class="math-container">\[\text{output}(i_0,j_0,c_0) = l\left(\sum_{c=1}^C\sum_{i=-a}^{a}\sum_{j=-b}^b \Big( K_{c_0}(i,j,c) \text{input}(i_0+i,j_0+j,c) + b(c)\Big)\right).\]</p><p>After the linear operation inside, an activation function <span>$l$</span> is applied. Without it, the whole network would a product of linear function and, therefore, linear function (written in a complicated form).</p><p>The natural question is the interpretation of the linear operator and the number of parameters:</p><ul><li>The kernel matrix <span>$K$</span> contains <span>$(2a+1)(2b+1)C_1C_2$</span> parameters. What does it mean? First, there is a separate kernel for each output channels. Second, the kernel also averages (more precisely, computes a linear combination) over all input channels. However, the coefficients of this linear combination do not depend on the position <span>$(i_0,j_0)$</span>. </li><li>The bias <span>$b$</span> has dimension <span>$C_2$</span>. Again, it does not depend on the position <span>$(i_0,j_0)$</span>.</li></ul><p>The important thing to realize is that the number of parameters does not depend on the size of the image or the hidden layers. For example, even for an input image <span>$500\times 500\times 3$</span>, the convolutional layer contains only 448 parameters for <span>$3\times 3$</span> kernel and <span>$16$</span> output channels (do the computations).</p><p>This results in fixing the two issues mentioned above.</p><ul><li>The number of parameters of convolutional layers stays relatively small.</li><li>Using kernels means that only local information from neighbouring pixels is propagated to subsequent layers.</li></ul><h2 id="Network-structure"><a class="docs-heading-anchor" href="#Network-structure">Network structure</a><a id="Network-structure-1"></a><a class="docs-heading-anchor-permalink" href="#Network-structure" title="Permalink"></a></h2><p>When an input is an image, the usual structure of the neural network is the following:</p><ul><li>Convolutional layer followed by a pooling layer.</li><li>This is repeated many times.</li><li>Flatten layer (it reshapes the three-dimensional tensor into a vector).</li><li>Dense (fully connected) layer.</li><li>Softmax layer.</li><li>Cross-entropy loss function.</li></ul><div class="admonition is-category-bonus"><header class="admonition-header">BONUS: Additional layers</header><div class="admonition-body"><p>Practical convolutional layers involve additional complexities such as layers with even size (we showed only even sizes), padding (should zeros be added or should the output image be smaller) or stride (should there be any distance between convolutions). This goes, however, beyond the lecture.</p><h4>Recurrent layer</h4><p>Recurrent layers are designed to handle one-dimensional data. They are similar to convolutional layers with <span>$J_1=J_2=C_1=C_2=1$</span>. Unlike convolutional layers, they store additional hidden variables. The most-known representative is the long short-term memory (LSTM) cell.</p><h4>Pooling layer</h4><p>The goal of pooling layers is to reduce the size of the network. They take a small (such as <span>$2\times 2$</span>) window and perform a simple operation on this window (such as maximum or mean). Since the pooled windows do not overlap, this reduces the size of each dimension in half. Pooling layers do not have any trainable parameters. </p><h4>Skip connections</h4><p>From the previous lecture, we know that the gradient is computed via the chain rule</p><p class="math-container">\[\nabla f = \nabla f_M\nabla f_{M-1}\dots\nabla f_1.\]</p><p>Since the formula contains multiplication, if any of the gradients is too small, then the whole gradient will be too small. Specifically, the deeper the network, the higher the chance that the initial point will be in a point with a small gradient and the training will progress slowly. This phenomenon is called vanishing gradients.</p><p>To solve the issue with vanishing gradients, skip connections are sometimes added. Even though it is not a layer, we include it here. They do precisely what their name suggests: They skip one or more layers. This makes the network more flexible: Due to its deep structure, it can approximate complicated functions, and due to its shallow structure (because of skip connections), the initial training can be fast.</p></div></div><h2 id="Stochastic-gradient-descent"><a class="docs-heading-anchor" href="#Stochastic-gradient-descent">Stochastic gradient descent</a><a id="Stochastic-gradient-descent-1"></a><a class="docs-heading-anchor-permalink" href="#Stochastic-gradient-descent" title="Permalink"></a></h2><p>We recall that machine learning problems minimize the loss function</p><p class="math-container">\[L(w) = \frac1n\sum_{i=1}^n \operatorname{loss}(y_i, f(w; x_i)).\]</p><p>Its gradient equals to</p><p class="math-container">\[\nabla L(w) = \frac1n\sum_{i=1}^n \operatorname{loss}&#39;(y_i, f(w; x_i))\nabla_w f(w; x_i).\]</p><p>If the dataset contains many samples (<span>$n$</span> is large), then it takes long time to compute the gradient. Therefore, the full gradient is replaced by its stochastic (random) approximation</p><p class="math-container">\[\frac1{|I|}\sum_{i\in I} \operatorname{loss}&#39;(y_i, f(w; x_i))\nabla_w f(w; x_i).\]</p><p>Here, the minibatch<span>$I$</span> is a small (<span>$32, 64, \dots$</span>) subset of all samples <span>$\{1,\dots,n\}$</span>. Sometimes the gradient descent is replaced by other options such as ADAM or RMSprop, which in some way consider the history of gradients.</p><p>This technique is called stochastic gradient descent. During one epoch (the time when the optimizer evaluates each sample once), it performs many gradient updates (unlike the standard gradient descent, which performs only one update). Even though these updates are imprecise, numerical experiments show that stochastic gradient descent is much faster than standard gradient descent. The probable reason is that the entire dataset contains lots of duplicate information, and the full gradient performs unnecessary computation, which slows it down.  </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_10/exercises/">« Exercises</a><a class="docs-footer-nextpage" href="../iris/">Introduction to Flux »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 7 September 2023 12:41">Thursday 7 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
