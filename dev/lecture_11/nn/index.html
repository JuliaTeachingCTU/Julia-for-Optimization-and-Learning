<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>More complex networks · Julia for Optimization and Learning</title><meta name="title" content="More complex networks · Julia for Optimization and Learning"/><meta property="og:title" content="More complex networks · Julia for Optimization and Learning"/><meta property="twitter:title" content="More complex networks · Julia for Optimization and Learning"/><meta name="description" content="Documentation for Julia for Optimization and Learning."/><meta property="og:description" content="Documentation for Julia for Optimization and Learning."/><meta property="twitter:description" content="Documentation for Julia for Optimization and Learning."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Julia for Optimization and Learning</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Setup guide</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/installation/">Install</a></li><li><a class="tocitem" href="../../installation/tutorial/">Project setup</a></li><li><a class="tocitem" href="../../installation/running/">Running Julia</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Basics I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Basics II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/functions/">Function basics</a></li><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_03/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_03/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_03/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_03/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_03/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_04/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_04/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_04/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/structure/">Package structure</a></li><li><a class="tocitem" href="../../lecture_06/workflow/">Development workflow</a></li><li><a class="tocitem" href="../../lecture_06/compatibility/">Package dependencies</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Code organization II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/tests/">Tests</a></li><li><a class="tocitem" href="../../lecture_07/documentation/">Documentation</a></li><li><a class="tocitem" href="../../lecture_07/extensions/">Extensions</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_08/gradients/">Gradients</a></li><li><a class="tocitem" href="../../lecture_08/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_09/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_09/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox" checked/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../iris/">Introduction to Flux</a></li><li class="is-active"><a class="tocitem" href>More complex networks</a><ul class="internal"><li><a class="tocitem" href="#Preparing-data"><span>Preparing data</span></a></li><li><a class="tocitem" href="#Training-and-storing-the-network"><span>Training and storing the network</span></a></li></ul></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/sparse/">Linear regression with sparse constraints</a></li><li><a class="tocitem" href="../../lecture_12/monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../../lecture_12/glm/">Linear regression revisited</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">11: Neural networks II.</a></li><li class="is-active"><a href>More complex networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>More complex networks</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_11/nn.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="More-complex-networks"><a class="docs-heading-anchor" href="#More-complex-networks">More complex networks</a><a id="More-complex-networks-1"></a><a class="docs-heading-anchor-permalink" href="#More-complex-networks" title="Permalink"></a></h1><p>This lecture shows how to train more complex networks using stochastic gradient descent. We will use the MNIST dataset containing 60000 images of digits 0-9. Each image is represented by 28 pixels in each dimension.</p><p><img src="../mnist_intro.svg" alt/></p><h2 id="Preparing-data"><a class="docs-heading-anchor" href="#Preparing-data">Preparing data</a><a id="Preparing-data-1"></a><a class="docs-heading-anchor-permalink" href="#Preparing-data" title="Permalink"></a></h2><p>During the last lecture, we implemented everything from scratch. This lecture will introduce the package <a href="https://fluxml.ai/Flux.jl/stable/models/basics/">Flux</a> (and <a href="https://fluxml.ai/Optimisers.jl/stable/">Optimisers</a>) which automates most of the things needed for neural networks.</p><ul><li>It creates many layers, including convolutional layers.</li><li>It creates the model by chaining layers together.</li><li>It efficiently represents model parameters.</li><li>It automatically computes gradients and trains the model by updating the parameters.</li></ul><p>This functionality requires inputs in a specific format.</p><ul><li>Images must be stored in <code>Float32</code> instead of the commonly used <code>Float64</code> to speed up operations.</li><li>Convolutional layers require that the input has dimension <span>$n_x\times n_y\times n_c\times n_s$</span>, where <span>$(n_x,n_y)$</span> is the number of pixels in each dimension, <span>$n_c$</span> is the number of channels (1 for grayscale, and 3 for coloured images) and <span>$n_s$</span> is the number of samples.</li><li>In general, samples are always stored in the last dimension.</li></ul><p>We use the package <a href="https://juliaml.github.io/MLDatasets.jl/stable/">MLDatasets</a> to load the data.</p><pre><code class="language-julia hljs">using MLDatasets

T = Float32
X_train, y_train = MLDatasets.MNIST(T, :train)[:]
X_test, y_test = MLDatasets.MNIST(T, :test)[:]</code></pre><p>The first two exercises visualize the data and transform it into the correct input shape required by Flux.</p><div class="admonition is-warning" id="Exercise:-231cbc74f2ff435a"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-231cbc74f2ff435a" title="Permalink"></a></header><div class="admonition-body"><p>Plot the first 15 images of the digit 0 from the training set.</p><p><strong>Hint</strong>: The <code>ImageInspector</code> package provides the function <code>imageplot(X_train, inds; nrows=3)</code>, where <code>inds</code> are the desired indices.</p><p><strong>Hint</strong>: To find the correct indices, use the function <code>findall</code>.</p></div></div><details class="admonition is-details" id="Solution:-fce371791bcfd389"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-fce371791bcfd389" title="Permalink"></a></summary><div class="admonition-body"><p>The unique elements in <code>y_train</code> show that it represents the digits.</p><pre><code class="language-julia hljs">unique(y_train)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10-element Vector{Int64}:
 5
 0
 4
 1
 9
 2
 3
 6
 7
 8</code></pre><p>Then we use the <code>findall</code> function to find the indices of the first 15 images of the digit zero.</p><pre><code class="language-julia hljs">inds = findall(y_train .== 0)[1:15]</code></pre><p>We use the <code>imageplot</code> function to plot the images. To invert the colours, we need to call it with <code>1 .- X_train</code> instead of <code>X_train</code>.</p><pre><code class="language-julia hljs">using Plots
using ImageInspector

imageplot(1 .- X_train, inds; nrows=3, size=(800,480))</code></pre></div></details><p><img src="../mnist_intro2.svg" alt/></p><div class="admonition is-warning" id="Exercise:-cff7c587701009b4"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-cff7c587701009b4" title="Permalink"></a></header><div class="admonition-body"><p>Write function <code>reshape_data</code>, which reshapes <code>X_train</code> and <code>X_test</code> into the correct size required by Flux.</p><p><strong>Hint</strong>: The function should work only on inputs with the correct size. This can be achieved by specifying the correct input type <code>X::AbstractArray{&lt;:Real, 3}</code>.</p></div></div><details class="admonition is-details" id="Solution:-c0f107a043c9636b"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-c0f107a043c9636b" title="Permalink"></a></summary><div class="admonition-body"><p>As we have never worked with MLDatasets, we do not know in which format the loading function returns the data.</p><pre><code class="language-julia hljs">typeof(X_train)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Array{Float32, 3}</code></pre><p>The variable <code>X_train</code> stores a three-dimensional array of images.</p><pre><code class="language-julia hljs">size(X_train)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(28, 28, 60000)</code></pre><p>Its size shows that the first two dimensions are the number of pixels and the last dimension are the samples. Since the images are grayscale, the dimension representing channels is missing. We need to add it.</p><pre><code class="language-julia hljs">function reshape_data(X::AbstractArray{&lt;:Real, 3})
    s = size(X)
    return reshape(X, s[1], s[2], 1, s[3])
end</code></pre><p>We specify that the input array has three dimensions via <code>X::AbstractArray{T, 3}</code>. This may prevent surprises when called with different input size.</p></div></details><p>We write now the function <code>load_data</code>, which loads the data and transform it into the correct shape. The keyword argument <code>onehot</code> specifies whether the labels should be converted into their one-hot representation. The <code>dataset</code> keyword specifies which dataset to load. It can be any dataset from the MLDatasets package, or we can even use datasets outside of this package provided that we define the <code>traindata</code> and <code>testdata</code> functions for it.</p><pre><code class="language-julia hljs">using Flux
using Flux: onehotbatch, onecold

function load_data(dataset; T=Float32, onehot=false, classes=0:9)
    X_train, y_train = dataset(T, :train)[:]
    X_test, y_test = dataset(T, :test)[:]

    X_train = reshape_data(X_train)
    X_test = reshape_data(X_test)

    if onehot
        y_train = onehotbatch(y_train, classes)
        y_test = onehotbatch(y_test, classes)
    end

    return X_train, y_train, X_test, y_test
end</code></pre><p>Now we use this function to load the data and modify them into the correct form.</p><pre><code class="language-julia hljs">X_train, y_train, X_test, y_test = load_data(MLDatasets.MNIST; T=T, onehot=true)</code></pre><p>The previous example mentioned that <code>load_data</code> is rather general. The next exercise makes it work for datasets with coloured images.</p><div class="admonition is-warning" id="Exercise:-2a716f5d36124ea4"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-2a716f5d36124ea4" title="Permalink"></a></header><div class="admonition-body"><p>Try to load the CIFAR10 dataset via the <code>load_data</code> function and fix the error in one line of code.</p><p><strong>Hint</strong>: Use <code>dataset = MLDatasets.CIFAR10</code>.</p></div></div><details class="admonition is-details" id="Solution:-98825fc385669ad6"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-98825fc385669ad6" title="Permalink"></a></summary><div class="admonition-body"><p>We first load the data in the same way as before.</p><pre><code class="language-julia hljs">load_data(MLDatasets.CIFAR10; T=T, onehot=true)</code></pre><pre><code class="language-julia hljs">│  MethodError: no method matching reshape_data(::Array{Float32,4})
│  Closest candidates are:
│    reshape_data(::AbstractArray{T,3} where T) where T</code></pre><p>It results in an error which states that the <code>reshape_function</code> function is not defined for inputs with 4 dimensions. We did not implement it because MNIST contains grayscale images, which leads to arrays with 3 dimensions. To fix the problem, it suffices to add a method to the <code>reshape_data</code> function.</p><pre><code class="language-julia hljs">reshape_data(X::AbstractArray{&lt;:Real, 4}) = X</code></pre><p>Now we can load the data.</p><pre><code class="language-julia hljs">typeof(load_data(MLDatasets.CIFAR10; T=T, onehot=true))</code></pre><pre><code class="language-julia hljs">Tuple{Array{Float32,4},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},Array{Float32,4},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}</code></pre><p>We see that it correctly returned a tuple of four items.</p></div></details><h2 id="Training-and-storing-the-network"><a class="docs-heading-anchor" href="#Training-and-storing-the-network">Training and storing the network</a><a id="Training-and-storing-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-and-storing-the-network" title="Permalink"></a></h2><p>We recall that machine learning minimizes the discrepancy between the predictions <span>$\operatorname{predict}(w; x_i)$</span> and labels <span>$y_i$</span>. Mathematically, this amounts to minimizing the following objective function:</p><p class="math-container">\[L(w) = \frac1n\sum_{i=1}^n \operatorname{loss}(y_i, \operatorname{predict}(w; x_i)).\]</p><p>The gradient descent works with the derivative <span>$\nabla L(w)$</span>, which contains the mean over all samples. Since the MNIST training set size is 50000, evaluating one full gradient is costly. For this reason, the gradient is approximated by a mean over a small number of samples. This small set is called a minibatch, and this accelerated method stochastic gradient descent.</p><p>The following exercise splits the dataset into minibatches. While we can do it manually, Flux provides a simple way to do so.</p><div class="admonition is-warning" id="Exercise:-470a89c4ed704cd"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-470a89c4ed704cd" title="Permalink"></a></header><div class="admonition-body"><p>Use the help of the function <code>DataLoader</code> to split the dataset into minibatches.</p><p><strong>Hint</strong>: It needs to be imported from MLUtils package via <code>using MLUtils: DataLoader</code>. Do not forget to install the package first.</p></div></div><details class="admonition is-details" id="Solution:-981336dd146572e1"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-981336dd146572e1" title="Permalink"></a></summary><div class="admonition-body"><p>We first load the function <code>DataLoader</code>.</p><pre><code class="language-julia hljs">using MLUtils: DataLoader</code></pre><p>The in-built help shows us how to call this function. It also includes multiple examples.</p><pre><code class="language-julia hljs">help?&gt; DataLoader
search:

    DataLoader(data; [batchsize, buffer, collate, parallel, partial, rng, shuffle])

An object that iterates over mini-batches of data, each mini-batch containing batchsize observations (except possibly
the last one).

Takes as input a single data array, a tuple (or a named tuple) of arrays, or in general any data object that
implements the numobs and getobs methods.

The last dimension in each array is the observation dimension, i.e. the one divided into mini-batches.

The original data is preserved in the data field of the DataLoader.</code></pre><p>We use the following code to split the dataset into minibatches. We need to include both <code>X_train</code> and <code>y_train</code> to perform the partition for the data and the labels.</p><pre><code class="language-julia hljs">batchsize = 32
batches = DataLoader((X_train, y_train); batchsize, shuffle = true)</code></pre></div></details><div class="admonition is-compat" id="BONUS:-Manually-splitting-the-dataset-bebf1489af70592e"><header class="admonition-header">BONUS: Manually splitting the dataset<a class="admonition-anchor" href="#BONUS:-Manually-splitting-the-dataset-bebf1489af70592e" title="Permalink"></a></header><div class="admonition-body"><p>We can do the same procedure manually. To create minibatches, we create a random partition of all indices <code>randperm(size(y, 2))</code> and use function <code>partition</code> to create an iterator, which creates the minibatches in the form of tuples <span>$(X,y)$</span>.</p><pre><code class="language-julia hljs">using Base.Iterators: partition
using Random

batches = map(partition(randperm(size(y, 2)), batchsize)) do inds
    return (X[:, :, :, inds], y[:, inds])
end</code></pre><p>This procedure is equivalent to the <code>map</code> function.</p><pre><code class="language-julia hljs">[(X[:, :, :, inds], y[:, inds]) for inds in partition(randperm(size(y, 2)), batchsize)]</code></pre><p>The type of <code>batches</code> is a one-dimensional array (vector) of tuples.</p></div></div><p>To build the objective <span>$L$</span>, we first specify the prediction function <span>$\operatorname{predict}$</span>. We keep the usual convention and denote it by model <code>m</code>. It is a composition of seven layers:</p><ul><li>Two convolutional layers extract low-level features from the images.</li><li>Two pooling layers reduce the size of the previous layer.</li><li>One flatten layer converts multi-dimensional arrays into one-dimensional vectors.</li><li>One dense layer is usually applied at the end of the chain.</li><li>One softmax layer is usually the last one and results in probabilities.</li></ul><pre><code class="language-julia hljs">using Random
using Flux: flatten

Random.seed!(666)
m = Chain(
    Conv((2,2), 1=&gt;16, relu),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, relu),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)),
    softmax,
)</code></pre><p>The objective function <span>$L$</span> then applies the cross-entropy loss to the predictions and labels. For us to be able to use <code>Flux.Optimise.train!</code> function to easily train a neural network, we will define the loss <span>$\operatorname{L}$</span> as</p><pre><code class="language-julia hljs">using Flux: crossentropy

L(model, X, y) = crossentropy(model(X), y)</code></pre><p>We now write the function <code>train_model!</code> to train the neural network <code>m</code>. Since this function modifies the input model <code>m</code>, its name should contain the exclamation mark. Besides the loss function <code>L</code>, data <code>X</code> and labels <code>y</code>, it also contains as keyword arguments optimizer the optimizer <code>opt</code>, the minibatch size <code>batchsize</code>, the number of epochs <code>n_epochs</code>, and the file name <code>file_name</code> to which the model should be saved.</p><div class="admonition is-info" id="Optimiser-and-optimiser-state:-fca9696e13168c8e"><header class="admonition-header">Optimiser and optimiser state:<a class="admonition-anchor" href="#Optimiser-and-optimiser-state:-fca9696e13168c8e" title="Permalink"></a></header><div class="admonition-body"><p>Note that we have to initialize the optimiser state <code>opt_state</code>. For a simple gradient descent <code>Descent(learning_rate)</code>, there is no internal state of the optimiser and internal parameters. However, when using different parametrized optimisers such as Adam, the internal state of <code>opt_state</code> is updated in each iteration, just as the parameters of the model. Therefore, if we want to save a model and continue its training later on, we need to save both the model (or its parameters) and the optimiser state.</p></div></div><pre><code class="language-julia hljs">using JLD2

function train_model!(m, L, X, y;
        opt = Descent(0.1),
        batchsize = 128,
        n_epochs = 10,
        file_name = &quot;&quot;)

    opt_state = Flux.setup(opt, m)
    batches = DataLoader((X, y); batchsize, shuffle = true)

    for epoch in 1:n_epochs
        @show epoch
        Flux.train!(L, m, batches, opt_state)
    end

    !isempty(file_name) &amp;&amp; jldsave(file_name; model_state=Flux.state(m) |&gt; cpu)

    return
end</code></pre><p>The function <code>train_model!</code> first splits the datasets into minibatches <code>batches</code> and then uses the optimizer for <code>n_epochs</code> epochs. In one epoch, the model <code>m</code> evaluates all samples exactly once. Therefore, the optimizer performs the same number of gradient updates as the number of minibatches during one epoch. On the other hand, the standard gradient descent makes only one gradient update during one epoch. The default optimizer is the stochastic gradient descent with stepsize <span>$0.1$</span>. Since we do not need an index in the loop, we use <code>_</code>. Finally, if <code>file_name</code> is non-empty, the function saves the trained model <code>m</code>.</p><div class="admonition is-warning" id="Exercise:-c519a4057cc3ae78"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-c519a4057cc3ae78" title="Permalink"></a></header><div class="admonition-body"><p>Train the model for one epoch and save it to <code>MNIST_simple.jld2</code>. Print the accuracy on the testing set.</p></div></div><details class="admonition is-details" id="Solution:-2b19f5598609f501"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-2b19f5598609f501" title="Permalink"></a></summary><div class="admonition-body"><p>To train the model, it suffices to call the previously written function.</p><pre><code class="language-julia hljs">file_name = &quot;mnist_simple.jld2&quot;
train_model!(m, L, X_train, y_train; n_epochs=1, file_name=file_name)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">epoch = 1</code></pre><p>The accuracy has been computed many times during the course.</p><pre><code class="language-julia hljs">using Statistics

accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))

&quot;Test accuracy = &quot; * string(accuracy(X_test, y_test))</code></pre><p>We defined <code>accuracy</code> in a different way than before. Can you spot the difference and explain why they are equivalent?</p></div></details><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Test accuracy = 0.9435</code></pre><p>The accuracy is over 93%, which is not bad for training for one epoch only. Let us recall that training for one epoch means that the classifier evaluates each sample only once. To obtain better accuracy, we need to train the model for more epochs. Since that may take some time, it is not good to train the same model repeatedly. The following exercise determines automatically whether the trained model already exists. If not, it trains it.</p><div class="admonition is-warning" id="Exercise:-46ef0c6b346e26b7"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-46ef0c6b346e26b7" title="Permalink"></a></header><div class="admonition-body"><p>Write a function <code>train_or_load!(file_name, m, args...; ???)</code> checking whether the file <code>file_name</code> exists.</p><ul><li>If it exists, it loads it and then copies model state into <code>m</code> using the function <code>Flux.loadmodel!</code>.</li><li>If it does not exist, it trains it using <code>train_model!</code>.</li></ul><p>In both cases, the model <code>m</code> should be modified inside the <code>train_or_load!</code> function. Pay special attention to the optional arguments <code>???</code>.</p><p>Use this function to load the model from <code>data/mnist.bson</code> and evaluate the performance at the testing set.</p></div></div><details class="admonition is-details" id="Solution:-5a70f2b114896a66"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-5a70f2b114896a66" title="Permalink"></a></summary><div class="admonition-body"><p>The optional arguments should contain <code>kwargs...</code>, which will be passed to <code>train_model!</code>. Besides that, we include <code>force</code> which enforces that the model is trained even if it already exists.</p><p>First, we should check whether the directory exists <code>!isdir(dirname(file_name))</code> and if not, we create it <code>mkpath(dirname(file_name))</code>. Then we check whether the file exists (or whether we want to enforce the training). If yes, we train the model, which already modifies <code>m</code>. If not, we load model state using <code>JLD2.load(file_name, &quot;model_state&quot;)</code> and copy the loaded state into <code>m</code> by <code>Flux.loadmodel!(m, model_state)</code>.</p><pre><code class="language-julia hljs">function train_or_load!(file_name, m, args...; force=false, kwargs...)

    !isdir(dirname(file_name)) &amp;&amp; mkpath(dirname(file_name))

    if force || !isfile(file_name)
        train_model!(m, args...; file_name=file_name, kwargs...)
    else
        model_state = JLD2.load(file_name, &quot;model_state&quot;)
        Flux.loadmodel!(m, model_state)
    end
end</code></pre><p>To load the model, we should use <code>joinpath</code> to be compatible with all operating systems. The accuracy is evaluated as before.</p><pre><code class="language-julia hljs">file_name = joinpath(&quot;data&quot;, &quot;mnist.jld2&quot;)
train_or_load!(file_name, m, L, X_train, y_train)

&quot;Test accuracy = &quot; * string(accuracy(X_test, y_test))</code></pre></div></details><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Test accuracy = 0.9775</code></pre><p>The externally trained model has an accuracy of almost 98% (it has the same architecture as the one defined above, but it was trained for 100 epochs.). Even though there are perfect models (with accuracy 100%) on MNIST, we are happy with this result. We will perform further analysis of the network in the exercises.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../iris/">« Introduction to Flux</a><a class="docs-footer-nextpage" href="../exercises/">Exercises »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 10 December 2025 09:48">Wednesday 10 December 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
