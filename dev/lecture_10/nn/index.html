<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>More complex networks · Numerical computing in Julia</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Numerical computing in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Numerical computing in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit">Numerical computing in Julia</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why_julia/">Why Julia?</a></li><li><a class="tocitem" href="../../howto/">How to...</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Mathematical operations and Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluation</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of Variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception Handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/basics/">Package management</a></li><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">5: Composite types and constructors</span></li><li><span class="tocitem">6: Modules and enviroments</span></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Theory of continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/gradients/">Visualization of gradients</a></li><li><a class="tocitem" href="../../lecture_07/numerical_methods/">Numerical methods</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox" checked/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li class="is-active"><a class="tocitem" href>More complex networks</a><ul class="internal"><li><a class="tocitem" href="#Loading-data"><span>Loading data</span></a></li><li><a class="tocitem" href="#Visualization-of-images"><span>Visualization of images</span></a></li><li><a class="tocitem" href="#Training-and-storing-the-network"><span>Training and storing the network</span></a></li></ul></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><span class="tocitem">11: Ordinary differential equations</span></li><li><span class="tocitem">12: Statistics I.</span></li><li><span class="tocitem">13: Statistics II.</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">10: Neural networks II.</a></li><li class="is-active"><a href>More complex networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>More complex networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_10/nn.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="More-complex-networks"><a class="docs-heading-anchor" href="#More-complex-networks">More complex networks</a><a id="More-complex-networks-1"></a><a class="docs-heading-anchor-permalink" href="#More-complex-networks" title="Permalink"></a></h1><p>This section will show how to train more complex networks using the stochastic gradient descent. We will also use the more complicated MNIST dataset which contains 60000 samples of the 0-9 digits.</p><p>As always, we start with the seed </p><pre><code class="language-julia">using Random

Random.seed!(666)</code></pre><h2 id="Loading-data"><a class="docs-heading-anchor" href="#Loading-data">Loading data</a><a id="Loading-data-1"></a><a class="docs-heading-anchor-permalink" href="#Loading-data" title="Permalink"></a></h2><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>The convolutional layers in Flux require that the inpit has dimension <span>$n_x\times n_y\times n_c\times n_s$</span>, where <span>$(n_x,n_y)$</span> is the number of pixels in each dimension, <span>$n_c$</span> is the number of channels (1 for grayscale images and 3 for coloured images) and <span>$n_s$</span> is the number of samples. The simplest way to load the dataset is to use the MLDatasets package via <code>MLDatasets.MNIST.traindata(T)</code>, where <code>T</code> is a given type (can be empty).</p><p>Write a function <code>load_data</code> which loads the data and transforms it into a correct shape. Do not forgot to transform the labels into the one-hot representation, which can be done by using the <code>onehotbatch</code> function from Flux.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>First we load the packages, define the desired type to be <code>Float32</code> and select the dataset the be MNIST. Working with a general <code>dataset</code> has the advantage that it is simple to modify the code if we want to work with a different dataset such as FashionMNIST and CIFAR.</p><pre><code class="language-julia">using MLDatasets
using Flux
using Flux: onehotbatch, onecold

T = Float32
dataset = MLDatasets.MNIST</code></pre><p>Since we have never worked with MLDatasets, we do not know in which format the loading function returns the data. For this reason, we check that</p><pre><code class="language-julia">typeof(dataset.traindata(T))</code></pre><pre class="documenter-example-output">Tuple{Array{Float32,3},Array{Int64,1}}</pre><p>is a tuple where its first part are the data and the second one are the labels. Performing one more check</p><pre><code class="language-julia">size(dataset.traindata(T)[1])</code></pre><pre class="documenter-example-output">(28, 28, 60000)</pre><p>shows that the channel is missing. For this reason, we need to add the channel by</p><pre><code class="language-julia">function reshape_data(X::AbstractArray{T, 3}, y::AbstractVector) where T
    s = size(X)
    return reshape(X, s[1], s[2], 1, s[3]), reshape(y, 1, :)
end</code></pre><p>To prevent unexpected surprises, we specify that the data have only three dimensions via <code>X::AbstractArray{T, 3}</code>.</p><p>Now we can write the loading function. It is similar is the one we have already written. Pay attention to the line <code>dataset.traindata(T)...</code>. It would be possible to use two arguments <code>dataset.traindata(T)[1]</code> and <code>dataset.traindata(T)[2]</code>. However, this would load the data two times. Line <code>y_train = T.(y_train)</code> should not be necessary as we specify <code>T</code> already in <code>traindata(T)</code>. We include the optional parameter <code>onehot</code>.</p><pre><code class="language-julia">function load_data(dataset; onehot=false, T=Float32)
    classes = 0:9
    X_train, y_train = reshape_data(dataset.traindata(T)...)
    X_test, y_test = reshape_data(dataset.testdata(T)...)
    y_train = T.(y_train)
    y_test = T.(y_test)

    if onehot
        y_train = onehotbatch(y_train[:], classes)
        y_test = onehotbatch(y_test[:], classes)
    end

    return X_train, y_train, X_test, y_test
end</code></pre><p>Now we load the data by</p><pre><code class="language-julia">X_train, y_train, X_test, y_test = load_data(dataset; T=T, onehot=true)</code></pre></p></details><p>The previous example is rather general. Only small modifications are needed for other datasets.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Try to load the CIFAR10 dataset (<code>dataset = MLDatasets.CIFAR10</code>) and fix the error in one line of code.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We try to load the data in the same way as before</p><pre><code class="language-julia">load_data(MLDatasets.CIFAR10; T=T, onehot=true)</code></pre><p>It results in an error</p><pre><code class="language-julia">│  MethodError: no method matching reshape_data(::Array{Float32,4}, ::Array{Int64,1})
│  Closest candidates are:
│    reshape_data(::AbstractArray{T,3}, ::AbstractArray{T,1} where T) where T</code></pre><p>We see that the problem is that we defined <code>reshape_data</code> only for input arrays of dimension 3 but since CIFAR contains coloured images, it has 4 dimensions. We therefore need to add more method for the function <code>reshape_data</code></p><pre><code class="language-julia">reshape_data(X::AbstractArray{T, 4}, y::AbstractVector) where T = (X, reshape(y, 1, :))</code></pre><p>Now we can load the data</p><pre><code class="language-julia">typeof(load_data(MLDatasets.CIFAR10; T=T, onehot=true))</code></pre><pre><code class="language-julia">Tuple{Array{Float32,4},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},Array{Float32,4},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}</code></pre><p>We see that it returned correct tuple of four items.</p></p></details><h2 id="Visualization-of-images"><a class="docs-heading-anchor" href="#Visualization-of-images">Visualization of images</a><a id="Visualization-of-images-1"></a><a class="docs-heading-anchor-permalink" href="#Visualization-of-images" title="Permalink"></a></h2><p>When working with data, it is always good to have some understanding for the dataset. Since MNIST is a dataset of images, the simplest way of understanding is plotting them.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Write a function <code>plot_image</code> which plots the input image. Since we work with grayscale images, the simplest way to plot is to use the <code>plot</code> function after converting all pixels to <code>Gray</code> type via a function of the same name from the Colours package.</p><p>Plot the third image from the training set and check that the label is correct. To do so, you will need our previously written <code>onecold</code> function or you can use the one from the Flux package.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To plot an image, we convert it into grayscale by <code>Gray.</code> We not the use the dot notation since the input is a matrix and we need to apply the operator to all of its entries. Since we are not interested in axis, we turn them off by <code>axis=nothing</code> argument to the <code>plot</code> function. Note that we need to transpose the input, otherwise the image would bo rotated.</p><pre><code class="language-julia">using Plots

plot_image(x::AbstractArray{T, 2}) where T = plot(Gray.(x&#39;), axis=nothing)</code></pre><p>To make sure that <code>plot_image</code> works even if we call it with an input with higher number of dimensions, we write these functions.</p><pre><code class="language-julia">function plot_image(x::AbstractArray{T, 4}) where T
    @assert size(x,4) == 1
    plot_image(x[:,:,:,1])
end

function plot_image(x::AbstractArray{T, 3}) where T
    @assert size(x,3) == 1
    plot_image(x[:,:,1])
end</code></pre><p>Plotting the image is then simple</p><pre><code class="language-julia">i = 3
plot_image(X_train[:,:,:,i])</code></pre><p>For the correct label, we need to specify the classes <code>0:9</code>. If we do not specify them, Flux will assign number 1 to 10 instead of correct 0 to 9 and the result will be larger by one</p><pre><code class="language-julia">onecold(y_train[:,i], 0:9)</code></pre></p></details><p>The correct answer is</p><pre class="documenter-example-output">4</pre><p><img src="../MNIST.svg" alt/></p><h2 id="Training-and-storing-the-network"><a class="docs-heading-anchor" href="#Training-and-storing-the-network">Training and storing the network</a><a id="Training-and-storing-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-and-storing-the-network" title="Permalink"></a></h2><p>We write a function <code>train_model!</code> for training the network. Since it modifies the input model <code>m</code>, its name should contain the exclamation mark. Besides the data <code>X</code> and labels <code>y</code>, it also contains optional arguments optimizer <code>opt</code>, batch size <code>batch_size</code>, number of epochs <code>n_epochs</code> and the file name <code>file_name</code> to which the model should be saved.</p><pre><code class="language-julia">using Base.Iterators: partition
using Flux: crossentropy
using BSON

function train_model!(m, X, y;
        opt=ADAM(0.001),
        batch_size=128,
        n_epochs=10,
        file_name=&quot;&quot;)

    loss(x, y) = crossentropy(m(x), y)

    batches_train = map(partition(randperm(size(y, 2)), batch_size)) do inds
        return (X[:, :, :, inds], y[:, inds])
    end

    for _ in 1:n_epochs
        Flux.train!(loss, params(m), batches_train, opt)
    end

    !isempty(file_name) &amp;&amp; BSON.bson(file_name, m=m)

    return
end</code></pre><p>It starts with the loss function. On the contrary to the models used before, it uses the stochastic gradient descent instead of the (full) gradient descent. The reason is that the MNIST contains 50000 samples and the computation of the full gradient would be too costly.</p><p>To create minibatches, we create first a random partion of all indices <code>randperm(size(y, 2))</code> and then use the function <code>partition</code> to create an iterator for which we then create the tuple of <span>$(X,y)$</span> on a minibatch</p><pre><code class="language-julia">batches_train = map(partition(randperm(size(y, 2)), batch_size)) do inds
    return (X[:, :, :, inds], y[:, inds])
end</code></pre><p>The equivalent formulation without the <code>map</code> function is the following</p><pre><code class="language-julia">batches_train = [(X[:, inds], y[:, inds]) for inds in partition(randperm(size(y, 2)), batch_size)]</code></pre><p>The type of <code>batches_train</code> is one-dimensional array (vector) of tuples</p><pre><code class="language-julia">Array{Tuple{Array{Int64,2},Array{Float64,2}},1}</code></pre><p>This allows us to call the <code>train!</code> function, which computes the gradients on all minibatches and performs the same number of gradient updates. Since <code>train!</code> looked at every sample exactly once,</p><pre><code class="language-julia">Flux.train!(loss, params(m), batches_train, opt)</code></pre><p>performs one training epoch. Computationally, this is roughly equivalent to one full gradient update but this line of code performed as many gradient updates as there are minibatches. Therefore, we train for <code>n_epoch</code> epochs by</p><pre><code class="language-julia">for _ in 1:n_epochs
    Flux.train!(loss, params(m), batches_train, opt)
end</code></pre><p>As we do need the index in the for loop, we use <code>_</code>. The last line saves the model whenever the file name is non-empty.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Train the model </p><pre><code class="language-julia">m = Chain(
    Conv((2,2), 1=&gt;16, relu),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, relu),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)), softmax)</code></pre><p>for one epoch and save it into the file <code>MNIST_simple.bson</code>. Print the accuracy of the model on the testing set.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To train the model, it suffices to call the previously written function</p><pre><code class="language-julia">file_name = &quot;mnist_simple.bson&quot;
train_model!(m, X_train, y_train; n_epochs=1, file_name=file_name)</code></pre><p>The accuracy has been computed many times during the course</p><pre><code class="language-julia">using Statistics

accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))

&quot;Test accuracy = &quot; * string(accuracy(X_test, y_test))</code></pre></p></details><pre class="documenter-example-output">Test accuracy = 0.9251</pre><p>The accuracy is over 92%, which is not bad for training for one epoch only. Let us recall that training for one epoch means that the classifier evaluates each sample only once. To obtain a better accuracy, we need to train the model for more epochs. Since that may take some time, it is not good to train the same model again and again. The next exercise determines automatically whether the trained model already exists. If not, it trains the model. If yes, it loads it without any training. Then it loads a model which was trained externally for 50 epochs.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Write a function <code>train_or_load!(file_name, m, X, y; ???)</code> which checks whether the file at <code>file_name</code> exists.</p><ul><li>If it exists, it loads it and then copies its parameters into <code>m</code> using the function <code>Flux.loadparams!</code></li><li>If it does not exists, it trains it using <code>train_model!</code>.</li></ul><p>In both cases, the model <code>m</code> should be modified inside the <code>train_or_load!</code> function. Pay special attention to the optional arguments <code>???</code>. </p><p>Load the model at <code>data/mnist.bson</code> and evaluate the performance at the testing set.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The optional argument need to contain <code>kwargs...</code>, which will be passed to <code>train_model!</code>. Besides that, we include <code>force</code> which enforces training the model even if the file exists (in which case, it will overwrite it). </p><p>First, we should check whether the directory exists <code>!isdir(dirname(file_name))</code> and if not, we will create <code>mkpath(dirname(file_name))</code>. Then we check whether the file exists (or whether we want to enforce the training). If yes, we train the model, which already modifies <code>m</code>. If not, we <code>BSON.load</code> the model and copy the loaded parameters into <code>m</code> by <code>Flux.loadparams!(m, params(m_loaded))</code>. We cannot load directly into <code>m</code> instead of <code>m_loaded</code> because that would create a local copy of <code>m</code> and the function would not modify the external <code>m</code>.</p><pre><code class="language-julia">function train_or_load!(file_name, m, X, y; force=false, kwargs...)

    !isdir(dirname(file_name)) &amp;&amp; mkpath(dirname(file_name))

    if force || !isfile(file_name)
        train_model!(m, X, y; file_name=file_name, kwargs...)
    else
        m_loaded = BSON.load(file_name)[:m]
        Flux.loadparams!(m, params(m_loaded))
    end
end</code></pre><p>For the loading of the model, we should use <code>joinpath</code> to be compatible with all operating systems. The accuracy is evaluated as before.</p><pre><code class="language-julia">file_name = joinpath(&quot;data&quot;, &quot;mnist.bson&quot;)
train_or_load!(file_name, m, X_train, y_train)

&quot;Test accuracy = &quot; * string(accuracy(X_test, y_test))</code></pre></p></details><pre class="documenter-example-output">Test accuracy = 0.9815</pre><p>The externall trained model has the accuracy of more 98%. Even though there are perfect models (with accuracy 100%) on MNIST, we are hapy with this results. We will perform a further analysis of the network in the exercises. We will also learn how to train the network using GPUs instead of CPUs. Even though this is extremely important to reduce time, we omit it here because some participants of the course may not have a compatible GPU for training.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../theory/">« Theory of neural networks</a><a class="docs-footer-nextpage" href="../exercises/">Exercises »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 6 January 2021 08:45">Wednesday 6 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
