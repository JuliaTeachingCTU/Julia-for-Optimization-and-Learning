<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>More complex networks · Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Machine Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Machine Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox" checked/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li class="is-active"><a class="tocitem" href>More complex networks</a><ul class="internal"><li><a class="tocitem" href="#Loading-data"><span>Loading data</span></a></li><li><a class="tocitem" href="#Visualization-of-images"><span>Visualization of images</span></a></li><li><a class="tocitem" href="#Training-and-storing-the-network"><span>Training and storing the network</span></a></li></ul></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Statistics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">10: Neural networks II.</a></li><li class="is-active"><a href>More complex networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>More complex networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_10/nn.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="More-complex-networks"><a class="docs-heading-anchor" href="#More-complex-networks">More complex networks</a><a id="More-complex-networks-1"></a><a class="docs-heading-anchor-permalink" href="#More-complex-networks" title="Permalink"></a></h1><p>This section will show how to train more complex networks using stochastic gradient descent. We will also use the more complicated MNIST dataset which contains 60000 images of digits 0-9.</p><p>As always, we start with the seed </p><pre><code class="language-julia">using Random

Random.seed!(666)</code></pre><h2 id="Loading-data"><a class="docs-heading-anchor" href="#Loading-data">Loading data</a><a id="Loading-data-1"></a><a class="docs-heading-anchor-permalink" href="#Loading-data" title="Permalink"></a></h2><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>The convolutional layers in Flux require that the input has dimension <span>$n_x\times n_y\times n_c\times n_s$</span>, where <span>$(n_x,n_y)$</span> is the number of pixels in each dimension, <span>$n_c$</span> is the number of channels (1 for grayscale, and 3 for coloured images) and <span>$n_s$</span> is the number of samples. The simplest way to load the dataset is to use the MLDatasets package via <code>MLDatasets.MNIST.traindata(T)</code>, where <code>T</code> is a given type (can be empty).</p><p>Write function <code>load_data</code> which loads the data and transforms it into the correct size. Do not forgot to transform the labels into the one-hot representation, which can be done by using the <code>onehotbatch</code> function from Flux.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To load data, we define the desired type to be <code>Float32</code>, and select the dataset to be MNIST. Working with a general <code>dataset</code> has the advantage that it is simple to modify the code if we want to work with a different dataset such as FashionMNIST or CIFAR.</p><pre><code class="language-julia">using MLDatasets
using Flux
using Flux: onehotbatch, onecold

T = Float32
dataset = MLDatasets.MNIST</code></pre><p>As we have never worked with MLDatasets, we do not know in which format the loading function returns the data. For this reason, we check that</p><pre><code class="language-julia">typeof(dataset.traindata(T))</code></pre><pre class="documenter-example-output">Tuple{Array{Float32,3},Array{Int64,1}}</pre><p>is a tuple of the data and the labels. Performing one more check</p><pre><code class="language-julia">size(dataset.traindata(T)[1])</code></pre><pre class="documenter-example-output">(28, 28, 60000)</pre><p>shows that the channels are missing. For this reason, we need to add them by</p><pre><code class="language-julia">function reshape_data(X::AbstractArray{T, 3}, y::AbstractVector) where T
    s = size(X)
    return reshape(X, s[1], s[2], 1, s[3]), reshape(y, 1, :)
end</code></pre><p>To prevent unexpected surprises, we specify that the data have only three dimensions via <code>X::AbstractArray{T, 3}</code>.</p><p>Now we can write the loading function. It is similar to the one we have already written. Pay attention to the line <code>dataset.traindata(T)...</code>. It would be possible to use two arguments <code>dataset.traindata(T)[1]</code> and <code>dataset.traindata(T)[2]</code>. However, this would load the data two times. Line <code>y_train = T.(y_train)</code> should not be necessary as we specify <code>T</code> already in <code>traindata(T)</code>. We include the optional parameter <code>onehot</code>.</p><pre><code class="language-julia">function load_data(dataset; T=Float32, onehot=false, classes=0:9)
    X_train, y_train = reshape_data(dataset.traindata(T)...)
    X_test, y_test = reshape_data(dataset.testdata(T)...)
    y_train = T.(y_train)
    y_test = T.(y_test)

    if onehot
        y_train = onehotbatch(y_train[:], classes)
        y_test = onehotbatch(y_test[:], classes)
    end

    return X_train, y_train, X_test, y_test
end</code></pre><p>Now we load the data by</p><pre><code class="language-julia">X_train, y_train, X_test, y_test = load_data(dataset; T=T, onehot=true)</code></pre></p></details><p>The previous example is rather general. Only small modifications are needed for other datasets.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Try to load the CIFAR10 dataset (<code>dataset = MLDatasets.CIFAR10</code>) and fix the error in one line of code.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We try to load the data in the same way as before</p><pre><code class="language-julia">load_data(MLDatasets.CIFAR10; T=T, onehot=true)</code></pre><p>It results in an error</p><pre><code class="language-julia">│  MethodError: no method matching reshape_data(::Array{Float32,4}, ::Array{Int64,1})
│  Closest candidates are:
│    reshape_data(::AbstractArray{T,3}, ::AbstractArray{T,1} where T) where T</code></pre><p>We see that the problem is that we defined <code>reshape_data</code> only for input arrays of dimension 3 but since CIFAR contains coloured images, it has 4 dimensions. We, therefore, need to add more method for the function <code>reshape_data</code></p><pre><code class="language-julia">reshape_data(X::AbstractArray{T, 4}, y::AbstractVector) where T = (X, reshape(y, 1, :))</code></pre><p>Now we can load the data</p><pre><code class="language-julia">typeof(load_data(MLDatasets.CIFAR10; T=T, onehot=true))</code></pre><pre><code class="language-julia">Tuple{Array{Float32,4},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},Array{Float32,4},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}</code></pre><p>We see that it correctly returned a tuple of four items.</p></p></details><h2 id="Visualization-of-images"><a class="docs-heading-anchor" href="#Visualization-of-images">Visualization of images</a><a id="Visualization-of-images-1"></a><a class="docs-heading-anchor-permalink" href="#Visualization-of-images" title="Permalink"></a></h2><p>When working with data, it is always good to have some understanding for them. Since MNIST is a dataset of images, the simplest way of understanding is plotting them.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Write a function <code>plot_image</code> which plots the input image. Since we work with grayscale images, the simplest way to plot is to use the <code>plot</code> function after converting all pixels to <code>Gray</code> type via a function of the same name, which is included in the Plots package.</p><p>Plot the third image from the training set and check that the label is correct. To do so, you will need our previously written <code>onecold</code> function or you can use the one from the Flux package.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To plot an image, we convert it into grayscale by <code>Gray</code>. We use the dot notation because the input is a matrix, and we need to apply the operator to all of its entries. Since we are not interested in the axis, we turn them off by <code>axis=false</code> and <code>ticks=false</code>. Note that we need to transpose the input; otherwise, the image would be rotated. We also use <code>1 .-x</code> to invert the black and white colours.</p><pre><code class="language-julia">using Plots

plot_image(x::AbstractArray{T, 2}) where T = plot(Gray.(1 .-x&#39;), axis=false, ticks=false)</code></pre><p>To make sure that <code>plot_image</code> works even if we call it with an input with three  dimensions, we add one more function.</p><pre><code class="language-julia">function plot_image(x::AbstractArray{T, 3}) where T
    size(x,3) == 1 || error(&quot;Image is not grayscale.&quot;)
    plot_image(x[:,:,1])
end</code></pre><p>Plotting the image is then simple. Note that this code calls the <code>plot_image(x::AbstractArray{T, 3})</code> which performs the check whether the image is grayscale and then calls the <code>plot_image(x::AbstractArray{T, 2})</code> function.</p><pre><code class="language-julia">i = 3
plot_image(X_train[:,:,:,i])</code></pre><p>For the correct label, we need to specify the classes <code>0:9</code>. If we do not specify them, Flux will assign numbers 1 to 10 instead of correct 0 to 9, and the result will be shifted by one</p><pre><code class="language-julia">onecold(y_train[:,i], 0:9)</code></pre></p></details><p>The correct answer is</p><pre class="documenter-example-output">4</pre><p><img src="../MNIST.svg" alt/></p><h2 id="Training-and-storing-the-network"><a class="docs-heading-anchor" href="#Training-and-storing-the-network">Training and storing the network</a><a id="Training-and-storing-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-and-storing-the-network" title="Permalink"></a></h2><p>To train the network, we will now write the function <code>train_model!</code>. Since it modifies the input model <code>m</code>, its name should contain the exclamation mark. Besides data <code>X</code> and labels <code>y</code>, it also contains as optional arguments optimizer the <code>opt</code>, the minibatch size <code>batch_size</code>, the number of epochs <code>n_epochs</code>, and the file name <code>file_name</code> to which the model should be saved.</p><pre><code class="language-julia">using Base.Iterators: partition
using Flux: crossentropy
using BSON

function train_model!(m, X, y;
        opt=ADAM(0.001),
        batch_size=128,
        n_epochs=10,
        file_name=&quot;&quot;)

    loss(x, y) = crossentropy(m(x), y)

    batches_train = map(partition(randperm(size(y, 2)), batch_size)) do inds
        return (X[:, :, :, inds], y[:, inds])
    end

    for _ in 1:n_epochs
        Flux.train!(loss, params(m), batches_train, opt)
    end

    !isempty(file_name) &amp;&amp; BSON.bson(file_name, m=m)

    return
end</code></pre><p>It starts with the <code>crossentropy</code> loss function, which needs to be loaded from the Flux package by <code>using Flux: crossentropy</code>.</p><p>On the contrary to the models used before, it uses stochastic gradient descent instead of gradient descent. The reason is that the MNIST training set contains 50000 samples, and the computation of the full gradient would be too costly. To create minibatches, we create a random partion of all indices <code>randperm(size(y, 2))</code>, and use function <code>partition</code> to create an iterator, which creates the minibatches in the form of tuples <span>$(X,y)$</span>.</p><pre><code class="language-julia">batches_train = map(partition(randperm(size(y, 2)), batch_size)) do inds
    return (X[:, :, :, inds], y[:, inds])
end</code></pre><p>The equivalent formulation without the <code>map</code> function would be</p><pre><code class="language-julia">batches_train = [(X[:, inds], y[:, inds]) for inds in partition(randperm(size(y, 2)), batch_size)]</code></pre><p>The type of <code>batches_train</code> is one-dimensional array (vector) of tuples</p><pre><code class="language-julia">Array{Tuple{Array{Int64,2},Array{Float64,2}},1}</code></pre><p>This allows us to call the <code>train!</code> function, which computes the gradients on all minibatches and performs the same number of gradient updates as the number of minibatches. Since <code>train!</code> looked at every sample exactly once,</p><pre><code class="language-julia">Flux.train!(loss, params(m), batches_train, opt)</code></pre><p>performs one training epoch. Computationally, this is roughly equivalent to one full gradient update, but this line of code performed as many gradient updates as there are minibatches. Therefore, we train for <code>n_epoch</code> epochs by</p><pre><code class="language-julia">for _ in 1:n_epochs
    Flux.train!(loss, params(m), batches_train, opt)
end</code></pre><p>As we do not need the index in the for loop, we use <code>_</code>. The last line saves the model whenever the file name is non-empty.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Train the model </p><pre><code class="language-julia">m = Chain(
    Conv((2,2), 1=&gt;16, relu),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, relu),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)),
    softmax,
)</code></pre><p>for one epoch and save it into the file <code>MNIST_simple.bson</code>. Print the accuracy of the model on the testing set.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To train the model, it suffices to call the previously written function</p><pre><code class="language-julia">file_name = &quot;mnist_simple.bson&quot;
train_model!(m, X_train, y_train; n_epochs=1, file_name=file_name)</code></pre><p>The accuracy has been computed many times during the course</p><pre><code class="language-julia">using Statistics

accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))

&quot;Test accuracy = &quot; * string(accuracy(X_test, y_test))</code></pre><p>We defined <code>accuracy</code> in a different way than before. Can you spot the difference and explain why they are equivalent?</p></p></details><pre class="documenter-example-output">Test accuracy = 0.9251</pre><p>The accuracy is over 92%, which is not bad for training for one epoch only. Let us recall that training for one epoch means that the classifier evaluates each sample only once. To obtain better accuracy, we need to train the model for more epochs. Since that may take some time, it is not good to train the same model again and again. The next exercise determines automatically whether the trained model already exists. If not, it trains it.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Write a function <code>train_or_load!(file_name, m, X, y; ???)</code> which checks whether the file <code>file_name</code> exists.</p><ul><li>If it exists, it loads it and then copies its parameters into <code>m</code> using the function <code>Flux.loadparams!</code></li><li>If it does not exist, it trains it using <code>train_model!</code>.</li></ul><p>In both cases, the model <code>m</code> should be modified inside the <code>train_or_load!</code> function. Pay special attention to the optional arguments <code>???</code>. </p><p>Load the model from <code>data/mnist.bson</code> and evaluate the performance at the testing set.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The optional arguments should contain <code>kwargs...</code>, which will be passed to <code>train_model!</code>. Besides that, we include <code>force</code> which enforces that the model is trained even if it already exists (in which case, it will overwrite it). </p><p>First, we should check whether the directory exists <code>!isdir(dirname(file_name))</code> and if not, we create it <code>mkpath(dirname(file_name))</code>. Then we check whether the file exists (or whether we want to enforce the training). If yes, we train the model, which already modifies <code>m</code>. If not, we <code>BSON.load</code> the model and copy the loaded parameters into <code>m</code> by <code>Flux.loadparams!(m, params(m_loaded))</code>. We cannot load directly into <code>m</code> instead of <code>m_loaded</code> because that would create a local copy of <code>m</code> and the function would not modify the external <code>m</code>.</p><pre><code class="language-julia">function train_or_load!(file_name, m, X, y; force=false, kwargs...)

    !isdir(dirname(file_name)) &amp;&amp; mkpath(dirname(file_name))

    if force || !isfile(file_name)
        train_model!(m, X, y; file_name=file_name, kwargs...)
    else
        m_loaded = BSON.load(file_name)[:m]
        Flux.loadparams!(m, params(m_loaded))
    end
end</code></pre><p>To load the model, we should use <code>joinpath</code> to be compatible with all operating systems. The accuracy is evaluated as before.</p><pre><code class="language-julia">file_name = joinpath(&quot;data&quot;, &quot;mnist.bson&quot;)
train_or_load!(file_name, m, X_train, y_train)

&quot;Test accuracy = &quot; * string(accuracy(X_test, y_test))</code></pre></p></details><pre class="documenter-example-output">Test accuracy = 0.9815</pre><p>The externally trained model has the accuracy of more than 98% (it has the same architecture as the one defined above, but it was trained for 50 epochs.). Even though there are perfect models (with accuracy 100%) on MNIST, we are happy with this result. We will perform further analysis of the network in the exercises.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../theory/">« Theory of neural networks</a><a class="docs-footer-nextpage" href="../exercises/">Exercises »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 26 March 2021 20:39">Friday 26 March 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
