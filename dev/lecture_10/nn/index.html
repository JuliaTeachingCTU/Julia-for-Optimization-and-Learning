<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>More complex networks · Julia for Optimization and Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Optimization and Learning</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox" checked/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../iris/">Introduction to Flux</a></li><li class="is-active"><a class="tocitem" href>More complex networks</a><ul class="internal"><li><a class="tocitem" href="#Preparing-data"><span>Preparing data</span></a></li><li><a class="tocitem" href="#Training-and-storing-the-network"><span>Training and storing the network</span></a></li></ul></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/sparse/">Linear regression with sparse constraints</a></li><li><a class="tocitem" href="../../lecture_11/monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../../lecture_11/glm/">Linear regression revisited</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">10: Neural networks II.</a></li><li class="is-active"><a href>More complex networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>More complex networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_10/nn.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="More-complex-networks"><a class="docs-heading-anchor" href="#More-complex-networks">More complex networks</a><a id="More-complex-networks-1"></a><a class="docs-heading-anchor-permalink" href="#More-complex-networks" title="Permalink"></a></h1><p>This lecture shows how to train more complex networks using stochastic gradient descent. We will use the MNIST dataset containing 60000 images of digits 0-9. Each image is represented by 28 pixels in each dimension.</p><p><img src="../mnist_intro.svg" alt/></p><h2 id="Preparing-data"><a class="docs-heading-anchor" href="#Preparing-data">Preparing data</a><a id="Preparing-data-1"></a><a class="docs-heading-anchor-permalink" href="#Preparing-data" title="Permalink"></a></h2><p>During the last lecture, we implemented everything from scratch. This lecture will introduce the package <a href="https://fluxml.ai/Flux.jl/stable/models/basics/">Flux</a> which automizes most of the things needed for neural networks.</p><ul><li>It creates many layers, including convolutional layers.</li><li>It creates the model by chaining layers together.</li><li>It efficiently represents model parameters.</li><li>It automatically computes gradients and trains the model by updating the parameters.</li></ul><p>This functionality requires inputs in a specific format.</p><ul><li>Images must be stored in <code>Float32</code> instead of the commonly used <code>Float64</code> to speed up operations.</li><li>Convolutional layers require that the input has dimension <span>$n_x\times n_y\times n_c\times n_s$</span>, where <span>$(n_x,n_y)$</span> is the number of pixels in each dimension, <span>$n_c$</span> is the number of channels (1 for grayscale, and 3 for coloured images) and <span>$n_s$</span> is the number of samples.</li><li>In general, samples are always stored in the last dimension.</li></ul><p>We use the package <a href="https://juliaml.github.io/MLDatasets.jl/stable/">MLDatasets</a> to load the data.</p><pre><code class="language-julia">using MLDatasets

T = Float32
X_train, y_train = MLDatasets.MNIST.traindata(T)
X_test, y_test = MLDatasets.MNIST.testdata(T)</code></pre><p>The first two exercises visualize the data and transform it into the correct input shape required by Flux.</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise:</header>
<div class="admonition-body"><p>Plot the first 15 images of the digit 0 from the training set.</p><p><strong>Hint</strong>: The <code>ImageInspector</code> package written earlier provides the function <code>imageplot(X_train, inds; nrows=3)</code>, where <code>inds</code> are the desired indices.</p><p><strong>Hint</strong>: To find the correct indices, use the function <code>findall</code>.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The unique elements in <code>y_train</code> show that it represents the digits.</p><pre><code class="language-julia">unique(y_train)</code></pre><pre class="documenter-example-output">10-element Array{Int64,1}:
 5
 0
 4
 1
 9
 2
 3
 6
 7
 8</pre><p>Then we use the <code>findall</code> function to find the indices of the first 15 images of the digit zero.</p><pre><code class="language-julia">inds = findall(y_train .== 0)[1:15]</code></pre><p>We use the <code>imageplot</code> function to plot the images. To invert the colours, we need to call it with <code>1 .- X_train</code> instead of <code>X_train</code>.</p><pre><code class="language-julia">using Plots
using ImageInspector

imageplot(1 .- X_train, inds; nrows=3, size=(800,480))

savefig(&quot;mnist_intro2.svg&quot;) # hide</code></pre></p></details><p><img src="../mnist_intro2.svg" alt/></p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise:</header>
<div class="admonition-body"><p>Write function <code>reshape_data</code>, which reshapes <code>X_train</code> and <code>X_test</code> into the correct size required by Flux.</p><p><strong>Hint</strong>: The function should work only on inputs with the correct size. This can be achieved by specifying the correct input type <code>X::AbstractArray{&lt;:Real, 3}</code>.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>As we have never worked with MLDatasets, we do not know in which format the loading function returns the data.</p><pre><code class="language-julia">typeof(X_train)</code></pre><pre class="documenter-example-output">Array{Float32,3}</pre><p>The variable <code>X_train</code> stores a three-dimensional array of images.</p><pre><code class="language-julia">size(X_train)</code></pre><pre class="documenter-example-output">(28, 28, 60000)</pre><p>Its size shows that the first two dimensions are the number of pixels and the last dimension are the samples. Since the images are grayscale, the dimension representing channels is missing. We need to add it.</p><pre><code class="language-julia">function reshape_data(X::AbstractArray{&lt;:Real, 3})
    s = size(X)
    return reshape(X, s[1], s[2], 1, s[3])
end</code></pre><p>We specify that the input array has three dimensions via <code>X::AbstractArray{T, 3}</code>. This may prevent surprises when called with different input size.</p></p></details><p>We write now the function <code>load_data</code>, which loads the data and transform it into the correct shape. The keyword argument <code>onehot</code> specifies whether the labels should be converted into their one-hot representation. The <code>dataset</code> keyword specifies which dataset to load. It can be any dataset from the MLDatasets package, or we can even use datasets outside of this package provided that we define the <code>traindata</code> and <code>testdata</code> functions for it.</p><pre><code class="language-julia">using Flux
using Flux: onehotbatch, onecold

function load_data(dataset; T=Float32, onehot=false, classes=0:9)
    X_train, y_train = dataset.traindata(T)
    X_test, y_test = dataset.testdata(T)

    X_train = reshape_data(X_train)
    X_test = reshape_data(X_test)

    if onehot
        y_train = onehotbatch(y_train, classes)
        y_test = onehotbatch(y_test, classes)
    end

    return X_train, y_train, X_test, y_test
end</code></pre><p>Now we use this function to load the data and modify them into the correct form.</p><pre><code class="language-julia">X_train, y_train, X_test, y_test = load_data(MLDatasets.MNIST; T=T, onehot=true)</code></pre><p>The previous example mentioned that <code>load_data</code> is rather general. The next exercise makes it work for datasets with coloured images.</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise:</header>
<div class="admonition-body"><p>Try to load the CIFAR10 dataset via the <code>load_data</code> function and fix the error in one line of code.</p><p><strong>Hint</strong>: Use <code>dataset = MLDatasets.CIFAR10</code>.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We first load the data in the same way as before.</p><pre><code class="language-julia">load_data(MLDatasets.CIFAR10; T=T, onehot=true)</code></pre><pre><code class="language-julia">│  MethodError: no method matching reshape_data(::Array{Float32,4})
│  Closest candidates are:
│    reshape_data(::AbstractArray{T,3} where T) where T</code></pre><p>It results in an error which states that the <code>reshape_function</code> functon is not defined for inputs with 4 dimensions. We did not implement it because MNIST contains grayscale images, which leads to arrays with 3 dimensions. To fix the problem, it suffices to add a method to the <code>reshape_data</code> function.</p><pre><code class="language-julia">reshape_data(X::AbstractArray{&lt;:Real, 4}) = X</code></pre><p>Now we can load the data.</p><pre><code class="language-julia">typeof(load_data(MLDatasets.CIFAR10; T=T, onehot=true))</code></pre><pre><code class="language-julia">Tuple{Array{Float32,4},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},Array{Float32,4},Flux.OneHotMatrix{Array{Flux.OneHotVector,1}}}</code></pre><p>We see that it correctly returned a tuple of four items.</p></p></details><h2 id="Training-and-storing-the-network"><a class="docs-heading-anchor" href="#Training-and-storing-the-network">Training and storing the network</a><a id="Training-and-storing-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-and-storing-the-network" title="Permalink"></a></h2><p>We recall that machine learning minimizes the discrepancy between the predictions <span>$\operatorname{predict}(w; x_i)$</span> and labels <span>$y_i$</span>. Mathematically, this amount to minimizing the following objective function.</p><p class="math-container">\[L(w) = \frac1n\sum_{i=1}^n \operatorname{loss}(y_i, \operatorname{predict}(w; x_i)).\]</p><p>The gradient descent works with the derivative <span>$\nabla L(w)$</span>, which contains the mean over all samples. Since the MNIST training set size is 50000, evaluating one full gradient is costly. For this reasons, the gradient is approximated by a mean over a small number of samples. This small set is called a minibatch, and this accelerated method stochastic gradient descent.</p><p>The following exercise splits the dataset into minibatches. While we can do it manually, Flux provides a simple way to do so.</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise:</header>
<div class="admonition-body"><p>Use the help of the function <code>DataLoader</code> to split the dataset into minibatches.</p><p><strong>Hint</strong>: It needs to be imported from Flux via <code>using Flux.Data: DataLoader</code>.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We first load the function <code>DataLoader</code>.</p><pre><code class="language-julia">using Flux.Data: DataLoader</code></pre><p>The in-built help shows us how to call this function. It also includes multiple examples.</p><pre><code class="language-julia">help?&gt; DataLoader
search:

  DataLoader(data; batchsize=1, shuffle=false, partial=true)</code></pre><p>We use the following code to split the dataset into minibatches. We need to include both <code>X_train</code> and <code>y_train</code> to perform the partition for the data and the labels.</p><pre><code class="language-julia">batchsize = 32
batches = DataLoader((X_train, y_train); batchsize, shuffle = true)</code></pre></p></details><div class="admonition is-category-bonus">
<header class="admonition-header">BONUS: Manually splitting the dataset</header>
<div class="admonition-body"><p>We can do the same procedure manually. To create minibatches, we create a random partition of all indices <code>randperm(size(y, 2))</code> and use function <code>partition</code> to create an iterator, which creates the minibatches in the form of tuples <span>$(X,y)$</span>.</p><pre><code class="language-julia">using Base.Iterators: partition
using Random

batches = map(partition(randperm(size(y, 2)), batchsize)) do inds
    return (X[:, :, :, inds], y[:, inds])
end</code></pre><p>This procedure is equivalent to the <code>map</code> function.</p><pre><code class="language-julia">[(X[:, :, :, inds], y[:, inds]) for inds in partition(randperm(size(y, 2)), batchsize)]</code></pre><p>The type of <code>batches</code> is a one-dimensional array (vector) of tuples.</p></div></div><p>To build the objective <span>$L$</span>, we first specify the prediction function <span>$\operatorname{predict}$</span>. We keep the usual convention and denote it by model <code>m</code>. It is a composition of seven layers:</p><ul><li>Two convolutional layers extract low-level features from the images.</li><li>Two pooling layers reduce the size of the previous layer.</li><li>One flatten layer converts multi-dimensional arrays into one-dimensional vectors.</li><li>One dense layer is usually applied at the end of the chain.</li><li>One softmax layer is usually the last one and results in probabilities.</li></ul><pre><code class="language-julia">using Random

Random.seed!(666)
m = Chain(
    Conv((2,2), 1=&gt;16, relu),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, relu),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)),
    softmax,
)</code></pre><p>The objective function <span>$L$</span> then applies the cross-entropy loss to the predictions and labels.</p><pre><code class="language-julia">using Flux: crossentropy

L(X, y) = crossentropy(m(X), y)</code></pre><p>We now write the function <code>train_model!</code> to train the neural network <code>m</code>. Since this function modifies the input model <code>m</code>, its name should contain the exclamation mark. Besides the loss function <code>L</code>, data <code>X</code> and labels <code>y</code>, it also contains as keyword arguments optimizer the optimizer <code>opt</code>, the minibatch size <code>batchsize</code>, the number of epochs <code>n_epochs</code>, and the file name <code>file_name</code> to which the model should be saved.</p><pre><code class="language-julia">using BSON

function train_model!(m, L, X, y;
        opt = Descent(0.1),
        batchsize = 128,
        n_epochs = 10,
        file_name = &quot;&quot;)

    batches = DataLoader((X, y); batchsize, shuffle = true)

    for _ in 1:n_epochs
        Flux.train!(L, params(m), batches, opt)
    end

    !isempty(file_name) &amp;&amp; BSON.bson(file_name, m=m)

    return
end</code></pre><p>The function <code>train_model!</code> first splits the datasets into minibatches <code>batches</code> and then uses the optimizer for <code>n_epochs</code> epochs. In one epoch, the model <code>m</code> evaluates all samples exactly once. Therefore, the optimizer performs the same number of gradient updates as the number of minibatches during one epoch. On the other hand, the standard gradient descent makes only one gradient update during one epoch. The default optimizer is the stochastic gradient descent with stepsize <span>$0.1$</span>. Since we do not need an index in the loop, we use <code>_</code>. Finally, if <code>file_name</code> is non-empty, the function saves the trained model <code>m</code>.</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise:</header>
<div class="admonition-body"><p>Train the model for one epoch and save it to <code>MNIST_simple.bson</code>. Print the accuracy on the testing set.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To train the model, it suffices to call the previously written function.</p><pre><code class="language-julia">file_name = &quot;mnist_simple.bson&quot;
train_model!(m, L, X_train, y_train; n_epochs=1, file_name=file_name)</code></pre><p>The accuracy has been computed many times during the course.</p><pre><code class="language-julia">using Statistics

accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))

&quot;Test accuracy = &quot; * string(accuracy(X_test, y_test))</code></pre><p>We defined <code>accuracy</code> in a different way than before. Can you spot the difference and explain why they are equivalent?</p></p></details><pre class="documenter-example-output">Test accuracy = 0.9328</pre><p>The accuracy is over 93%, which is not bad for training for one epoch only. Let us recall that training for one epoch means that the classifier evaluates each sample only once. To obtain better accuracy, we need to train the model for more epochs. Since that may take some time, it is not good to train the same model repeatedly. The following exercise determines automatically whether the trained model already exists. If not, it trains it.</p><div class="admonition is-category-exercise">
<header class="admonition-header">Exercise:</header>
<div class="admonition-body"><p>Write a function <code>train_or_load!(file_name, m, args...; ???)</code> checking whether the file <code>file_name</code> exists.</p><ul><li>If it exists, it loads it and then copies its parameters into <code>m</code> using the function <code>Flux.loadparams!</code>.</li><li>If it does not exist, it trains it using <code>train_model!</code>.</li></ul><p>In both cases, the model <code>m</code> should be modified inside the <code>train_or_load!</code> function. Pay special attention to the optional arguments <code>???</code>.</p><p>Use this function to load the model from <code>data/mnist.bson</code> and evaluate the performance at the testing set.</p></div></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The optional arguments should contain <code>kwargs...</code>, which will be passed to <code>train_model!</code>. Besides that, we include <code>force</code> which enforces that the model is trained even if it already exists.</p><p>First, we should check whether the directory exists <code>!isdir(dirname(file_name))</code> and if not, we create it <code>mkpath(dirname(file_name))</code>. Then we check whether the file exists (or whether we want to enforce the training). If yes, we train the model, which already modifies <code>m</code>. If not, we <code>BSON.load</code> the model and copy the loaded parameters into <code>m</code> by <code>Flux.loadparams!(m, params(m_loaded))</code>. We cannot load directly into <code>m</code> instead of <code>m_loaded</code> because that would create a local copy of <code>m</code> and the function would not modify the external <code>m</code>.</p><pre><code class="language-julia">function train_or_load!(file_name, m, args...; force=false, kwargs...)

    !isdir(dirname(file_name)) &amp;&amp; mkpath(dirname(file_name))

    if force || !isfile(file_name)
        train_model!(m, args...; file_name=file_name, kwargs...)
    else
        m_weights = BSON.load(file_name)[:m]
        Flux.loadparams!(m, params(m_weights))
    end
end</code></pre><p>To load the model, we should use <code>joinpath</code> to be compatible with all operating systems. The accuracy is evaluated as before.</p><pre><code class="language-julia">file_name = joinpath(&quot;data&quot;, &quot;mnist.bson&quot;)
train_or_load!(file_name, m, L, X_train, y_train)

&quot;Test accuracy = &quot; * string(accuracy(X_test, y_test))</code></pre></p></details><pre class="documenter-example-output">Test accuracy = 0.9815</pre><p>The externally trained model has an accuracy of more than 98% (it has the same architecture as the one defined above, but it was trained for 50 epochs.). Even though there are perfect models (with accuracy 100%) on MNIST, we are happy with this result. We will perform further analysis of the network in the exercises.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../iris/">« Introduction to Flux</a><a class="docs-footer-nextpage" href="../exercises/">Exercises »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 4 June 2021 08:04">Friday 4 June 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
