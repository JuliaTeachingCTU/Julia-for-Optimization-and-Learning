<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Exercises · Numerical computing in Julia</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Numerical computing in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Numerical computing in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit">Numerical computing in Julia</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why_julia/">Why Julia?</a></li><li><a class="tocitem" href="../../howto/">How to...</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Mathematical operations and Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluation</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of Variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception Handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/basics/">Package management</a></li><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">5: Composite types and constructors</span></li><li><span class="tocitem">6: Modules and enviroments</span></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Theory of optimization</a></li><li><a class="tocitem" href="../../lecture_07/gradients/">Visualization of gradients</a></li><li><a class="tocitem" href="../../lecture_07/numerical_methods/">Numerical methods</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox" checked/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../nn/">More complex networks</a></li><li class="is-active"><a class="tocitem" href>Exercises</a></li></ul></li><li><span class="tocitem">11: Ordinary differential equations</span></li><li><span class="tocitem">12: Statistics I.</span></li><li><span class="tocitem">13: Statistics II.</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">10: Neural networks II.</a></li><li class="is-active"><a href>Exercises</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Exercises</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_10/exercises.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h1><div class = "exercise-body">
<header class = "exercise-header">Exercise 1: Operations on GPUs</header><p><p>While most operations in a computer are performed on CPUs (central processing unit), neural networks are trained on other hardware such as GPUs (graphics processing unit) or specialized hardware such as TPUs. </p><p>To use GPUs, include packages Flux and CUDA. Then generate a radnom matrix <span>$A\in \mathbb{R}^{100\times 100}$</span> and a random vector <span>$b\in \mathbb{R}^{100}$</span>. They will be stored in the memory and the computation will be performed on the CPU. To save them to the GPU memory and allow computations on the GPU, use <code>gpu(A)</code> or the more commonly used <code>A |&gt; gpu</code>.</p><p>Investigate how long it takes to perform multiplication <span>$Ab$</span> if both objects are on CPU, GPU or if they are saved differently. Check that both multiplications resulted in the same vector.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The beginning is simple</p><pre><code class="language-julia">using Flux
using CUDA

A = randn(100,100)
b = randn(100)
A_g = A |&gt; gpu
b_g = b |&gt; gpu</code></pre><p>To test the time, we measure the time for multiplication</p><pre><code class="language-julia">@time A*b;
@time A_g*b_g;
@time A_g*b;</code></pre><pre><code class="language-julia">0.069785 seconds (294.76 k allocations: 15.585 MiB, 14.75% gc time)
0.806913 seconds (419.70 k allocations: 22.046 MiB)
0.709140 seconds (720.01 k allocations: 34.860 MiB, 1.53% gc time)</code></pre><p>We see that all three times are different. Can we infer anything from it? No! The problem is that during a first call to a function, some compilation usually takes place. We should always compare only the second time.</p><pre><code class="language-julia">@time A*b;
@time A_g*b_g;
@time A_g*b;</code></pre><pre><code class="language-julia">0.000083 seconds (1 allocation: 896 bytes)
0.000154 seconds (11 allocations: 272 bytes)
0.475280 seconds (10.20 k allocations: 957.125 KiB)</code></pre><p>We conclude that while the computation on CPU and GPU takes approximately the same time, when using the mixed types, it takes much longer. </p><p>To compare the results, the first idea would be to run</p><pre><code class="language-julia">norm(A*b - A_g*b_g)</code></pre><p>which would result in an error. We cannot use any operations on arrays stored both on CPU and GPU. The correct way is to move the GPU array to CPU and only then to compute the norm</p><pre><code class="language-julia">using LinearAlgebra

norm(A*b - cpu(A_g*b_g))</code></pre><pre class="documenter-example-output">0.0</pre><p>The norm is surprisingly large. Checking the types</p><pre><code class="language-julia">(typeof(A), typeof(A_g))</code></pre><pre class="documenter-example-output">(Array{Float64,2}, Array{Float64,2})</pre><p>we realize that one of the arrays is stored in <code>Float64</code> while the second one in <code>Float32</code>. To to the different number of saved digits, the multiplication results in this error.</p></p></details><p>The previous exercise did not show any differences when performing a matrix-vector multiplication. The probable reason was that the running times were too short. The next exercise shows the time difference when applied to a larger problem.  </p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Load the MNIST dataset and the model saved in <code>data/mnist.bson</code>. Compare the evaluation of all samples from the testing set when done on CPU and GPU. For the latter, you need to convert the model to GPU as well via <code>m |&gt; gpu</code>.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We load the data, model and convert everything to GPU</p><pre><code class="language-julia">using CUDA

m = Chain(
    Conv((2,2), 1=&gt;16, relu),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, relu),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)), softmax)

file_name = joinpath(&quot;data&quot;, &quot;mnist.bson&quot;)
train_or_load!(file_name, m, X_train, y_train)

m_g = m |&gt; gpu
X_test_g = X_test |&gt; gpu</code></pre><p>Nowe we can measure the evaluation time. Remember that before doing so, we need to compile all the functions by evaluating at least one sample.</p><pre><code class="language-julia">m(X_test[:,:,:,1:1])
m_g(X_test_g[:,:,:,1:1])

@time m(X_test);
@time m_g(X_test_g);</code></pre><pre><code class="language-julia">1.190033 seconds (40.24 k allocations: 1.069 GiB, 21.73% gc time)
0.071805 seconds (789 allocations: 27.641 KiB)</code></pre><p>Using the GPU speeded the computation by more than ten times. </p></p></details><div class = "info-body">
<header class = "info-header">Computation on GPU</header><p><p>Make sure that all computation is performed either on CPU or GPU. Do not mix them. When computing on GPU, make sure that all computations are fast. One important example is</p><pre><code class="language-julia">accuracy(x, y) = mean(onecold(cpu(m(x))) .== onecold(cpu(y)))</code></pre><p>Because <code>onecold</code> accesses individual elements of an array, it is extremely slow on GPU. For this reason, we need to move the arrays on CPU first.</p><p>Another thing to remember is to always convert all objects to CPU before saving them.</p></p></div><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Load the network from <code>data/mnist.bson</code>. Then create a <span>$10\times 10$</span> table, where the <span>$(i+1,j+1)$</span> entry is the number of samples, where digit <span>$i$</span> was wrongly classified as digit <span>$j$</span>.</p><p>Convert the table into a dataframe and add labels.</p><p>Finally, plot all figures which are <span>$9$</span> but were classified as <span>$7$</span>.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>First, we load the data as many times before</p><pre><code class="language-">m = Chain(
    Conv((2,2), 1=&gt;16, relu),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, relu),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)), softmax)

file_name = joinpath(&quot;data&quot;, &quot;mnist.bson&quot;)
train_or_load!(file_name, m, X_train, y_train)</code></pre><p>To create the table, we specify that the entries are <code>Int</code>. We save the labels and predictions. Since we do not use the second argument to <code>onecold</code>, the entries of <code>y</code> and <code>y_hat</code> are between 1 and 10. Then we run a foor loop over all misclassified samples which adds to the error counts. </p><pre><code class="language-">y_hat = onecold(m(X_test))
y = onecold(y_test)

errors = zeros(Int, 10, 10)
for i in findall(y_hat .!= y)
    errors[y[i], y_hat[i]] += 1
end</code></pre><p>To create the dataframe, we use <code>df = DataFrame(errors)</code>. Note that it prints correctly integers and not strings. We change labels x1 to miss0, ... Similarly we add a first rows with the actual labels. </p><pre><code class="language-">using DataFrames

df = DataFrame(errors)

names!(df, [Symbol(&quot;miss$(i)&quot;) for i in 0:9])
insert!(df, 1, string.(0:9), :label)

nothing # hide</code></pre><p>It is surprising that the largest number of misclassifications is 9 into 7. One would expect 8 to 0, 5 to 6 or 8 to 9. To plot all these images, we find the misclassified indices and use the function <code>plot_image</code>. Since <code>y</code> are stored in the 1:10 format, we need to shift the indices by one. Since the number of 11 is a prime number, we cannot plot it in a <code>layout</code>. We use a hack and add an empty plot <code>p_empty</code>.</p><pre><code class="language-">i1 = 9
i2 = 7

p = [plot_image(X_test[:,:,:,i]) for i in findall((y.==i1+1) .&amp; (y_hat.==i2+1))]
p_empty = plot(legend=false,grid=false,foreground_color_subplot=:white) 

plot(p..., p_empty; layout=(6,2)) 

savefig(&quot;miss.svg&quot;) # hide</code></pre><p><img src="miss.svg" alt/></p><p>We see that some of the nines could be recognized as a seven even by humans. </p></p></details><p>The correct answer is</p><pre><code class="language-">df # hide</code></pre><div class = "exercise-body">
<header class = "exercise-header">Exercise 4: Visualization of neural networks 1</header><p><p>From the theoretical part we know that output of convolutional layers have the same dimension as inputs. If the activation function is a sigmoid, the output values stay in the interval <span>$[0,1]$</span> and can, therefore, be also interpreted as images. The following two exercises will depict how images are propagated through the network.</p><p>???</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>???</p></p></details><pre><code class="language-">m = Chain(
    Conv((2,2), 1=&gt;16, sigmoid),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, sigmoid),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)), softmax)

file_name = joinpath(&quot;data&quot;, &quot;mnist_sigmoid.bson&quot;)
train_or_load!(file_name, m, X_train, y_train)</code></pre><p>0.9831</p><pre><code class="language-">for qwe = 0:9
    ii0 = findall(onecold(y_train, 0:9) .== qwe)[1:5]

    p0 = [plot_image(X_train[:,:,:,i:i][:,:,1,1]) for i in ii0]
    p1 = [plot_image((m[1:2](X_train[:,:,:,i:i]))[:,:,1,1]) for i in ii0]
    p2 = [plot_image((m[1:4](X_train[:,:,:,i:i]))[:,:,1,1]) for i in ii0]

    plot(p0..., p1..., p2...; layout=(3,5))
    
    savefig(&quot;asd$(qwe).svg&quot;)
end</code></pre><p><img src="asd0.svg" alt/> <img src="asd1.svg" alt/> <img src="asd2.svg" alt/> <img src="asd3.svg" alt/> <img src="asd4.svg" alt/> <img src="asd5.svg" alt/> <img src="asd6.svg" alt/> <img src="asd7.svg" alt/> <img src="asd8.svg" alt/> <img src="asd9.svg" alt/></p><pre><code class="language-">for qwe = 0:9
    ii0 = findall(onecold(y_train, 0:9) .== qwe)[1:5]

    p0 = [plot_image(X_train[:,:,:,i:i][:,:,end,1]) for i in ii0]
    p1 = [plot_image((m[1:2](X_train[:,:,:,i:i]))[:,:,end,1]) for i in ii0]
    p2 = [plot_image((m[1:4](X_train[:,:,:,i:i]))[:,:,end,1]) for i in ii0]

    p = plot(p0..., p1..., p2...; layout=(3,5))
    
    savefig(&quot;zxc$(qwe).svg&quot;)
end</code></pre><p><img src="zxc0.svg" alt/> <img src="zxc1.svg" alt/> <img src="zxc2.svg" alt/> <img src="zxc3.svg" alt/> <img src="zxc4.svg" alt/> <img src="zxc5.svg" alt/> <img src="zxc6.svg" alt/> <img src="zxc7.svg" alt/> <img src="zxc8.svg" alt/> <img src="zxc9.svg" alt/></p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Some text that describes the exercise</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Solution</p></p></details></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../nn/">« More complex networks</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 4 January 2021 14:12">Monday 4 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
