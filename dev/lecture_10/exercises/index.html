<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Exercises · Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Machine Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Machine Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox"/><label class="tocitem" for="menuitem-2"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quick Start Guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary Functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data Structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional Evaluation</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and Iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft Local Scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of Variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception Handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/basics/">Package Management</a></li><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard Library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other Useful Packages</a></li></ul></li><li><span class="tocitem">5: Composite types and constructors</span></li><li><span class="tocitem">6: Modules and enviroments</span></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox" checked/><label class="tocitem" for="menuitem-12"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../nn/">More complex networks</a></li><li class="is-active"><a class="tocitem" href>Exercises</a></li></ul></li><li><span class="tocitem">11: Statistics</span></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">10: Neural networks II.</a></li><li class="is-active"><a href>Exercises</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Exercises</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_10/exercises.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Exercises"><a class="docs-heading-anchor" href="#Exercises">Exercises</a><a id="Exercises-1"></a><a class="docs-heading-anchor-permalink" href="#Exercises" title="Permalink"></a></h1><p>The first two exercises handle training neural networks on GPUs instead of CPUs. Even though this is extremely important for reducing the training time, we postponed it to the exercises because some course participants may not have a compatible GPU for training. If you are not able to do these two exercises for this reason, we apologize.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise 1: Operations on GPUs</header><p><p>While most computer operations are performed on CPUs (central processing unit), neural networks are trained on other hardware such as GPUs (graphics processing unit) or specialized hardware such as TPUs.</p><p>To use GPUs, include packages Flux and CUDA. Then generate a random matrix <span>$A\in \mathbb{R}^{100\times 100}$</span> and a random vector <span>$b\in \mathbb{R}^{100}$</span>. They will be stored in the memory (RAM) and the computation will be performed on CPU. To move them to the GPU memory and allow computations on GPU, use <code>gpu(A)</code> or the more commonly used <code>A |&gt; gpu</code>.</p><p>Investigate how long it takes to perform multiplication <span>$Ab$</span> if both objects are on CPU, GPU or if they are saved differently. Check that both multiplications resulted in the same vector.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The beginning is simple</p><pre><code class="language-julia">using Flux
using CUDA

A = randn(100,100)
b = randn(100)
A_g = A |&gt; gpu
b_g = b |&gt; gpu</code></pre><p>To test the time, we measure the time for multiplication</p><pre><code class="language-julia">@time A*b;
@time A_g*b_g;
@time A_g*b;</code></pre><pre><code class="language-julia">0.069785 seconds (294.76 k allocations: 15.585 MiB, 14.75% gc time)
0.806913 seconds (419.70 k allocations: 22.046 MiB)
0.709140 seconds (720.01 k allocations: 34.860 MiB, 1.53% gc time)</code></pre><p>We see that all three times are different. Can we infer anything from it? No! The problem is that during a first call to a function, some compilation usually takes place. We should always compare only the second time.</p><pre><code class="language-julia">@time A*b;
@time A_g*b_g;
@time A_g*b;</code></pre><pre><code class="language-julia">0.000083 seconds (1 allocation: 896 bytes)
0.000154 seconds (11 allocations: 272 bytes)
0.475280 seconds (10.20 k allocations: 957.125 KiB)</code></pre><p>We conclude that while the computation on CPU and GPU takes approximately the same time, when using the mixed types, it takes much longer.</p><p>To compare the results, the first idea would be to run</p><pre><code class="language-julia">norm(A*b - A_g*b_g)</code></pre><p>which would result in an error. We cannot use any operations on arrays stored both on CPU and GPU. The correct way is to move the GPU array to CPU and only then to compute the norm</p><pre><code class="language-julia">using LinearAlgebra

norm(A*b - cpu(A_g*b_g))</code></pre><pre><code class="language-julia">1.2004562847861718e-5</code></pre><p>The norm is surprisingly large. Checking the types</p><pre><code class="language-julia">(typeof(A), typeof(A_g))</code></pre><pre><code class="language-julia">(Array{Float64,2}, CUDA.CuArray{Float32,2})</code></pre><p>we realize that one of the arrays is stored in <code>Float64</code> while the second one in <code>Float32</code>. Due to the different number of saved digits, the multiplication results in this error.</p></p></details><p>The previous exercise did not show any differences when performing a matrix-vector multiplication. The probable reason was that the running times were too short. The next exercise shows the time difference when applied to a larger problem.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Load the MNIST dataset and the model saved in <code>data/mnist.bson</code>. Compare the evaluation of all samples from the testing set when done on CPU and GPU. For the latter, you need to convert the model to GPU.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We load the data, model and convert everything to GPU</p><pre><code class="language-julia">using CUDA

m = Chain(
    Conv((2,2), 1=&gt;16, relu),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, relu),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)),
    softmax,
)

file_name = joinpath(&quot;data&quot;, &quot;mnist.bson&quot;)
train_or_load!(file_name, m, X_train, y_train)

m_g = m |&gt; gpu
X_test_g = X_test |&gt; gpu</code></pre><p>Now we can measure the evaluation time. Remember that before doing so, we need to compile all the functions by evaluating at least one sample.</p><pre><code class="language-julia">m(X_test[:,:,:,1:1])
m_g(X_test_g[:,:,:,1:1])

@time m(X_test);
@time m_g(X_test_g);</code></pre><pre><code class="language-julia">1.190033 seconds (40.24 k allocations: 1.069 GiB, 21.73% gc time)
0.071805 seconds (789 allocations: 27.641 KiB)</code></pre><p>Using GPU speeded the computation by more than ten times.</p></p></details><div class = "info-body">
<header class = "info-header">Computation on GPU</header><p><p>Using GPUs speeds up the training of neural networks in orders of magnitude. However, one needs to be aware of some pitfalls.</p><p>Make sure that all computation is performed either on CPU or GPU. Do not mix them. When computing on GPU, make sure that all computations are fast. One important example is</p><pre><code class="language-julia">accuracy(x, y) = mean(onecold(cpu(m(x))) .== onecold(cpu(y)))</code></pre><p>Because <code>onecold</code> accesses individual elements of an array, it is extremely slow on GPU. For this reason, we need to move the arrays on CPU first.</p><p>Another thing to remember is to always convert all objects to CPU before saving them.</p></p></div><p>Exercises which do not require GPUs start here.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise 3:</header><p><p>Load the network from <code>data/mnist.bson</code>. Then create a <span>$10\times 10$</span> table, where the <span>$(i+1,j+1)$</span> entry is the number of samples, where digit <span>$i$</span> was misclassified as digit <span>$j$</span>.</p><p>Convert the table into a dataframe and add labels.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>First, we load the data as many times before</p><pre><code class="language-julia">m = Chain(
    Conv((2,2), 1=&gt;16, relu),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, relu),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)),
    softmax,
)

file_name = joinpath(&quot;data&quot;, &quot;mnist.bson&quot;)
train_or_load!(file_name, m, X_train, y_train)</code></pre><p>When creating a table, we specify that its entries are <code>Int</code>. We save the predictions <code>y_hat</code> and labels <code>y</code>. Since we do not use the second argument to <code>onecold</code>, the entries of <code>y_hat</code> and <code>y</code> are between 1 and 10. Then we run a for loop over all misclassified samples and add to the error counts.</p><pre><code class="language-julia">y_hat = onecold(m(X_test))
y = onecold(y_test)

errors = zeros(Int, 10, 10)
for i in findall(y_hat .!= y)
    errors[y[i], y_hat[i]] += 1
end</code></pre><p>To create the dataframe, we use <code>df = DataFrame(errors)</code>. It prints correctly integers and not strings. We change labels x1 to miss0, ... Similarly, we add the labels as the first column.</p><pre><code class="language-julia">using DataFrames

df = DataFrame(errors)

rename!(df, [Symbol(&quot;miss$(i)&quot;) for i in 0:9])
insertcols!(df, 1, :label =&gt; string.(0:9))</code></pre></p></details><table class="data-frame"><thead><tr><th></th><th>label</th><th>miss0</th><th>miss1</th><th>miss2</th><th>miss3</th><th>miss4</th><th>miss5</th><th>miss6</th><th>miss7</th><th>miss8</th><th>miss9</th></tr><tr><th></th><th>String</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>10 rows × 11 columns</p><tr><th>1</th><td>0</td><td>0</td><td>0</td><td>3</td><td>0</td><td>0</td><td>2</td><td>2</td><td>1</td><td>2</td><td>1</td></tr><tr><th>2</th><td>1</td><td>0</td><td>0</td><td>3</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><th>3</th><td>2</td><td>3</td><td>6</td><td>0</td><td>3</td><td>0</td><td>0</td><td>1</td><td>4</td><td>3</td><td>0</td></tr><tr><th>4</th><td>3</td><td>0</td><td>1</td><td>4</td><td>0</td><td>0</td><td>5</td><td>0</td><td>1</td><td>4</td><td>2</td></tr><tr><th>5</th><td>4</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>3</td></tr><tr><th>6</th><td>5</td><td>1</td><td>0</td><td>1</td><td>6</td><td>0</td><td>0</td><td>2</td><td>1</td><td>2</td><td>0</td></tr><tr><th>7</th><td>6</td><td>7</td><td>3</td><td>1</td><td>0</td><td>5</td><td>7</td><td>0</td><td>0</td><td>4</td><td>0</td></tr><tr><th>8</th><td>7</td><td>0</td><td>4</td><td>8</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>2</td><td>2</td></tr><tr><th>9</th><td>8</td><td>5</td><td>0</td><td>4</td><td>4</td><td>0</td><td>4</td><td>0</td><td>4</td><td>0</td><td>6</td></tr><tr><th>10</th><td>9</td><td>2</td><td>4</td><td>1</td><td>4</td><td>7</td><td>6</td><td>0</td><td>11</td><td>2</td><td>0</td></tr></tbody></table><p>Surprisingly, the largest number of misclassifications is 9 into 7. One would expect 8 to 0, 5 to 6 or 8 to 9. We investigate this in the next exercise.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise 4:</header><p><p>Plot all images which are <span>$9$</span> but were classified as <span>$7$</span>.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To plot all these misclassified images, we find their indices and use the function <code>plot_image</code>. Since <code>y</code> are stored in the 1:10 format, we need to shift the indices by one. Since there are 11 of these images, and since 11 is a prime number, we cannot plot it in a <code>layout</code>. We use a hack and add an empty plot <code>p_empty</code>. When plotting, we specify <code>layout</code> and to minimize the empty space between images also <code>size</code>.</p><pre><code class="language-julia">i1 = 9
i2 = 7

p = [plot_image(X_test[:,:,:,i]) for i in findall((y.==i1+1) .&amp; (y_hat.==i2+1))]
p_empty = plot(legend=false,grid=false,foreground_color_subplot=:white)

plot(p..., p_empty; layout=(3,4), size=(800,600))</code></pre></p></details><p><img src="../miss.svg" alt/></p><p>We see that some of the nines could be recognized as a seven even by humans.</p><p>The following exercise depicts how images propagate through the network.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise 5: Visualization of neural networks 1</header><p><p>We know that the output of the convolutional layers has the same number of dimensions as the inputs. If the activation function is the sigmoid, the output values stay within <span>$[0,1]$</span> and can also be interpreted as images. Use the same network as before but replace ReLU by sigmoid activation functions. Load the model from <code>data/mnist_sigmoid.bson</code> (you can check that the model accuracy is 0.9831).</p><p>For all digits, select the first five samples from the training set of this digit. Then create <span>$5\times 5$</span> graph (there will be 10 of them for each digit), where each column corresponds to one sample. The rows should be:</p><ul><li>The original image.</li><li>The first channel of the layer after the first pooling layer.</li><li>The last channel of the layer after the first pooling layer.</li><li>The first channel of the layer after the second pooling layer.</li><li>The last channel of the layer after the second pooling layer.</li></ul><p>Discuss the images.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To create the network and to load the data, we use</p><pre><code class="language-julia">m = Chain(
    Conv((2,2), 1=&gt;16, sigmoid),
    MaxPool((2,2)),
    Conv((2,2), 16=&gt;8, sigmoid),
    MaxPool((2,2)),
    flatten,
    Dense(288, size(y_train,1)),
    softmax,
)

file_name = joinpath(&quot;data&quot;, &quot;mnist_sigmoid.bson&quot;)
train_or_load!(file_name, m, X_train, y_train)</code></pre><p>Before plotting, we perform a for loop over the digits. Then <code>onecold(y_train, classes) .== i</code> creates a <code>BitArray</code> with ones if the condition is satisfied, and zeros if the condition is not satisfied. Then <code>findall(???)</code> selects all ones, and <code>???[1:5]</code> finds the first five indices. Since we need to plot the original image, and the images after the second and fourth layer (there is always a convolutional layer before the pooling layer), we save these values into <code>z1</code>, <code>z2</code> and <code>z3</code>. Since <code>plot_image(z1[:,:,1,i])</code> plots the first channel of the <span>$i^{\rm th}$</span> samples from <code>z1</code>, we create an array of plots by <code>p1 = [plot_image(z1[:,:,1,i]) for i in 1:size(z1,4)]</code>. As the length of <code>z1</code> is five, the length of <code>p1</code> is also five. This is the first row of the final plot. We create the other rows in the same way. To plot the final plot, we do <code>plot(p1..., p2a..., p2b..., p3a..., p3b...)</code>, which unpacks the 5 arrays into 25 inputs to the <code>plot</code> function.</p><pre><code class="language-julia">classes = 0:9
for i in classes
    ii = findall(onecold(y_train, classes) .== i)[1:5]

    z1 = X_train[:,:,:,ii]
    z2 = m[1:2](X_train[:,:,:,ii])
    z3 = m[1:4](X_train[:,:,:,ii])

    p1 = [plot_image(z1[:,:,1,i]) for i in 1:size(z1,4)]
    p2a = [plot_image(z2[:,:,1,i]) for i in 1:size(z2,4)]
    p3a = [plot_image(z3[:,:,1,i]) for i in 1:size(z3,4)]
    p2b = [plot_image(z2[:,:,end,i]) for i in 1:size(z2,4)]
    p3b = [plot_image(z3[:,:,end,i]) for i in 1:size(z3,4)]

    plot(p1..., p2a..., p2b..., p3a..., p3b...; layout=(5,5), size=(600,600))
    savefig(&quot;Layers_$(i).svg&quot;)
end</code></pre><p>We plot and comment on three selected digits below.</p></p></details><p>Digit 0</p><p><img src="../Layers_0.svg" alt/></p><p>Digit 1</p><p><img src="../Layers_1.svg" alt/></p><p>Digit 9</p><p><img src="../Layers_9.svg" alt/></p><p>We may observe several things:</p><ul><li>The functions inside the neural network do the same operations on all samples. The second row is always a black digit on a grey background.</li><li>The size of the image decreases when propagated deeper into the network. The second and third rows (after the second layer) contain more pixels than the fourth and fifth rows (after the fourth layer).</li><li>The channels of the same layer produce different outputs. While the second row (first channel after the second layer) depicts black digits on a grey background, the third row (last channel after the second layer) depicts white digits on black background.</li><li>Each digit produce different images. This is important for separation and correct predictions.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../nn/">« More complex networks</a><a class="docs-footer-nextpage" href="../../lecture_12/theory/">Differential equations »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 1 February 2021 10:59">Monday 1 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
