<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction to Flux Â· Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Machine Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Machine Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../nn/">More complex networks</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Statistics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Introduction to Flux</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Introduction to Flux</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_10/iris.md" title="Edit on GitHub"><span class="docs-icon fab">ï</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Introduction-to-Flux"><a class="docs-heading-anchor" href="#Introduction-to-Flux">Introduction to Flux</a><a id="Introduction-to-Flux-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction-to-Flux" title="Permalink"></a></h1><p>Flux is a library for using neural networks. This part will present the basics of Flux on the Iris dataset from the previous lecture. We include the auxiliary functions from the previous lesson into the <code>utilities.jl</code> file, which we include by</p><pre><code class="language-julia">include(&quot;utilities.jl&quot;)</code></pre><p>We set the seed and load the data in the same way as during the last lecture.</p><pre><code class="language-julia">using Random
using BSON: @load

Random.seed!(666)

file_name = joinpath(&quot;data&quot;, &quot;iris.bson&quot;)
@load file_name X y y_name

X_train, y_train, X_test, y_test, classes = prepare_data(X, y)</code></pre><p>We start by creating the same network.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Find the documentation of Flux.jl online and create the same network as in the previous lecture.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To create the neural network, Flux uses <code>Chain(???, ???, ...)</code>, where <code>???</code> are individual layers. Dense layers are created by <code>Dense</code> with the correct number of input and output neurons. We also need to specify the activation functions.</p><pre><code class="language-julia">using Flux

n_hidden = 5
m = Chain(
    Dense(size(X_train,1), n_hidden, relu),
    Dense(n_hidden, size(y_train,1), identity),
    softmax,
    )</code></pre><p>Since <code>identity</code> is the default argument, it is possible to remove it. However, we recommend to keep it for clarity.</p></p></details><p>To evaluate the whole dataset, we call</p><pre><code class="language-julia">m(X_train)</code></pre><pre class="documenter-example-output">3Ã120 Array{Float32,2}:
 0.302199  0.308447  0.367382  0.13673   â¦  0.017684  0.381633  0.317811
 0.468215  0.256639  0.409098  0.170793     0.162731  0.373635  0.350714
 0.229586  0.434914  0.22352   0.692477     0.819585  0.244732  0.331475</pre><p>Because there are <span>$3$</span> classes and <span>$120$</span> samples, it returns an array of size <span>$3\times 120$</span>. Note that the columns are probabilities.</p><p>We can access the neural network parameters by using <code>params(m)</code>. At the same time, we can select the second layer of <code>m</code> by <code>m[2]</code>. This can be naturally combined, so that</p><pre><code class="language-julia">params(m[2])</code></pre><pre class="documenter-example-output">Params([Float32[-0.12225063 0.49250954 â¦ -0.8572132 -0.48205933; 0.026877465 -0.13672298 â¦ -0.15691267 -0.111053; -0.81124574 0.4019993 â¦ 0.5207074 -0.33657947], Float32[0.0, 0.0, 0.0]])</pre><p>returns the parameters of the second layer. Since the second layer has <span>$5$</span> input and <span>$3$</span> output neurons, the matrix is of size <span>$3\times 5$</span> and the bias is a vector of length <span>$3$</span>. The parameters <code>params(m[2])</code> are a tuple of the matrix and the vector. This also implies that the parameters are initialized randomly, and we do not need to take care of it. If for any reason, we need to use a special initialization, we can assign to parameters via</p><pre><code class="language-julia">params(m[2])[2] .= [-1;0;1]</code></pre><p>Here, we assigned to the bias of the second layer.</p><p>To train the model, we first need to specify the loss function. Since it is the cross-entropy between the prediction and the label, it can be done via</p><pre><code class="language-julia">import Flux: crossentropy

loss(x,y) = crossentropy(m(x), y)</code></pre><p>The <code>loss</code> function does not have <code>m</code> as input. Even though there could be an additional input parameter, it is customary to write it without it. To evaluate the loss function, we simply write</p><pre><code class="language-julia">loss(X_train, y_train)</code></pre><pre class="documenter-example-output">1.4853470732147496</pre><p>This computes the loss function on the whole training set. Since Flux is (unlike our implementation from the last lecture) smart, there is no need to take care of individual samples.</p><div class = "info-body">
<header class = "info-header">Notation</header><p><p>While the <a href="https://en.wikipedia.org/wiki/Cross_entropy">standard definition</a> of cross-entropy is <span>$\operatorname{loss}(y,\hat y)$</span>, <a href="https://fluxml.ai/Flux.jl/stable/models/losses/">Flux</a> uses <span>$\operatorname{loss}(\hat y,y)$</span>.</p></p></div><p>Since we have the model and the loss function, the only remaining required thing for training is the gradient. This can be done in a simple way by</p><pre><code class="language-julia">gs = gradient(() -&gt; loss(X_train, y_train), params(m))</code></pre><p>The function <code>gradient</code> takes two inputs. The first one is the function we want to differentiate, and the second one are the parameters. The <code>loss</code> function needs to be evaluated at the correct points <code>X_train</code> and <code>y_train</code>. In some applications (adversarial learning), we may need to differentiate with respect to other parameters such as <code>X_train</code>. This can be achieved by changing the second parameters of the <code>gradient</code> function</p><pre><code class="language-julia">gs = gradient(() -&gt; loss(X_train, y_train), params(X_train))

size(gs[X_train])</code></pre><pre class="documenter-example-output">(4, 120)</pre><p>Since <code>X_train</code> is of shape <span>$4\times 120$</span>, the gradient needs to have the same size.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Use the documentation of Flux.jl once again and train the neural network for 250 iterations with <code>ADAM(0.01)</code> optimizer and no minibatches. You may want to use the Flux <code>update!</code> function.</p><p>Plot the accuracy on the testing set in every iteration.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We first create the <code>accuracy</code> function as before</p><pre><code class="language-julia">accuracy(x, y) = mean(onecold(m(x), classes) .== onecold(y, classes))</code></pre><p>To compute the gradient, we first save <code>ps = params(m)</code> so that we do not need to call <code>params</code> all the time. The gradient can be computed by <code>gs = gradient(() -&gt; loss(X_train, y_train), ps)</code>. Finally, the update takes the optimizer, the parameters and the gradient and performs the update. The whole code is</p><pre><code class="language-julia">ps = params(m)
opt = ADAM(0.01)
max_iter = 250

acc_test = zeros(max_iter)
for i in 1:max_iter
    gs = gradient(() -&gt; loss(X_train, y_train), ps)
    Flux.Optimise.update!(opt, ps, gs)
    acc_test[i] = accuracy(X_test, y_test)
end</code></pre><pre class="documenter-example-output">â Warning: Assignment to `gs` in soft scope is ambiguous because a global variable by the same name exists: `gs` will be treated as a new local. Disambiguate by using `local gs` to suppress this warning or `global gs` to assign to the existing global variable.
â @ none:2</pre></p></details><p><img src="../Iris_acc.svg" alt/></p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 25 March 2021 16:34">Thursday 25 March 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
