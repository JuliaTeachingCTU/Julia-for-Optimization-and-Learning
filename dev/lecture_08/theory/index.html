<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theory of regression and classification · Numerical computing in Julia</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Numerical computing in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Numerical computing in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit">Numerical computing in Julia</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why_julia/">Why Julia?</a></li><li><a class="tocitem" href="../../howto/">How to...</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Mathematical operations and Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluation</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of Variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception Handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/basics/">Package management</a></li><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">5: Composite types and constructors</span></li><li><span class="tocitem">6: Modules and enviroments</span></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox" checked/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Theory of regression and classification</a><ul class="internal"><li><a class="tocitem" href="#Linear-regression"><span>Linear regression</span></a></li><li><a class="tocitem" href="#Logistic-regression"><span>Logistic regression</span></a></li></ul></li><li><a class="tocitem" href="../linear/">Linear regression</a></li><li><a class="tocitem" href="../logistic/">Logistic regression</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">11: Ordinary differential equations</span></li><li><span class="tocitem">12: Statistics I.</span></li><li><span class="tocitem">13: Statistics II.</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">8: Regression and classification</a></li><li class="is-active"><a href>Theory of regression and classification</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theory of regression and classification</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_08/theory.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Theory-of-regression-and-classification"><a class="docs-heading-anchor" href="#Theory-of-regression-and-classification">Theory of regression and classification</a><a id="Theory-of-regression-and-classification-1"></a><a class="docs-heading-anchor-permalink" href="#Theory-of-regression-and-classification" title="Permalink"></a></h1><p>Regression and classification are a part of machine learning which try to predict certain variables based on labelled data.</p><ul><li>Regression predicts a continuous variable (such as height based on weight).</li><li>Classification predict a variable with a finite number of states (such as cat/dog/none from images).</li></ul><h2 id="Linear-regression"><a class="docs-heading-anchor" href="#Linear-regression">Linear regression</a><a id="Linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression" title="Permalink"></a></h2><p>Linear regression requires a dataset with data points (samples) <span>$x_i$</span> and labels <span>$y_i$</span>. It uses a linear classifier to minimize the error between the prediction <span>$w^\top x_i$</span> and the label <span>$y_i$</span>, that is</p><p class="math-container">\[(w^\top x_i - y_i)^2.\]</p><p>Since we are interested in average performance, we sum this (mean square) error over all samples</p><p class="math-container">\[\operatorname{minimize}\qquad \sum_{i=1}^n (w^\top x_i - y_i)^2.\]</p><p>Many algorithms use average (mean) instead of sum. However, both these formulations are equivalent.</p><p>In this case, it is simpler to work in the matrix notation, where we form a matrix <span>$X$</span> whose rows are the samples <span>$x_i$</span>. It is not difficult to show that the previous problem is equivalent to</p><p class="math-container">\[\operatorname{minimize}\qquad \|Xw - y\|^2,\]</p><p>where the norm is the <span>$l_2$</span> norm. Since this is a convex quadratic problem, it is equivalent to its optimality conditions. Setting the derivative to zero yields</p><p class="math-container">\[2X^\top (Xw-y) = 0.\]</p><p>From here, we obtain the closed-form solution to the linear regression</p><p class="math-container">\[w = (X^\top X)^{-1}X^\top y.\]</p><div class = "info-body">
<header class = "info-header">Closed-form solution</header><p><p>Linear regression is probably the only machine learning model with a closed-form solution. All other models must be solved by iterative algorithms such as gradient descent. In some cases, it may be advantageous to use iterative algorithms even for linear regression. This includes, for example, the case of a large number of features <span>$m$</span> because then <span>$X^\top X$</span> is an <span>$m\times m$</span> matrix which may be difficult to invert.</p></p></div><div class = "info-body">
<header class = "info-header">Linear classifiers</header><p><p>We realize that</p><p class="math-container">\[w^\top x + b = (w, b)^\top \begin{pmatrix}x \\ 1\end{pmatrix}.\]</p><p>That means that if we add <span>$1$</span> to each sample <span>$x_i$</span>, it is sufficient to consider the classifier in the form <span>$w^\top x$</span> without the bias (shift, intercept) <span>$b$</span>. This allows for simpler implementation.</p></p></div><h2 id="Logistic-regression"><a class="docs-heading-anchor" href="#Logistic-regression">Logistic regression</a><a id="Logistic-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Logistic-regression" title="Permalink"></a></h2><p>The name logistic regression is misleading because it is actually a classification problem. In its simplest form, it assumes binary labels <span>$y\in\{0,1\}$</span>. It considers the linear classifier <span>$f(x)=w^\top x$</span> and predicts the positive class with probability</p><p class="math-container">\[\mathbb{P}(y=1\mid x) = \sigma(f(w)) = \frac{1}{1+e^{-w^\top x}},\]</p><p>where</p><p class="math-container">\[\sigma(z) = \frac{1}{1+e^{-z}} = \frac{e^z}{1+e^z}\]</p><p>is the sigmoid function. The probability of the negative class is then</p><p class="math-container">\[\mathbb{P}(y=0\mid x) = 1 - \sigma(f(w)) = \frac{e^{-w^\top x}}{1+e^{-w^\top x}}.\]</p><p>Denoting <span>$\hat y$</span> the probabily of predicting <span>$1$</span>, the loss function is the cross-entropy loss</p><p class="math-container">\[\operatorname{loss}(y,\hat y) = - y\log \hat y - (1-y)\log(1-\hat y).\]</p><p>It is not difficult to show that then the logistic regression problems reads</p><p class="math-container">\[\operatorname{minimize}\qquad \frac1n\sum_{i=1}^n\left(\log(1+e^{-w^\top x_i}) + (1-y_i)w^\top x_i \right).\]</p><h4 id="Numerical-method"><a class="docs-heading-anchor" href="#Numerical-method">Numerical method</a><a id="Numerical-method-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-method" title="Permalink"></a></h4><p>The logistic regression can be optimized by Newton&#39;s method. Denoting the loss function <span>$L(w)$</span>, its partial derivative with respect to one component equals to</p><p class="math-container">\[\begin{aligned}
\frac{\partial L}{\partial w_j}(w) &amp;= \frac1n\sum_{i=1}^n\left(-\frac{1}{1+e^{-w^\top x_i}}e^{-w^\top x_i}x_{i,j} + (1-y_i)x_{i,j} \right) \\
&amp;= \frac1n\sum_{i=1}^n\left(-\frac{1}{1+e^{w^\top x_i}}x_{i,j} + (1-y_i)x_{i,j} \right),
\end{aligned}\]</p><p>where <span>$x_{i,j}$</span> is the <span>$j$</span>-th component of <span>$x_i$</span> (it is also the <span>$(i,j)$</span> entry of matrix <span>$X$</span>). The second partial derivative amounts to</p><p class="math-container">\[\frac{\partial^2 L}{\partial w_j \partial w_k}(w) = \frac1n\sum_{i=1}^n \frac{1}{(1+e^{w^\top x_i})^2}e^{w^\top x_i}x_{i,j}x_{i,k} = \frac1n\sum_{i=1}^n \hat y_i(1-\hat y_i)x_{i,j}x_{i,k}.\]</p><p>Now we will write it in a more compact notation (recall that <span>$x_i$</span> is a column vector). We have</p><p class="math-container">\[\begin{aligned}
\nabla L(w) &amp;= \frac1n \sum_{i=1}^n \left((\hat y_i-1)x_i + (1-y_i)x_i \right) = \frac1n \sum_{i=1}^n (\hat y_i-y_i)x_i, \\ 
\nabla^2 L(w) &amp;= \frac 1n \sum_{i=1}^n\hat y_i(1-\hat y_i)x_i x_i^\top.
\end{aligned}\]</p><p>If the fit is perfect, <span>$y_i=\hat y_i$</span>, then the Jacobian <span>$\nabla L(w)$</span> is zero. Then the optimizer minimized the objective and satisfied the optimality condition.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_07/exercises/">« Exercises</a><a class="docs-footer-nextpage" href="../linear/">Linear regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 11 January 2021 13:25">Monday 11 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
