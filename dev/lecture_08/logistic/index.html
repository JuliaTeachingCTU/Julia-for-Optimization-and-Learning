<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Logistic regression · Numerical computing in Julia</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Numerical computing in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Numerical computing in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit">Numerical computing in Julia</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why_julia/">Why Julia?</a></li><li><a class="tocitem" href="../../howto/">How to...</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Mathematical operations and Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluation</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of Variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception Handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Useful packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/packages/">Packages</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">5: Composite types and constructors</span></li><li><span class="tocitem">6: Modules and enviroments</span></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Theory of optimization</a></li><li><a class="tocitem" href="../../lecture_07/gradients/">Visualization of gradients</a></li><li><a class="tocitem" href="../../lecture_07/numerical_methods/">Numerical methods</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox" checked/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../linear/">Linear regression</a></li><li class="is-active"><a class="tocitem" href>Logistic regression</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">???</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">11: Ordinary differential equations</span></li><li><span class="tocitem">12: Statistics I.</span></li><li><span class="tocitem">13: Statistics II.</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">8: Regression and classification</a></li><li class="is-active"><a href>Logistic regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Logistic regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_08/logistic.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Logistic-regression"><a class="docs-heading-anchor" href="#Logistic-regression">Logistic regression</a><a id="Logistic-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Logistic-regression" title="Permalink"></a></h1><p>The previous part dealt with predicting a continous variables. This part will handle predicting one of two classes.</p><p>Load the data as before</p><pre><code class="language-julia">using BSON: @load

file_name = joinpath(&quot;data&quot;, &quot;iris.bson&quot;)
@load file_name X y y_name</code></pre><p>The data contain three classes <code>[1 2 3]</code>. However, in the theory we considered only binary problems with two classes. We therefore cheat.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Write function <code>modify_data</code> which converts <code>X</code> and <code>y</code> into binary dataset in the following way: label 1 will be deleted, label 2 will be the negative class and label 3 will be the positive class. For <code>X</code> consider only columns 3 and 4. This will create a two-dimensional feature vector, which will enable nice visualizations.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Since we want to keep <span>$y\in\{2,3\}$</span>, we store the indices into <code>i</code>. Then we consider reduce <code>X</code>, <code>y</code> and <code>y_name</code> into the correct rows and columns.</p><pre><code class="language-julia">function modify_data(X, y, y_name)
    ii = y .&gt; 1.5
    return X[ii,3:4], y[ii] .&gt; 2.5, y_name[2:3]
end</code></pre><p>Finally, we call the function</p><pre><code class="language-julia">X, y, y_name = modify_data(X, y, y_name)</code></pre></p></details><p>When designing a classification method, a good practice is to perform an analysis of the data. That may include checking for NaNs, infinite values, obvious errors, standard deviations of features or others. Here, we only plot the data.  </p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Since <code>X</code> has two features (columns), it is simple to visualize. Use scatter plot to show the data. Use different colours for different classes. Try to produce a nice graph by including names of classes (in <code>y_name</code>) and axis labels (petal length and petal width).</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>This should be known by now. The only possibly unknown command is <code>legend=:topleft</code> to move the legend to the top-left corner.</p><pre><code class="language-julia">using Plots

scatter(X[y.==0,1], X[y.==0,2], label=y_name[1], legend=:topleft,
    xlabel=&quot;Petal length&quot;, ylabel=&quot;Petal width&quot;)
scatter!(X[y.==1,1], X[y.==1,2], label=y_name[2])</code></pre></p></details><p><img src="../iris1.svg" alt/></p><p>We see that the classes are almost perfectly separable. It would not be difficult to estimate the separating hyperplane by hand. However, we will do it automatically.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Write a function <code>log_reg</code> which takes as an input the dataset, an initial point and uses the Newton&#39;s method to find the optimal weights <span>$w$</span>. Print the result when started from the zero point.</p><p>It would be possible to use the code <code>optim(f, g, x, s::Step)</code> from the previous lecture and define only the step function <code>s</code> for the Newton&#39;s method. However, sometimes it may be better to write simple functions separately instead of using more complex machinery.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>To write the desired function, we need to implement the gradient and Hessian from derived in the theoretical lecture. First we need to create <span>$\hat y$</span>. We may use for loop notation <code>[1/(1+exp(-w&#39;*x)) for x in eachrow(X)]</code>. However, in this case, it is simpler to use matrix operations <code>1 ./(1 .+exp.(-X*w))</code> to get the same result. The gradient can be written in the same way. Again, there are two ways, we use the matrix notation. For the Hessian, we first create <code>X_mult = [row*row&#39; for row in eachrow(X)]</code> which computes all products <span>$x_ix_i^\top$</span>. Note that this is outside the for loop for iterations, as it can be computed only once. This will create an array of length <span>$100$</span>, each element of this array will be a <span>$2\times$</span> matrix. Since it is an array, we may multiply it by <code>y_hat.*(1 .-y_hat)</code>. Since <code>mean</code> from the <code>Statistics</code> package operates on any array, we can call it (or similarly <code>sum</code>). We may use <code>mean(???)</code> but we find the alternative  <code>??? |&gt; mean</code> more readable in this case. To update we use <code>hess \ grad</code> as explained in the previous lecture for the Newton&#39;s method.</p><pre><code class="language-julia">using Statistics

function log_reg(X, w; max_iter=100)
    X_mult = [row*row&#39; for row in eachrow(X)]
    for i in 1:max_iter
        y_hat = 1 ./(1 .+exp.(-X*w))
        grad = X&#39;*(y_hat.-y) / size(X,1)
        hess = y_hat.*(1 .-y_hat).*X_mult |&gt; mean
        w -= hess \ grad
    end
    return w
end</code></pre><p>After the tough work, it remains to call the it.</p><pre><code class="language-julia">w0 = zeros(size(X,2))
w_wrong = log_reg(X, w0)</code></pre><p>The name <code>w_wrong</code> suggests that something is wrong. The problem is that the logistic regression was called with the original dataset. However, in doing so, there is no intercept. For this reason, we need to modify <code>X</code> first by including the column of all ones and then calling it again.</p><pre><code class="language-julia">X_ext = hcat(X, repeat([1],size(X,1)))
w = log_reg(X_ext, zeros(size(X_ext,2)))</code></pre><p>Hooray, this time it is correct.</p></p></details><p>If you obtained</p><pre class="documenter-example-output">[-2.0041, 6.1127]</pre><p>the function for solving the logistic regression is correct. But you call it in a wrong way. The correct solution is</p><pre class="documenter-example-output">[5.7545, 10.4467, -45.2723]</pre><p>We can now show the solution. Since the intercept is the third component (therefore <span>$x_3=1$</span>), the separating hyperplane takes form</p><p class="math-container">\[w_1x_1 + x_2x_2 + w_3 = 0.\]</p><p>To express it as a function, we obtain</p><p class="math-container">\[\operatorname{sep}(x_1) = \frac{-w_1x_1 - w_3}{w_2}.\]</p><p>Now we plot it.</p><pre><code class="language-julia">scatter(X[y.==0,1], X[y.==0,2], label=y_name[1], legend=:topleft,
    xlabel=&quot;Petal length&quot;, ylabel=&quot;Petal width&quot;)
scatter!(X[y.==1,1], X[y.==1,2], label=y_name[2])

f_hyper = x -&gt; (-w[3]-w[1]*x)/w[2]
x_lim = [minimum(X[:,1])-0.1; maximum(X[:,1])+0.1]
plot!(x_lim, f_hyper.(x_lim), label=&quot;Separating hyperplane&quot;, line=(:black,3),
    ylim=(minimum(X[:,2])-0.1,maximum(X[:,2])+0.1))</code></pre><p><img src="../iris2.svg" alt/></p><p>This is the optimal solution obtained by the logistic regression. Since the norm of the gradient</p><pre><code class="language-julia">using LinearAlgebra

y_hat = 1 ./(1 .+exp.(-X_ext*w))
grad = X_ext&#39;*(y_hat.-y) / size(X_ext,1)
norm(grad)</code></pre><pre class="documenter-example-output">4.499770257220964e-16</pre><p>equals to zero, we found a stationary point. It can be shows that logistic regression is a convex problem and therefore, we found a global solution.</p><p>The picture shows that there are misclassified samples. The mext example analyses them.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Print how mnay samples were correctly and incorrectly classified.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Since <span>$\hat y_i$</span> is a probability that sample is of the positive class, we will predict that it is positive if this probability is greater than <span>$\frac 12$</span>. Then it suffices to compare the predictions <code>pred</code> with the correct labels <code>y</code>.</p><pre><code class="language-julia">pred = y_hat .&gt;= 0.5
&quot;Correct number of predictions: &quot; * string(sum(pred .== y))
&quot;Wrong   number of predictions: &quot; * string(sum(pred .!= y))</code></pre><p>There is an alternative (but equivalent way). Since the separating hyperplane is of form <span>$w^\top x$</span>, we predict that a sample is positive whenever <span>$w^\top x\ge 0$</span>. Write the code and try to reason why these two approaches ar equivalent.</p></p></details><p>The correct answer is</p><pre class="documenter-example-output">Correct number of predictions: 94
Wrong   number of predictions: 6</pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../linear/">« Linear regression</a><a class="docs-footer-nextpage" href="../exercises/">Exercises »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 2 January 2021 09:53">Saturday 2 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
