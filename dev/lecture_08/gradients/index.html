<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gradients · Julia for Optimization and Learning</title><meta name="title" content="Gradients · Julia for Optimization and Learning"/><meta property="og:title" content="Gradients · Julia for Optimization and Learning"/><meta property="twitter:title" content="Gradients · Julia for Optimization and Learning"/><meta name="description" content="Documentation for Julia for Optimization and Learning."/><meta property="og:description" content="Documentation for Julia for Optimization and Learning."/><meta property="twitter:description" content="Documentation for Julia for Optimization and Learning."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Julia for Optimization and Learning</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/installation/">Installation</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Basics I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Basics II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/functions/">Funcion basics</a></li><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_03/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_03/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_03/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_03/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_03/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_04/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_04/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_04/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06_07/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06_07/develop/">Package development</a></li></ul></li><li><span class="tocitem">7: Code organization II</span></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox" checked/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Introduction to continuous optimization</a></li><li class="is-active"><a class="tocitem" href>Gradients</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Visualization-of-gradients"><span>Visualization of gradients</span></a></li><li><a class="tocitem" href="#comp-grad"><span>Computation of gradients</span></a></li></ul></li><li><a class="tocitem" href="../unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_09/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_09/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_11/iris/">Introduction to Flux</a></li><li><a class="tocitem" href="../../lecture_11/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_11/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/sparse/">Linear regression with sparse constraints</a></li><li><a class="tocitem" href="../../lecture_12/monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../../lecture_12/glm/">Linear regression revisited</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">8: Optimization</a></li><li class="is-active"><a href>Gradients</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Gradients</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_08/gradients.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Gradients"><a class="docs-heading-anchor" href="#Gradients">Gradients</a><a id="Gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Gradients" title="Permalink"></a></h1><p>To minimize a function, it is useful to use derivatives. For a function <span>$f:\mathbb{R}\to \mathbb{R}$</span>, its gradient is defined by</p><p class="math-container">\[f&#39;(x) = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}.\]</p><p>For a mapping  <span>$f:\mathbb{R}^n\to \mathbb{R}^m$</span>, its Jacobian is a matrix <span>$\nabla f(x)$</span> of size <span>$m\times n$</span> of partial derivatives</p><p class="math-container">\[(\nabla f(x))_{i,j} = \frac{\partial f_i}{\partial x_j}(x) = \lim_{h\to 0}\frac{f_i(x_1,\dots,x_{j-1},x_j+h,x_{j+1},\dots,x_n)-f(x_1,\dots,x_n)}{h}.\]</p><p>The formal definition is more complicated, but this one is better for visualization.</p><div class="admonition is-info"><header class="admonition-header">Confusion:</header><div class="admonition-body"><p>Gradient <span>$\nabla f(x)$</span> of a function <span>$f:\mathbb{R}^n\to\mathbb{R}$</span> should be of size  <span>$1\times n$</span> but it is commonly considered as <span>$n\times 1$</span>.</p></div></div><p>Functions are usually complicated, and this definition cannot be used to compute the gradient. Instead, the objective function <span>$f$</span> is rewritten as a composition of simple functions, these simple functions are differentiated, and the chain rule is applied to get <span>$\nabla f$</span>.</p><div class="admonition is-todo"><header class="admonition-header">Theorem: Chain</header><div class="admonition-body"><p>Consider two differentiable functions <span>$f:\mathbb{R}^m\to\mathbb{R}^s$</span> and <span>$g:\mathbb{R}^n\to\mathbb{R}^m$</span>. Then its composition <span>$h(x) := f(g(x))$</span> is differentiable with Jacobian</p><p class="math-container">\[\nabla h(x) = \nabla f(g(x))\nabla g(x).\]</p></div></div><p>The derivative is the direction of the steepest ascent. The following figure shows the vector field of derivatives, where each arrow shows the direction and size of derivatives at the points in the domain. Since a function has the same function values along its contour lines and since derivate is the direction of the steepest ascent, derivatives are perpendicular to contour lines. The figure also shows that local minima and local maxima have zero derivatives.</p><p><img src="../grad3.svg" alt/></p><h1 id="Visualization-of-gradients"><a class="docs-heading-anchor" href="#Visualization-of-gradients">Visualization of gradients</a><a id="Visualization-of-gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Visualization-of-gradients" title="Permalink"></a></h1><p>For the numerical experiments, we will consider the following function</p><p class="math-container">\[f(x) = \sin(x_1 + x_2) + \cos(x_1)^2\]</p><p>on domain <span>$[-3,1]\times [-2,1]$</span>.</p><div class="admonition is-warning"><header class="admonition-header">Exercise: Contour plot</header><div class="admonition-body"><p>Write a function <code>g(x)</code> which computes the derivative of <span>$f$</span> at a point  <span>$x$</span>. Plot the contours of <span>$f$</span> on the domain. </p><p><strong>Hint</strong>: Use the keyword argument <code>color = :jet</code> for better visualization.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>Function <code>f(x)</code> takes as an input a vector of two dimensions and returns a scalar. Therefore, the gradient is a two-dimensional vector, which we create by <code>[?; ?]</code>. Its components are computed from the chain rule.</p><pre><code class="language-julia hljs">f(x) = sin(x[1] + x[2]) + cos(x[1])^2
g(x) = [cos(x[1] + x[2]) - 2*cos(x[1])*sin(x[1]); cos(x[1] + x[2])]</code></pre><p>Since sometimes it is better to use notation <span>$f(x)$</span> and sometimes <span>$f(x_1,x_2)$</span>, we overload the function <code>f</code>.</p><pre><code class="language-julia hljs">f(x1,x2) = f([x1;x2])

f([0; 0])
f(0, 0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.0
1.0</code></pre><p>We use the <code>Plots</code> package for plotting. We create the discretization <code>xs</code> and <code>ys</code> of both axis and then call the <code>contourf</code> function.</p><pre><code class="language-julia hljs">using Plots

xs = range(-3, 1, length = 40)
ys = range(-2, 1, length = 40)

contourf(xs, ys, f, color = :jet)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_08/grad1.svg&quot;</code></pre></div></details><p><img src="../grad1.svg" alt/></p><h2 id="comp-grad"><a class="docs-heading-anchor" href="#comp-grad">Computation of gradients</a><a id="comp-grad-1"></a><a class="docs-heading-anchor-permalink" href="#comp-grad" title="Permalink"></a></h2><p>The simplest way to compute the gradients is to use a finite difference approximation. It replaces the limit in</p><p class="math-container">\[f&#39;(x) = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\]</p><p>by fixing some <span>$h$</span> and approximates the gradient by</p><p class="math-container">\[f&#39;(x) \approx \frac{f(x+h)-f(x)}{h}.\]</p><div class="admonition is-warning"><header class="admonition-header">Exercise: Finite difference approximation</header><div class="admonition-body"><p>Write a function <code>finite_difference</code> which computes the approximation of <span>$f&#39;(x)$</span> by finite differences. The inputs are a function <span>$f:\mathbb R\to\mathbb R$</span> and a point <span>$x\in\mathbb{R}$</span>. It should have an optional input <span>$h\in\mathbb{R}$</span>, for which you need to choose a reasonable value.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>It is sufficient to rewrite the formula above. Since the argument <code>h</code> is optional, it should be after <code>;</code>. Its good default value is anything between <span>$10^{-10}$</span> and <span>$10^{-5}$</span>. We specify <code>x::Real</code> as a sanity check for the case when a function of more variables is passed as input.</p><pre><code class="language-julia hljs">finite_difference(f, x::Real; h=1e-8) = (f(x+h) - f(x)) / h</code></pre></div></details><p>This way of computing the gradient has two disadvantages:</p><ol><li>It is slow. For a function of <span>$n$</span> variables, we need to evaluate the function at least <span>$n+1$</span> times to get the whole gradient.</li><li>It is not precise, as the following example shows.</li></ol><div class="admonition is-warning"><header class="admonition-header">Exercise: Finite difference approximation</header><div class="admonition-body"><p>Fix a point <span>$x=(-2,-1)$</span>. For a proper discretization of <span>$h\in [10^{-15}, 10^{-1}]$</span> compute the finite difference approximation of the partial derivative of <span>$f$</span> with respect to the second variable.</p><p>Plot the dependence of this approximation on <span>$h$</span>. Add the true derivative computed from <code>g</code>.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>To compute the partial derivative with respect to the second argument, we need to fix the first argument and vary only the second one. We create an anonymous function <code>y -&gt; f(-2, y)</code> and another function <code>fin_diff</code> which for an input <code>h</code> computes the finite difference.</p><pre><code class="language-julia hljs">x = [-2; -1]
fin_diff(h) = finite_difference(y -&gt; f(x[1], y), x[2]; h=h)</code></pre><p>The true gradient is computed by <code>g(x)</code>. It returns a vector of length two. Since we need only the partial derivative with respect to the second component, we select it by adding  <code>[2]</code>.</p><pre><code class="language-julia hljs">true_grad = g(x)[2]</code></pre><p>Now we create the discretization of <span>$h$</span> in <code>hs</code>. When the orders of magnitude are so different, the logarithmic scale should be used. For this reason, we create a uniform discretization of the interval <span>$[-15,-1]$</span> and then use it as an exponent.</p><pre><code class="language-julia hljs">hs = 10. .^ (-15:0.01:-1)</code></pre><p>There are many possibilities of how to create the plot. Probably the simplest one is to plot the function <code>fin_diff</code> and then add the true gradient (which does not depend on <span>$h$</span> and is, therefore, a horizontal line) via <code>hline!</code>.</p><pre><code class="language-julia hljs">plot(hs, fin_diff,
    xlabel = &quot;h&quot;,
    ylabel = &quot;Partial gradient wrt y&quot;,
    label = [&quot;Approximation&quot; &quot;True gradient&quot;],
    xscale = :log10,
)

hline!([true_grad]; label =  &quot;True gradient&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_08/grad2.svg&quot;</code></pre></div></details><p><img src="../grad2.svg" alt/></p><p>The approximation is good if <span>$h$</span> is not too small or too large. It cannot be too large because the definition of the gradient considers the limit to zero. It cannot be too small because the numerical errors kick in. This is connected with machine precision, which is most vulnerable to subtracting two numbers of almost the same value. A simple example shows</p><p class="math-container">\[(x + h)^2 - x^2 = 2xh + h^2\]</p><p>but the numerical implementation</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; x = 1;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; h = 1e-13;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; (x+h)^2 - x^2</code><code class="nohighlight hljs ansi" style="display:block;">1.9984014443252818e-13</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; 2*x*h + h^2</code><code class="nohighlight hljs ansi" style="display:block;">2.0000000000001e-13</code></pre><p>gives an error already at the fourth valid digit. It is important to realize how numbers are stored. Julia uses the <a href="https://en.wikipedia.org/wiki/IEEE_754">IEEE 754 standard</a>. For example, <code>Float64</code> uses 64 bits to store the number, from which 1 bit represents the sign, 11 bits the exponent and 52 bits the significand precision. As <span>$2^{52}\approx 10^{16}$</span>, numbers are stored with a 16-digit precision. Since the exponent is stored separately, it is possible to represent numbers smaller than the machine precision, such as <span>$10^{-25}$</span>. To prevent numerical errors, all computations are done in higher precision, and the resulting variable is rounded to the type precision.</p><p>Finally, we show how the gradients look like.</p><div class="admonition is-warning"><header class="admonition-header">Exercise: Direction of gradients</header><div class="admonition-body"><p>Reproduce the previous figure with the vector field of derivatives. Therefore, plot the contours of <span>$f$</span> and its gradients at a grid of its domain <span>$[-3,1]\times [-2,1]$</span>.</p><p><strong>Hint</strong>: when a plot is updated in a loop, it needs to be saved to a variable <code>plt</code> and then displayed via <code>display(plt)</code>.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>First we reduce the number of grid elements and plot the contour plot.</p><pre><code class="language-julia hljs">xs = range(-3, 1, length = 20)
ys = range(-2, 1, length = 20)

plt = contourf(xs, ys, f;
    xlims = (minimum(xs), maximum(xs)),
    ylims = (minimum(ys), maximum(ys)),
    color = :jet
)</code></pre><img src="7d83f4c0.svg" alt="Example block output"/><p>We use the same functions as before. Since we want to add a line, we use <code>plot!</code> instead of <code>plot</code>. We specify its parameters in an optional argument <code>line = (:arrow, 2, :black)</code>. These parameters add the pointed arrow, the thickness and the colour of the line. Since we do not want any legend, we use <code>label = &quot;&quot;</code>. Finally, since we want to create a grid, we make a loop over <code>xs</code> and <code>ys</code>.</p><pre><code class="language-julia hljs">α = 0.25
for x1 in xs, x2 in ys
    x = [x1; x2]
    x_grad = [x x.+α.*g(x)]

    plot!(x_grad[1, :], x_grad[2, :];
        line = (:arrow, 2, :black),
        label = &quot;&quot;,
    )
end
display(plt)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_08/grad3.svg&quot;</code></pre></div></details><p><img src="../grad3.svg" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../theory/">« Introduction to continuous optimization</a><a class="docs-footer-nextpage" href="../unconstrained/">Unconstrained optimization »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Thursday 26 September 2024 06:13">Thursday 26 September 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
