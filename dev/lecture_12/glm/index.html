<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear regression revisited · Julia for Optimization and Learning</title><meta name="title" content="Linear regression revisited · Julia for Optimization and Learning"/><meta property="og:title" content="Linear regression revisited · Julia for Optimization and Learning"/><meta property="twitter:title" content="Linear regression revisited · Julia for Optimization and Learning"/><meta name="description" content="Documentation for Julia for Optimization and Learning."/><meta property="og:description" content="Documentation for Julia for Optimization and Learning."/><meta property="twitter:description" content="Documentation for Julia for Optimization and Learning."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Julia for Optimization and Learning</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/installation/">Installation</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Basics I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Basics II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/functions/">Function basics</a></li><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_03/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_03/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_03/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_03/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_03/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_04/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_04/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_04/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/structure/">Package structure</a></li><li><a class="tocitem" href="../../lecture_06/workflow/">Development workflow</a></li><li><a class="tocitem" href="../../lecture_06/compatibility/">Package dependencies</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Code organization II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/tests/">Tests</a></li><li><a class="tocitem" href="../../lecture_07/documentation/">Documentation</a></li><li><a class="tocitem" href="../../lecture_07/extensions/">Extensions</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_08/gradients/">Gradients</a></li><li><a class="tocitem" href="../../lecture_08/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_09/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_09/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_11/iris/">Introduction to Flux</a></li><li><a class="tocitem" href="../../lecture_11/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_11/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox" checked/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../sparse/">Linear regression with sparse constraints</a></li><li><a class="tocitem" href="../monte/">Monte Carlo sampling</a></li><li class="is-active"><a class="tocitem" href>Linear regression revisited</a><ul class="internal"><li><a class="tocitem" href="#Theory-of-hypothesis-testing"><span>Theory of hypothesis testing</span></a></li><li><a class="tocitem" href="#Hypothesis-testing"><span>Hypothesis testing</span></a></li><li><a class="tocitem" href="#Theory-of-generalized-linear-models"><span>Theory of generalized linear models</span></a></li><li><a class="tocitem" href="#Linear-models"><span>Linear models</span></a></li><li><a class="tocitem" href="#Generalized-linear-models"><span>Generalized linear models</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">12: Statistics</a></li><li class="is-active"><a href>Linear regression revisited</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear regression revisited</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_12/glm.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="statistics"><a class="docs-heading-anchor" href="#statistics">Linear regression revisited</a><a id="statistics-1"></a><a class="docs-heading-anchor-permalink" href="#statistics" title="Permalink"></a></h1><p>This section revisits the linear regression. The classical statistical approach uses derives the same formulation for linear regression as the optimization approach. Besides point estimates for parameters, it also computes their confidence intervals and can test whether some parameters can be omitted from the model. We will start with hypothesis testing and then continue with regression.</p><p>Julia provides lots of statistical packages. They are summarized at the <a href="https://juliastats.org/">JuliaStats</a> webpage. This section will give a brief introduction to many of them.</p><h2 id="Theory-of-hypothesis-testing"><a class="docs-heading-anchor" href="#Theory-of-hypothesis-testing">Theory of hypothesis testing</a><a id="Theory-of-hypothesis-testing-1"></a><a class="docs-heading-anchor-permalink" href="#Theory-of-hypothesis-testing" title="Permalink"></a></h2><p>Hypothesis testing verifies whether data satisfy a given null hypothesis <span>$H_0$</span>. Most of the tests need some assumptions about the data, such as normality. Under the validity of the null hypothesis, the test derives that a transformation of the data follows some distribution. Then it constructs a confidence interval of this distribution and checks whether the transformed variable lies in this confidence interval. If it lies outside of it, the test rejects the null hypothesis. In the opposite case, it fails to reject the null hypothesis. The latter is different from confirming the null hypothesis. Hypothesis testing is like a grumpy professor during exams. He never acknowledges that a student knows the topic sufficiently, but he is often clear that the student does not know it.</p><p>An example is the one-sided <a href="https://en.wikipedia.org/wiki/Student&#39;s_t-test">Student&#39;s t-test</a> that verifies that a one-dimensional dataset has mean <span>$\mu$</span>. It can be generalized to compare the mean (performance) of two datasets. Under some assumptions, it derives that</p><p class="math-container">\[t = \sqrt{n}\frac{\hat \mu - \mu}{\hat\sigma}\]</p><p>follows the <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">Student&#39;s distribution</a> with <span>$n-1$</span> degrees of freedom. Here, <span>$n$</span> is the number of datapoints. Their mean is <span>$\hat \mu$</span> and their standard deviation <span>$\hat \sigma$</span>. Instead of computing the confidence interval, the usual way is to define the <a href="https://en.wikipedia.org/wiki/P-value"><span>$p$</span>-value</a></p><p class="math-container">\[p = 2\min\{\mathbb P(T\le t \mid H_0), \mathbb P(T\ge t\mid H_0)\}\]</p><p>If the <span>$p$</span>-value is smaller than a given threshold, usually <span>$5\%$</span>, the null hypothesis is rejected. In the opposite case, it is not rejected. The <span>$p$</span>-value is a measure of the probability that an observed difference could have occurred just by random chance.</p><h2 id="Hypothesis-testing"><a class="docs-heading-anchor" href="#Hypothesis-testing">Hypothesis testing</a><a id="Hypothesis-testing-1"></a><a class="docs-heading-anchor-permalink" href="#Hypothesis-testing" title="Permalink"></a></h2><p>We first randomly generate data from the normal distribution with zero mean.</p><pre><code class="language-julia hljs">using Random
using Statistics
using LinearAlgebra
using Plots

Random.seed!(666)

n = 1000
xs = randn(n)</code></pre><p>The following exercise performs the <span>$t$</span>-test to check whether the data come from a distribution with zero mean.</p><div class="admonition is-warning"><header class="admonition-header">Exercise:</header><div class="admonition-body"><p>Use the <span>$t$</span>-test to verify whether the samples were generated from a distribution with zero mean.</p><p><strong>Hints:</strong></p><ul><li>The Student&#39;s distribution is invoked by <code>TDist()</code>.</li><li>The probability <span>$\mathbb P(T\le t)$</span> equals to the <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">distribution function</a> <span>$F(t)$</span>, which can be called by <code>cdf</code>.</li></ul></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>We compute the statistic <span>$t$</span>, then define the Student&#39;s distribution with <span>$n-1$</span> degrees of freedom, evaluate the distribution function at <span>$t$</span> and finally compute the <span>$p$</span>-value.</p><pre><code class="language-julia hljs">using Distributions

t = mean(xs) / std(xs) * sqrt(n)

prob = cdf(TDist(n-1), t)
p = 2*min(prob, 1-prob)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.18717789140741847</code></pre><p>The <span>$p$</span>-value is significantly larger than <span>$5\%$</span>. Therefore, we cannot reject the zero hypothesis, which is fortunate because the data were generated from the normal distribution with zero mean.</p></div></details><p>Even though the computation of the <span>$p$</span>-value is simple, we can use the <a href="https://juliastats.org/HypothesisTests.jl/stable/">HypothesisTests</a> package. When we run the test, it gives us the same results as we computed.</p><pre><code class="language-julia hljs">using HypothesisTests

OneSampleTTest(xs)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">One sample t-test
-----------------
Population details:
    parameter of interest:   Mean
    value under h_0:         0
    point estimate:          0.0420876
    95% confidence interval: (-0.02049, 0.1047)

Test summary:
    outcome with 95% confidence: fail to reject h_0
    two-sided p-value:           0.1872

Details:
    number of observations:   1000
    t-statistic:              1.319878673760679
    degrees of freedom:       999
    empirical standard error: 0.03188744768562347
</code></pre><h2 id="Theory-of-generalized-linear-models"><a class="docs-heading-anchor" href="#Theory-of-generalized-linear-models">Theory of generalized linear models</a><a id="Theory-of-generalized-linear-models-1"></a><a class="docs-heading-anchor-permalink" href="#Theory-of-generalized-linear-models" title="Permalink"></a></h2><p>The statistical approach to linear regression is different from the one from machine learning. It also assumes a linear prediction function:</p><p class="math-container">\[\operatorname{predict}(w;x) = w^\top x.\]</p><p>Then it considers some invertible link function <span>$g:\mathbb R\to \mathbb R$</span> and assumes that <span>$y$</span> conditioned on <span>$x$</span> follows an apriori specified distribution with density <span>$f$</span>. The parameters of this distribution are unknown, but the distribution should satisfy the conditional expectation <span>$E(y\mid x) = g^{-1}(w^\top x)$</span>. The goal is to find the weights <span>$w$</span> by the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimate</a>. This technique maximizes the likelihood function:</p><p class="math-container">\[\operatorname{maximize}\qquad \prod_{i=1}^n f(y_i).\]</p><p>Since the density is the derivative of the distribution function, the term <span>$f(y_i)$</span> describes the &quot;probability&quot; of <span>$y_i$</span> under density <span>$f$</span>. If <span>$y_i$</span> are independent, then the product is the joint probability for all samples. Therefore, maximizing the likelihood function amounts to finding the parameters of the apriori specified distribution such that the observed samples <span>$y_i$</span> have the highest probability. Since these distributions are usually taken from the <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential family</a>, the log-likelihood</p><p class="math-container">\[\operatorname{maximize}\qquad \sum_{i=1}^n \log f(y_i).\]</p><p>is often maximized. Since the logarithm is an increasing function, these two formulas are equivalent.</p><h4 id="Case-1:-Linear-regression"><a class="docs-heading-anchor" href="#Case-1:-Linear-regression">Case 1: Linear regression</a><a id="Case-1:-Linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Case-1:-Linear-regression" title="Permalink"></a></h4><p>The first case considers <span>$g(z)=z$</span> to be the identity function and <span>$y\mid x$</span> with the <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> <span>$N(\mu_i, \sigma^2)$</span>. Then</p><p class="math-container">\[w^\top x_i = g^{-1}(w^\top x_i) = \mathbb E(y_i \mid x_i) = \mu_i,\]</p><p>and, therefore, we need the solve the following optimization problem:</p><p class="math-container">\[\operatorname{maximize}_w\qquad \sum_{i=1}^n \log \left(\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-(y_i - w^\top x_i)^2}{2\sigma^2}\right)\right).\]</p><p>Since we maximize with respect to <span>$w$</span>, most terms behave like constants, and this optimization problem is equivalent to</p><p class="math-container">\[\operatorname{minimize}_w\qquad \sum_{i=1}^n (y_i - w^\top x_i)^2.\]</p><p>This is precisely linear regression as derived in the previous lectures.</p><h4 id="Case-2:-Logistic-regression"><a class="docs-heading-anchor" href="#Case-2:-Logistic-regression">Case 2: Logistic regression</a><a id="Case-2:-Logistic-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Case-2:-Logistic-regression" title="Permalink"></a></h4><p>The second case considers <span>$g(z)=\log z$</span> to be the logarithm function and <span>$y\mid x$</span> with the <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> <span>$Po(\lambda)$</span>. The inverse function to <span>$g$</span> is <span>$g^{-1}(z)=e^z$</span>. Since the Poisson distribution has non-negative discrete values with probabilities <span>$\mathbb P(Y=k) = \frac{1}{k!}\lambda^ke^{-\lambda}$</span>, labels <span>$y_i$</span> must also be non-negative integers. The same formula for the conditional expectation as before yields:</p><p class="math-container">\[e^{w^\top x_i} = g^{-1}(w^\top x_i) = \mathbb E(y_i \mid x_i) = \lambda_i.\]</p><p>Plugging this term into the log-likelihood function results in the following optimization problem:</p><p class="math-container">\[\operatorname{maximize}_w\qquad \sum_{i=1}^n\log\left( \frac{1}{y_i!}\lambda_i^{y_i} e^{-\lambda_i}\right).\]</p><p>By using the formula for <span>$\lambda_i$</span> and getting rid of constants, we transform this problem into</p><p class="math-container">\[\operatorname{minimize}_w\qquad  \sum_{i=1}^n \left(e^{w^\top x_i} - y_iw^\top x_i\right).\]</p><p>This function is similar to the one derived for logistic regression.</p><h2 id="Linear-models"><a class="docs-heading-anchor" href="#Linear-models">Linear models</a><a id="Linear-models-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-models" title="Permalink"></a></h2><p>We will use the <a href="https://vincentarelbundock.github.io/Rdatasets/doc/plm/Snmesp.html">Employment and Wages in Spain</a> dataset because it is slightly larger than the iris dataset. It contains 5904 observations of wages from 738 companies in Spain from 1983 to 1990. We will estimate the dependence of wages on other factors such as employment or cash flow. We first load the dataset and transform the original log-wages into non-normalized wages. We use base <span>$2$</span> to obtain relatively small numbers.</p><pre><code class="language-julia hljs">using RDatasets

wages = dataset(&quot;plm&quot;, &quot;Snmesp&quot;)
wages.W = 2. .^ (wages.W)</code></pre><p>We can use the already known procedure to compute the best fit. </p><pre><code class="language-julia hljs">X = Matrix(wages[:, [:N, :Y, :I, :K, :F]])
X = hcat(ones(size(X,1)), X)
y = wages[:, :W]

w0 = (X&#39;*X) \ (X&#39;*y)</code></pre><p>Another possibility is to use the package <a href="https://juliastats.org/GLM.jl/stable/">GLM</a> and its command <code>lm</code> for linear models. </p><pre><code class="language-julia hljs">using GLM

model = lm(@formula(W ~ 1 + N + Y + I + K + F), wages)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">StatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}

W ~ 1 + N + Y + I + K + F

Coefficients:
─────────────────────────────────────────────────────────────────────────────────
                   Coef.  Std. Error       t  Pr(&gt;|t|)     Lower 95%    Upper 95%
─────────────────────────────────────────────────────────────────────────────────
(Intercept)  -0.233407    0.0262272    -8.90    &lt;1e-18  -0.284821     -0.181992
N            -0.399454    0.00760488  -52.53    &lt;1e-99  -0.414362     -0.384545
Y             1.27339     0.024799     51.35    &lt;1e-99   1.22477       1.322
I            -0.806594    0.0187143   -43.10    &lt;1e-99  -0.843281     -0.769907
K             0.00869993  0.00408397    2.13    0.0332   0.000693846   0.016706
F            -3.44324e-5  5.93769e-6   -5.80    &lt;1e-08  -4.60725e-5   -2.27924e-5
─────────────────────────────────────────────────────────────────────────────────</code></pre><p>The table shows the parameter values and their confidence intervals. Besides that, it also tests the null hypothesis <span>$H_0: w_j = 0$</span> whether some of the regression coefficients can be omitted. The <span>$t$</span> statistics is in column <code>t</code>, while its <span>$p$</span>-value in column <code>Pr(&gt;|t|)</code>. The next exercise checks whether we can achieve the same results with fewer features.</p><div class="admonition is-warning"><header class="admonition-header">Exercise:</header><div class="admonition-body"><p>Check that the solution computed by hand and by <code>lm</code> are the same.</p><p>Then remove the feature with the highest <span>$p$</span>-value and observe whether there was any performance drop. The performance is usually evaluated by the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coeffient of determination</a> denoted by <span>$R^2\in[0,1]$</span>. Its higher values indicate a better model.</p><p><strong>Hint</strong>: Use functions <code>coef</code> and <code>r2</code>.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>Since the parameters for both approaches are almost the same, the approaches give the same result. </p><pre><code class="language-julia hljs">norm(coef(model) - w0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.2569735303973925e-13</code></pre><p>The table before this exercise shows that the <span>$p$</span>-value for feature <span>$K$</span> is <span>$3.3\%$</span>. We define the reduced model without this feature.</p><pre><code class="language-julia hljs">model_red = lm(@formula(W ~ 1 + N + Y + I + F), wages)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">StatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}

W ~ 1 + N + Y + I + F

Coefficients:
────────────────────────────────────────────────────────────────────────────────
                   Coef.  Std. Error       t  Pr(&gt;|t|)    Lower 95%    Upper 95%
────────────────────────────────────────────────────────────────────────────────
(Intercept)  -0.252352    0.0246807   -10.22    &lt;1e-23  -0.300735    -0.203969
N            -0.398952    0.0076035   -52.47    &lt;1e-99  -0.413857    -0.384046
Y             1.29473     0.0226924    57.06    &lt;1e-99   1.25024      1.33922
I            -0.818356    0.0178865   -45.75    &lt;1e-99  -0.85342     -0.783292
F            -3.43501e-5  5.93935e-6   -5.78    &lt;1e-08  -4.59934e-5  -2.27068e-5
────────────────────────────────────────────────────────────────────────────────</code></pre><p>Now we show the performances of both models.</p><pre><code class="language-julia hljs">(r2(model), r2(model_red))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(0.509321690482024, 0.5089441549925815)</code></pre><p>Since we observe only a small performance drop, we could omit this feature without changing the model prediction capability.</p></div></details><p>The core assumption of this approach is that <span>$y$</span> follows the normal distribution. We use the <code>predict</code> function for predictions and then use the <code>plot_histogram</code> function written earlier to plot the histogram and a density of the normal distribution. For the normal distribution, we need to specify the correct mean and variance.</p><pre><code class="language-julia hljs">y_hat = predict(model)

plot_histogram(y_hat, x -&gt; pdf(Normal(mean(y_hat), std(y_hat)), x))</code></pre><img src="e878855f.svg" alt="Example block output"/><p>Another possibility would be the <code>fit</code> function from the <code>Distributions</code> package.</p><pre><code class="language-julia hljs">plot_histogram(y_hat, x -&gt; pdf(fit(Normal, y_hat), x))</code></pre><img src="e37b97f4.svg" alt="Example block output"/><p>The results look identical. The distribution resembles the normal distribution, but there are some differences. We can use the more formal <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov</a>, which verifies whether a sample comes from some distribution.</p><pre><code class="language-julia hljs">test_normality = ExactOneSampleKSTest(y_hat, Normal(mean(y_hat), std(y_hat)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Exact one sample Kolmogorov-Smirnov test
----------------------------------------
Population details:
    parameter of interest:   Supremum of CDF differences
    value under h_0:         0.0
    point estimate:          0.0217836

Test summary:
    outcome with 95% confidence: reject h_0
    two-sided p-value:           0.0073

Details:
    number of observations:   5904
</code></pre><p>The result is expected. The <span>$p$</span>-value is close to <span>$1\%$</span>, which means that we reject the null hypothesis that the data follow the normal distribution even though it is not entirely far away.</p><h2 id="Generalized-linear-models"><a class="docs-heading-anchor" href="#Generalized-linear-models">Generalized linear models</a><a id="Generalized-linear-models-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-linear-models" title="Permalink"></a></h2><p>While the linear models do not transform the labels, the generalized models transform them by the link function. Moreover, they allow choosing other than the normal distribution for labels. Therefore, we need to specify the link function <span>$g$</span> and the distribution of <span>$y \mid x$</span>.</p><p>We repeat the same example with the link function <span>$g(z) = \sqrt{z}$</span> and the <a href="https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution">inverse Gaussian</a> distribution for the labels. Since we want to use the generalized linear model, we replace <code>lm</code> by <code>glm</code>.</p><pre><code class="language-julia hljs">model = glm(@formula(W ~ 1 + N + Y + I + K + F), wages, InverseGaussian(), SqrtLink())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">StatsModels.TableRegressionModel{GLM.GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Distributions.InverseGaussian{Float64}, GLM.SqrtLink}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}

W ~ 1 + N + Y + I + K + F

Coefficients:
────────────────────────────────────────────────────────────────────────────────
                   Coef.  Std. Error       z  Pr(&gt;|z|)    Lower 95%    Upper 95%
────────────────────────────────────────────────────────────────────────────────
(Intercept)   0.464875    0.00920656   50.49    &lt;1e-99   0.446831     0.48292
N            -0.186646    0.00294066  -63.47    &lt;1e-99  -0.19241     -0.180883
Y             0.591837    0.00980902   60.34    &lt;1e-99   0.572611     0.611062
I            -0.384294    0.00759787  -50.58    &lt;1e-99  -0.399185    -0.369402
K             0.00666287  0.00147633    4.51    &lt;1e-05   0.00376931   0.00955643
F            -2.28629e-5  2.672e-6     -8.56    &lt;1e-16  -2.80999e-5  -1.76259e-5
────────────────────────────────────────────────────────────────────────────────</code></pre><p>The following exercise plots the predictions for the generalized linear model.</p><div class="admonition is-warning"><header class="admonition-header">Exercise:</header><div class="admonition-body"><p>Create the scatter plot of predictions and labels. Do not use the <code>predict</code> function.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>Due to the construction of the generalized linear model, the prediction equals <span>$g^{-1}(w^\top x)$</span>. We save it into <span>$\hat y$</span>.</p><pre><code class="language-julia hljs">g_inv(z) = z^2

y_hat = g_inv.(X*coef(model))</code></pre><p>The scatter plot is now simple.</p><pre><code class="language-julia hljs">scatter(y, y_hat;
    label=&quot;&quot;,
    xlabel=&quot;Label&quot;,
    ylabel=&quot;Prediction&quot;,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_12/glm_predict.svg&quot;</code></pre></div></details><p><img src="../glm_predict.svg" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../monte/">« Monte Carlo sampling</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Sunday 27 October 2024 13:06">Sunday 27 October 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
