<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear regression with sparse constraints · Julia for Optimization and Learning</title><meta name="title" content="Linear regression with sparse constraints · Julia for Optimization and Learning"/><meta property="og:title" content="Linear regression with sparse constraints · Julia for Optimization and Learning"/><meta property="twitter:title" content="Linear regression with sparse constraints · Julia for Optimization and Learning"/><meta name="description" content="Documentation for Julia for Optimization and Learning."/><meta property="og:description" content="Documentation for Julia for Optimization and Learning."/><meta property="twitter:description" content="Documentation for Julia for Optimization and Learning."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Julia for Optimization and Learning</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Setup guide</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/installation/">Install</a></li><li><a class="tocitem" href="../../installation/tutorial/">Project setup</a></li><li><a class="tocitem" href="../../installation/running/">Running Julia</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Basics I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Basics II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/functions/">Function basics</a></li><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_03/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_03/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_03/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_03/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_03/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_04/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_04/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_04/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/structure/">Package structure</a></li><li><a class="tocitem" href="../../lecture_06/workflow/">Development workflow</a></li><li><a class="tocitem" href="../../lecture_06/compatibility/">Package dependencies</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Code organization II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/tests/">Tests</a></li><li><a class="tocitem" href="../../lecture_07/documentation/">Documentation</a></li><li><a class="tocitem" href="../../lecture_07/extensions/">Extensions</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_08/gradients/">Gradients</a></li><li><a class="tocitem" href="../../lecture_08/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_09/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_09/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_11/iris/">Introduction to Flux</a></li><li><a class="tocitem" href="../../lecture_11/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_11/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox" checked/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Linear regression with sparse constraints</a><ul class="internal"><li><a class="tocitem" href="#matrix-eigen"><span>Theory of matrix eigendecomposition</span></a></li><li><a class="tocitem" href="#Theory-of-ridge-regression"><span>Theory of ridge regression</span></a></li><li><a class="tocitem" href="#lasso"><span>Theory of LASSO</span></a></li><li><a class="tocitem" href="#Ridge-regression"><span>Ridge regression</span></a></li><li><a class="tocitem" href="#Lasso"><span>Lasso</span></a></li></ul></li><li><a class="tocitem" href="../monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../glm/">Linear regression revisited</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">12: Statistics</a></li><li class="is-active"><a href>Linear regression with sparse constraints</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear regression with sparse constraints</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_12/sparse.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-regression-with-sparse-constraints"><a class="docs-heading-anchor" href="#Linear-regression-with-sparse-constraints">Linear regression with sparse constraints</a><a id="Linear-regression-with-sparse-constraints-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression-with-sparse-constraints" title="Permalink"></a></h1><p>The standard regression problem reads</p><p class="math-container">\[\operatorname{minimize}_w\qquad \sum_{i=1}^n(w^\top x_i - y_i)^2.\]</p><p>Often, a regularization term is added. There are two possibilities. The <a href="https://en.wikipedia.org/wiki/Ridge_regression">ridge regression</a> adds the weighted squared <span>$l_2$</span>-norm penalization term to the objective:</p><p class="math-container">\[\operatorname{minimize}_w\qquad \sum_{i=1}^n(w^\top x_i - y_i)^2 + \frac{\mu}{2}\|w\|_2^2.\]</p><p><a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">LASSO</a> adds the weighted <span>$l_1$</span>-norm penalization term to the objective:</p><p class="math-container">\[\operatorname{minimize}_w\qquad \sum_{i=1}^n(w^\top x_i - y_i)^2 + \mu \|w\|_1.\]</p><p>Both approaches try to keep the norm of parameters <span>$w$</span> small to prevent overfitting. The first approach results in a simpler numerical method, while the second one induces sparsity. Before we start with both topics, we will briefly mention matrix decompositions which plays a crucial part in numerical computations.</p><h2 id="matrix-eigen"><a class="docs-heading-anchor" href="#matrix-eigen">Theory of matrix eigendecomposition</a><a id="matrix-eigen-1"></a><a class="docs-heading-anchor-permalink" href="#matrix-eigen" title="Permalink"></a></h2><p>Consider a square matrix <span>$A\in \mathbb R^{n\times n}$</span> with real-valued entries. If there exist <span>$\lambda\in\mathbb R$</span> and <span>$v\in\mathbb R^n$</span> such that</p><p class="math-container">\[Av = \lambda v,\]</p><p>we say that <span>$\lambda$</span> is a eigenvalue of <span>$A$</span> and <span>$v$</span> is the corresponding eigenvector.</p><p>For the rest of this section, we will assume that <span>$A$</span> is a symmetric matrix. Then these eigenvectors are perpendicular to each other. We create the diagonal matrix <span>$\Lambda$</span> with eigenvalues on diagonal and the matrix <span>$Q$</span> with columns consisting of the corresponding eigenvectors. Then we have</p><p class="math-container">\[A = Q\Lambda Q^\top\]</p><p>and for any real number <span>$\mu$</span>, we also have</p><p class="math-container">\[A + \mu I = Q(\Lambda + \mu I) Q^\top.\]</p><p>Since the eigenvectors are perpendicular, <span>$Q$</span> is an orthonormal matrix and therefore <span>$Q^{-1} = Q^\top$</span>. This implies that we can easily invert the matrix <span>$A + \mu I$</span> by</p><p class="math-container">\[(A + \mu I)^{-1} = Q (\Lambda + \mu I)^{-1} Q^\top.\]</p><p>Because <span>$\Lambda + \mu I$</span> is a diagonal matrix, its inverse is simple to compute.</p><h2 id="Theory-of-ridge-regression"><a class="docs-heading-anchor" href="#Theory-of-ridge-regression">Theory of ridge regression</a><a id="Theory-of-ridge-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Theory-of-ridge-regression" title="Permalink"></a></h2><p>The optimality condition for the ridge regression reads</p><p class="math-container">\[X^\top (Xw - y) + \mu w = 0.\]</p><p>Therefore, the optimal solution satisfies</p><p class="math-container">\[w = (X^\top X + \mu I)^{-1}X^\top y.\]</p><p>For <span>$\mu=0$</span>, we obtain the classical result for the linear regression. Since <span>$X^\top X$</span> is symmetric, we can compute its eigendecomposition</p><p class="math-container">\[X^\top X = Q\Lambda Q^\top.\]</p><p>Then the formula for optimal weights simplifies into </p><p class="math-container">\[w = Q(\Lambda+\mu I)^{-1} Q^\top X^\top y.\]</p><p>Since this formula uses only matrix-vector multiplication and an inversion of a diagonal matrix, we can employ it to fast compute the solution for multiple values of <span>$\mu$</span>.</p><h2 id="lasso"><a class="docs-heading-anchor" href="#lasso">Theory of LASSO</a><a id="lasso-1"></a><a class="docs-heading-anchor-permalink" href="#lasso" title="Permalink"></a></h2><p>Unlike ridge regression, LASSO does not have a closed-form solution. Since it is a structured convex problem, it can be solved the <a href="https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">ADMM algorithm</a>. It is a primal-dual algorithm, which employs the primal original variable <span>$w$</span>, the primal auxiliary variable <span>$z$</span> and the dual variable <span>$u$</span> with the iterative updates:</p><p class="math-container">\[\begin{aligned}
w^{k+1} &amp;= (X^\top X + \rho I)^{-1}(X^\top y + \rho z^k - \rho u^k), \\
z^{k+1} &amp;= S_{\mu / \rho}(w^{k+1} + u^k) \\
u^{k+1} &amp;= u^k + w^{k+1} - z^k.
\end{aligned}\]</p><p>Here, <span>$\rho &gt; 0$</span> is an arbitrary number and</p><p class="math-container">\[S_\eta(z) = \max\{z - \eta, 0\} - \max\{-z -\eta, 0\}\]</p><p>is the so-called soft thresholding operator. Since these updates must be performed many times, it may be a good idea to perform the same factorization of the matrix <span>$X^\top X + \rho I$</span> as in the case of ridge regression.</p><h2 id="Ridge-regression"><a class="docs-heading-anchor" href="#Ridge-regression">Ridge regression</a><a id="Ridge-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Ridge-regression" title="Permalink"></a></h2><p>We will randomly generate <span>$10000$</span> samples in <span>$\mathbb R^{1000}$</span>.</p><pre><code class="language-julia hljs">using LinearAlgebra
using Random
using Plots

n = 10000
m = 1000

Random.seed!(666)
X = randn(n, m)</code></pre><p>The real dependence depends only on the first two features and reads</p><p class="math-container">\[y = 10x_1 + x_2.\]</p><p>This is a natural problem for sparse models because most of the weights should be zero. We generate the labels but add noise to them.</p><pre><code class="language-julia hljs">y = 10*X[:,1] + X[:,2] + randn(n)</code></pre><p>The first exercise compares both approaches to solving the ridge regression.</p><div class="admonition is-warning" id="Exercise:-b7b16915fd2779f7"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-b7b16915fd2779f7" title="Permalink"></a></header><div class="admonition-body"><p>Implement the methods for the <code>ridge_reg</code> function. Verify that the result in the same result.</p><p><strong>Hints:</strong></p><ul><li>The eigendecomposition can be found by <code>eigen(A)</code> or <code>eigen(A).values</code>.</li><li>The identity matrix is implemented by <code>I</code> in the <code>LinearAlgebra</code> package.</li></ul></div></div><details class="admonition is-details" id="Solution:-94cde41219237257"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-94cde41219237257" title="Permalink"></a></summary><div class="admonition-body"><p>The simple implementation for the solution is the same as in the case of linear regression. We only need to add <code>μ*I</code>.</p><pre><code class="language-julia hljs">ridge_reg(X, y, μ) = (X&#39;*X + μ*I) \ (X&#39;*y)</code></pre><p>We first compute the eigendecomposition and save it into <code>eigen_dec</code>. Then we extract the eigenvector and eigenvalues. We also transpose the matrix <span>$Q$</span> and save it into <code>Q_inv</code> so that we do not have to compute it repeatedly.</p><pre><code class="language-julia hljs">eigen_dec = eigen(X&#39;*X)
Q = eigen_dec.vectors
Q_inv = Matrix(Q&#39;)
λ = eigen_dec.values</code></pre><p>The more sophisticated way of solving the ridge regression contains only matrix-vector multiplication and the inversion of the diagonal matrix <span>$(\Lambda + \mu I)^{-1}$</span>. We need to properly add paranthesis, to start multiplication from the right and evade matrix-matrix multiplication, which would occur if we started from the left. Since the matrix <span>$\Lambda + \mu I$</span> is diagonal, its inverse is the diagonal matrix formed from the inverted diagonal.</p><pre><code class="language-julia hljs">ridge_reg(X, y, μ, Q, Q_inv, λ) = Q * ((Diagonal(1 ./ (λ .+ μ)) * ( Q_inv * (X&#39;*y))))</code></pre><p>When we compare both solution, we see that they are the same.</p><pre><code class="language-julia hljs">w1 = ridge_reg(X, y, 10)
w2 = ridge_reg(X, y, 10, Q, Q_inv, λ)

norm(w1 - w2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6.816080981087694e-13</code></pre></div></details><p>To test the speed, we use the <code>BenchmarkTools</code> package. The second option is significantly faster. The price to pay is the need to pre-compute the matrix decomposition.</p><pre><code class="language-julia hljs">julia&gt; using BenchmarkTools

julia&gt; @btime ridge_reg(X, y, 10);
  114.182 ms (9 allocations: 22.91 MiB)

julia&gt; @btime ridge_reg(X, y, 10, Q, Q_inv, λ);
  6.194 ms (5 allocations: 39.69 KiB)</code></pre><p>Now we create multiple values of <span>$\mu$</span> and compute the ridge regression for all of them. Since the broadcasting would broadcast all matrices, we need to fix all but one by the <code>Ref</code> command.</p><pre><code class="language-julia hljs">μs = range(0, 1000; length=50)
ws = hcat(ridge_reg.(Ref(X), Ref(y), μs, Ref(Q), Ref(Q_inv), Ref(λ))...)

plot(μs, abs.(ws&#39;);
    label=&quot;&quot;,
    yscale=:log10,
    xlabel=&quot;mu&quot;,
    ylabel=&quot;weights: log scale&quot;,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_12/Sparse1.svg&quot;</code></pre><p><img src="../Sparse1.svg" alt/></p><p>The regularization seems to have a small effect on the solution.</p><h2 id="Lasso"><a class="docs-heading-anchor" href="#Lasso">Lasso</a><a id="Lasso-1"></a><a class="docs-heading-anchor-permalink" href="#Lasso" title="Permalink"></a></h2><p>For LASSO, we define the soft thresholding operator.</p><pre><code class="language-julia hljs">S(x, η) = max(x-η, 0) - max(-x-η, 0)</code></pre><p>Then we define the iterative updates from ADMM. It is important that we allow to insert the initial values for <span>$w^0$</span>, <span>$z^0$</span> and <span>$u^0$</span>. If they are not provided, they are initialized by zeros with the correct dimension. We should implement a proper termination condition but, for simplicity, we run ADMM for a fixed number of iterations.</p><pre><code class="language-julia hljs">function lasso(X, y, μ, Q, Q_inv, λ;
        max_iter = 100,
        ρ = 1e3,
        w = zeros(size(X,2)),
        u = zeros(size(X,2)),
        z = zeros(size(X,2)),
    )

    for i in 1:max_iter
        w = Q * ( (Diagonal(1 ./ (λ .+ ρ)) * ( Q_inv * (X&#39;*y + ρ*(z-u)))))
        z = S.(w + u, μ / ρ)
        u = u + w - z
    end
    return w, u, z
end</code></pre><p>Finally, we compute the values for all regularization parameters <span>$\mu$</span>. The second line in the loop says that if <span>$i=1$</span>, then run LASSO without the initial values, and if <span>$i&gt;1$</span>, then run it with the initial values from the previous iteration. Since the visibility of <code>w</code>, <code>u</code> and <code>z</code> is only one iteration, we need to specify that they are global variables.</p><pre><code class="language-julia hljs">ws = zeros(size(X,2), length(μs))

for (i, μ) in enumerate(μs)
    global w, u, z
    w, u, z = i &gt; 1 ? lasso(X, y, μ, Q, Q_inv, λ; w, u, z) : lasso(X, y, μ, Q, Q_inv, λ)
    ws[:,i] = w
end</code></pre><p>When we plot the parameter values, we see that they are significantly smaller than for the ridge regression. This is precisely what we meant when we mentioned that <span>$l_1$</span>-norm regularization induces sparsity. </p><pre><code class="language-julia hljs">plot(μs, abs.(ws&#39;);
    label=&quot;&quot;,
    yscale=:log10,
    xlabel=&quot;mu&quot;,
    ylabel=&quot;weights: log scale&quot;,
)</code></pre><img src="83874143.svg" alt="Example block output"/><div class="admonition is-info" id="Warm-start:-a6d9c4b707d3e66b"><header class="admonition-header">Warm start:<a class="admonition-anchor" href="#Warm-start:-a6d9c4b707d3e66b" title="Permalink"></a></header><div class="admonition-body"><p>The technique of starting from a previously computed value is called warm start or hot start. It is commonly used when some parameter changes only slightly. Then the solution changes only slightly and the previous solution provides is close to the new solution. Therefore, we initialize the algorithm from the old solution.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_11/exercises/">« Exercises</a><a class="docs-footer-nextpage" href="../monte/">Monte Carlo sampling »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 30 September 2025 09:12">Tuesday 30 September 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
