<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Monte Carlo sampling · Julia for Optimization and Learning</title><meta name="title" content="Monte Carlo sampling · Julia for Optimization and Learning"/><meta property="og:title" content="Monte Carlo sampling · Julia for Optimization and Learning"/><meta property="twitter:title" content="Monte Carlo sampling · Julia for Optimization and Learning"/><meta name="description" content="Documentation for Julia for Optimization and Learning."/><meta property="og:description" content="Documentation for Julia for Optimization and Learning."/><meta property="twitter:description" content="Documentation for Julia for Optimization and Learning."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Julia for Optimization and Learning</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/installation/">Installation</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Basics I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Basics II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/functions/">Function basics</a></li><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_03/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_03/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_03/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_03/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_03/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_04/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_04/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_04/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/structure/">Package structure</a></li><li><a class="tocitem" href="../../lecture_06/workflow/">Development workflow</a></li><li><a class="tocitem" href="../../lecture_06/compatibility/">Package dependencies</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Code organization II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/tests/">Tests</a></li><li><a class="tocitem" href="../../lecture_07/documentation/">Documentation</a></li><li><a class="tocitem" href="../../lecture_07/extensions/">Extensions</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_08/gradients/">Gradients</a></li><li><a class="tocitem" href="../../lecture_08/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_09/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_09/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_11/iris/">Introduction to Flux</a></li><li><a class="tocitem" href="../../lecture_11/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_11/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox" checked/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../sparse/">Linear regression with sparse constraints</a></li><li class="is-active"><a class="tocitem" href>Monte Carlo sampling</a><ul class="internal"><li><a class="tocitem" href="#Gamma-function"><span>Gamma function</span></a></li><li><a class="tocitem" href="#Volume-of-m-dimensional-ball"><span>Volume of m-dimensional ball</span></a></li><li><a class="tocitem" href="#Sampling-from-distributions"><span>Sampling from distributions</span></a></li><li class="toplevel"><a class="tocitem" href="#How-many-samples-do-we-need?"><span>How many samples do we need?</span></a></li></ul></li><li><a class="tocitem" href="../glm/">Linear regression revisited</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">12: Statistics</a></li><li class="is-active"><a href>Monte Carlo sampling</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Monte Carlo sampling</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_12/monte.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Monte-Carlo-sampling"><a class="docs-heading-anchor" href="#Monte-Carlo-sampling">Monte Carlo sampling</a><a id="Monte-Carlo-sampling-1"></a><a class="docs-heading-anchor-permalink" href="#Monte-Carlo-sampling" title="Permalink"></a></h1><p>The three previous lectures presented data analysis from an optimization viewpoint. We considered the dataset as fixed and then derived an optimization problem of minimizing the discrepancy between predictions and labels. This lecture returns to linear models. It presents a statistical viewpoint, which considers the data and the labels as random realizations (samples) of random variables. The family of methods using random sampling from the same random variable to obtain numerical results is called the <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo</a> methods.</p><p>We will also present several topics on the curse of dimensionality, where behaviour in a large dimension is entirely different from the one in a small dimension. Since we humans cannot properly comprehend more than three dimensions, some of the results may be counter-intuitive.</p><h2 id="Gamma-function"><a class="docs-heading-anchor" href="#Gamma-function">Gamma function</a><a id="Gamma-function-1"></a><a class="docs-heading-anchor-permalink" href="#Gamma-function" title="Permalink"></a></h2><p>One of the most commonly used functions in statistical analysis is the <a href="https://en.wikipedia.org/wiki/Gamma_function"><span>$\Gamma$</span>-function</a> defined by</p><p class="math-container">\[\Gamma(z) = \int_0^\infty x^{z-1}e^{-z}dx.\]</p><p>It can be evaluated only approximately except for positive integers <span>$k$</span>, for which it holds</p><p class="math-container">\[\Gamma(k) = (k-1)!.\]</p><p>It is implemented in the <a href="https://github.com/JuliaMath/SpecialFunctions.jl">SpecialFunctions</a> package.</p><pre><code class="language-julia hljs">using SpecialFunctions
using Plots

plot(0:0.1:10, gamma;
    xlabel=&quot;x&quot;,
    ylabel=&quot;gamma(x): log scale&quot;,
    label=&quot;&quot;,
    yscale=:log10,
)</code></pre><img src="09639908.svg" alt="Example block output"/><h2 id="Volume-of-m-dimensional-ball"><a class="docs-heading-anchor" href="#Volume-of-m-dimensional-ball">Volume of m-dimensional ball</a><a id="Volume-of-m-dimensional-ball-1"></a><a class="docs-heading-anchor-permalink" href="#Volume-of-m-dimensional-ball" title="Permalink"></a></h2><p>Machine learning datasets contain many features. Even simple datasets such as MNIST live in <span>$28\times 28=784$</span> dimensions. However, we humans are unable to think in more than three dimensions. Working with more-dimensional spaces can bring many surprises. This section computes the volume of <span>$m$</span>-dimensional balls. Before we start, try to guess the volume of the unit balls in <span>$\mathbb R^{10}$</span> and <span>$\mathbb R^{100}$</span>. </p><div class="admonition is-warning"><header class="admonition-header">Exercise:</header><div class="admonition-body"><p>Use the <a href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball">formula</a> to compute the volume of a <span>$m$</span>-dimensional ball. Plot the dependence of the volume on the dimension <span>$m=1,\dots,100$</span>.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>The formula can be easily transferred to a function.</p><pre><code class="language-julia hljs">volume_true(m, R) = π^(m/2) *R^2 / gamma(m/2 + 1)</code></pre><p>Then we create the plot. We use the log-scaling of the <span>$y$</span>-axis.</p><pre><code class="language-julia hljs">plot(1:100, m -&gt; volume_true.(m, 1);
    xlabel=&quot;dimension&quot;,
    ylabel=&quot;unit ball volume: log scale&quot;,
    label=&quot;&quot;,
    yscale=:log10,
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_12/dimension1.svg&quot;</code></pre></div></details><p><img src="../dimension1.svg" alt/></p><p>This result may be surprising. While the volume of the <span>$10$</span>-dimensional ball is approximately <span>$2.55$</span>, the volume of the <span>$100$</span>-dimensional ball is almost <span>$0$</span>.</p><p>The following exercise uses the Monte Carlo sampling to estimate this volume. We will sample points in the hypercube <span>$[-1,1]^m$</span> and then compute the unit ball volume by realizing that the volume of the ball divided by the volume of the box equals the fraction of sampled points inside the ball.</p><div class="admonition is-warning"><header class="admonition-header">Exercise:</header><div class="admonition-body"><p>Write the function <code>volume_monte_carlo</code>, which estimates the volume of the <span>$m$</span>-dimensional ball based on <span>$n$</span> randomly sampled points.</p><p><strong>Hint</strong>: function <code>rand(m,n)</code> creates a <span>$m\times n$</span> matrix, which can be understood as <span>$n$</span> randomly sampled points in <span>$[0,1]^m$</span>. Transform them to <span>$[-1,1]^m$</span>.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>To transform the random variable from <span>$[0,1]$</span> to <span>$[-1,1]$</span>, we need to multiply it by two and subtract one. Then we compute the norm of each sampled point. The estimated volume is computed as the fraction of the points whose norm is smaller than one multiplied by the hypercube volume. The latter equals to <span>$2^m$</span>.</p><pre><code class="language-julia hljs">using Random
using Statistics

function volume_monte_carlo(m::Int; n::Int=10000)
    X = 2*rand(m, n).-1
    X_norm_sq = sum(X.^2; dims=1)
    return 2^m*mean(X_norm_sq .&lt;= 1)
end</code></pre></div></details><p>The next figure shows the estimated volume from <span>$n\in \{10, 1000, 100000\}$</span> samples for the unit ball in dimension <span>$m=1,\dots,15$</span>.</p><pre><code class="language-julia hljs">ms = 1:15
ns = Int64.([1e1; 1e3; 1e5])

Random.seed!(666)

plt = plot(ms, m -&gt; volume_true(m, 1);
    xlabel=&quot;dimension&quot;,
    ylabel=&quot;unit ball volume&quot;,
    legend=:topleft,
    label=&quot;True&quot;,
    line=(4,:black),
)

for n in ns
    plot!(plt, ms, m -&gt; volume_monte_carlo.(m; n=n); label=&quot;n = $n&quot;)
end

display(plt)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_12/dimension2.svg&quot;</code></pre><p><img src="../dimension2.svg" alt/></p><p>It is not surprising that with increasing dimension, we need a much larger number of samples to obtain good estimates. This number grows exponentially with the dimension. This phenomenon explains why machine learning models with large feature spaces need lots of data. Moreover, the number of samples should increase with the complexity of the input and the network. </p><div class="admonition is-info"><header class="admonition-header">Generating from the uniform distribution:</header><div class="admonition-body"><p>While we wrote our function for generating from the uniform distribution, we can also use the Distributions package.</p><pre><code class="language-julia hljs">using Distributions

rand(Uniform(-1, 1), 10, 5)</code></pre><p>We will discuss this topic more in the following section.</p></div></div><h2 id="Sampling-from-distributions"><a class="docs-heading-anchor" href="#Sampling-from-distributions">Sampling from distributions</a><a id="Sampling-from-distributions-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-from-distributions" title="Permalink"></a></h2><p>This section shows how to generate from various distributions. We use the <a href="https://juliastats.org/Distributions.jl/stable/">Distributions</a> package to create the normal distribution with three sets of parameters.</p><pre><code class="language-julia hljs">using Distributions

d1 = Normal()
d2 = Normal(1, 1)
d3 = Normal(0, 0.01)

f1(x) = pdf(d1, x)
f2(x) = pdf(d2, x)
f3(x) = pdf(d3, x)</code></pre><p>We create the <code>plot_histogram</code> function, which plots the histogram of <code>xs</code> and its density <code>f</code>. We use the <code>normalize</code> keyword to obtain probabilities in the histogram.</p><pre><code class="language-julia hljs">function plot_histogram(xs, f; kwargs...)
    plt = histogram(xs;
        label=&quot;Sampled density&quot;,
        xlabel = &quot;x&quot;,
        ylabel = &quot;pdf(x)&quot;,
        nbins = 85,
        normalize = :pdf,
        opacity = 0.5,
        kwargs...
    )

    plot!(plt, range(minimum(xs), maximum(xs); length=100), f;
        label=&quot;True density&quot;,
        line=(4,:black),
    )

    return plt
end</code></pre><p>The Distributions package allows to easily generate random samples from most distributions by <code>rand(d, n)</code>. Do not confuse the call <code>rand(d, n)</code> with <code>rand(m, n)</code>. The former employs the Distribution package and generates <span>$n$</span> samples from distribution <span>$d$</span>, while the latter employs Base and generates <span>$m\times n$</span> samples from <span>$[0,1]$</span>. </p><pre><code class="language-julia hljs">plot_histogram(rand(d1, 1000000), f1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_12/density0.svg&quot;</code></pre><p><img src="../density0.svg" alt/></p><p>When the sampled point number is high enough, the sampled histogram is a good approximation of its density function.</p><p>We may work with a distribution <span>$d$</span> for which we know the density <span>$f$</span>, but there is no sampling function. Then we can use the <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a> technique, which assumes the knowledge of:</p><ul><li>the interval <span>$[x_{\rm min}, x_{\rm max}]$</span> containing the support of <span>$f$</span>.</li><li>the upper bound  <span>$f_{\rm max}$</span> for the density <span>$f$</span>.</li></ul><p>The rejection sampling technique first randomly samples a trial point <span>$x\in [x_{\rm min}, x_{\rm max}]$</span> and a scalar <span>$p\in [0,f_{\rm max}]$</span>. It accepts <span>$x$</span> if <span>$p \le f(x)$</span> and rejects it otherwise. This technique ensures that a point is accepted with a probability proportional to its density function value.</p><div class="admonition is-warning"><header class="admonition-header">Exercise:</header><div class="admonition-body"><p>Implement the <code>rejection_sampling</code> function. It should generate <span>$n$</span> trial points and return all accepted points.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>While it is possible to generate the random points one by one, we prefer to generate them all at once and discard the rejected samples. The function follows precisely the steps summarized before this exercise.</p><pre><code class="language-julia hljs">function rejection_sampling(f, f_max, x_min, x_max; n=1000000)
    xs = x_min .+ (x_max - x_min)*rand(n)
    ps = f_max*rand(n)
    return xs[f.(xs) .&gt;= ps]
end</code></pre></div></details><p>We will now use the rejection sampling technique to generate the random samples from the three distributions from above. Since the density <span>$f$</span> of the normal distribution achieves its maximum at the mean, we specify <code>f_max = f(d.μ)</code>.</p><pre><code class="language-julia hljs">xlims = (-10, 10)

for (f, d) in zip((f1, f2, f3), (d1, d2, d3))
    Random.seed!(666)
    xs = rejection_sampling(f, f(d.μ), xlims...)

    pl = plot_histogram(xs, f)
    display(pl)
end</code></pre><p><img src="../densityf1.svg" alt/></p><p><img src="../densityf2.svg" alt/></p><p><img src="../densityf3.svg" alt/></p><p>While the rejection sampling provides a good approximation for the first two distributions, it performs subpar for the last distribution. The reason is that the rejection sampling is sensitive to the choice of the interval <span>$[x_{\rm min}, x_{\rm max}]$</span>. Because we chose the interval <span>$[-10,10]$</span> and <span>$f_3$</span> has negligible values outside of the interval <span>$[-0.1,0.1]$</span>,  most trial points got rejected. It is not difficult to verify that from the <span>$1000000$</span> trial points, only approximately <span>$1200$</span> got accepted. The small number of accepted points makes for the poor approximation. If we generated from a narrower interval, the results would be much better.</p><div class="admonition is-compat"><header class="admonition-header">BONUS: Using rejection sampling to compute expected value</header><div class="admonition-body"><p>This exercise computes the expected value</p><p class="math-container">\[\mathbb E_3 \cos(100X) = \int_{-\infty}^\infty \cos(100 x) f_3(x) dx,\]</p><p>where we consider the expectation <span>$\mathbb E$</span> with respect to <span>$d_3\sim  N(0, 0.01)$</span> with density <span>$f_3$</span>. The first possibility to compute the expectation is to discretize the integral.</p><pre><code class="language-julia hljs">h(x) = cos(100*x)

Δx = 0.001
xs = range(xlims...; step=Δx)
e0 = Δx * sum(f3.(xs) .* h.(xs))</code></pre><p>The second possibility is to approximate the integral by</p><p class="math-container">\[\mathbb E_3 \cos(100X) \approx \frac 1n\sum_{i=1}^n \cos(x_i),\]</p><p>where <span>$x_i$</span> are sampled from <span>$d_3$</span>. We do this in <code>expectation1</code>, and <code>expectation2</code>, where the formed generates from the Distributions package while the latter uses our rejection sampling. We use the method of the <code>mean</code> function, which takes a function as its first argument.</p><pre><code class="language-julia hljs">expectation1(h, d; n = 1000000) = mean(h, rand(d, n))

function expectation2(h, f, f_max, xlims; n=1000000)
    return mean(h, rejection_sampling(f, f_max, xlims...; n))
end</code></pre><p>If it is difficult to sample from <span>$d_3$</span>, we can use a trick to sample from some other distribution. This is based on the following formula:</p><p class="math-container">\[\mathbb E_3 h(x) = \int_{-\infty}^\infty h(x) f_3(x) dx = \int_{-\infty}^\infty h(x) \frac{f_3(x)}{f_1(x)}f_1(x) dx = \mathbb E_1 \frac{h(x)f_3(x)}{f_1(x)}.\]</p><p>This gives rise to another implementation of the same thing.</p><pre><code class="language-julia hljs">function expectation3(h, f, d_gen; n=1000000)
    g(x) = h(x)*f(x)/pdf(d_gen, x)
    return mean(g, rand(d_gen, n))
end</code></pre><p>We run these three approaches for <span>$20$</span> repetitions.</p><pre><code class="language-julia hljs">n = 100000
n_rep = 20

Random.seed!(666)
e1 = [expectation1(h, d3; n=n) for _ in 1:n_rep]
e2 = [expectation2(h, f3, f3(d3.μ), xlims; n=n) for _ in 1:n_rep]
e3 = [expectation3(h, f3, d1; n=n) for _ in 1:n_rep]</code></pre><p>Finally, we plot the results. Sampling from the package gives the best results because it generates the full amount of points, while the rejection sampling rejects many points. </p><pre><code class="language-julia hljs">scatter([1], [e0]; label=&quot;Integral discretization&quot;, legend=:topleft)
scatter!(2*ones(n_rep), e1; label=&quot;Generating from Distributions.jl&quot;)
scatter!(3*ones(n_rep), e2; label=&quot;Generating from rejection sampling&quot;)
scatter!(4*ones(n_rep), e3; label=&quot;Generating from other distribution&quot;)</code></pre><img src="8ebcbbb3.svg" alt="Example block output"/><p>This exercise considered the computation of a one-dimensional integral. It is important to realize that even for such a simple case, it is necessary to sample a sufficiently large number of points. Even when we sampled <span>$100000$</span> points, there is still some variance in the results, as the last three columns show.</p></div></div><h1 id="How-many-samples-do-we-need?"><a class="docs-heading-anchor" href="#How-many-samples-do-we-need?">How many samples do we need?</a><a id="How-many-samples-do-we-need?-1"></a><a class="docs-heading-anchor-permalink" href="#How-many-samples-do-we-need?" title="Permalink"></a></h1><p>Previous sections showed that we need many samples to obtain a good approximation of a desired quantity. The natural question is, how exactly many samples do we need? Even though many results estimate such errors, unfortunately, the answer depends on the application. This section will present two examples. The first one shows the distance of sampled points in a more-dimensional space, while the second one computes quantiles.</p><div class="admonition is-warning"><header class="admonition-header">Exercise:</header><div class="admonition-body"><p>Sample <span>$n=1000$</span> points in the unit cube in the <span>$m=9$</span>-dimensional space. What is the minimum distance of these points? Before implementing the exercise, try to guess the answer.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Solution:</summary><div class="admonition-body"><p>We first sample the points.</p><pre><code class="language-julia hljs">n = 1000
m = 9

Random.seed!(666)
xs = rand(m, n)</code></pre><p>Then we save the pairwise of points in <code>dist1</code>. Since this variable contains zeros on the diagonal, and since lower and upper diagonal are the same, we select only the upper part of the matrix and save it into <code>dist2</code>.</p><pre><code class="language-julia hljs">using LinearAlgebra

dist1 = [norm(x-y) for x in eachcol(xs), y in eachcol(xs)]
dist2 = [dist1[i,j] for i in 1:n for j in i+1:n]</code></pre><p>This approach has the disadvantage that it allocates an <span>$n\times n$</span> matrix.</p></div></details><p>The minimum of these distances is roughly <span>$0.2$</span>, while the maximum is <span>$2.2$</span>. The minimum is surprisingly high and shows that sampling even <span>$1000$</span> points in <span>$\mathbb R^9$</span> forms a very sparse structure. The maximum distance is far away from the distance of two corners of the hypercube, which equals <span>$\sqrt{m}=3$</span>.</p><pre><code class="language-julia hljs">extrema(dist2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(0.20051862230649456, 2.215876305466382)</code></pre><div class="admonition is-compat"><header class="admonition-header">BONUS: Approximating the quantiles</header><div class="admonition-body"><p>Quantiles form an important concept in statistics. Its definition is slightly complicated; we will consider only absolutely continuous random variables: one-dimensional variables <span>$X$</span> with continuous density <span>$f$</span>. Then the quantile at a level <span>$\alpha\in[0,1]$</span> is the unique point <span>$x$</span> such that </p><p class="math-container">\[\mathbb P(X\le x) = \int_{-\infty}^x f(x)dx = \alpha. \]</p><p>The quantile at level <span>$\alpha=0.5$</span> is the mean. Quantiles play an important role in estimates, where they form upper and lower bounds for confidence intervals. They are also used in hypothesis testing.</p><p>This part will investigate how quantiles on a finite sample differ from the true quantile. We will consider two ways of computing the quantile. Both of them sample <span>$n$</span> points from some distribution <span>$d$</span>. The first one follows the statistical definition and selects the index of the <span>$n\alpha$</span> smallest observation by the <code>partialsort</code> function. The second one uses the function <code>quantile</code>, which performs some interpolation.</p><pre><code class="language-julia hljs">quantile_sampled1(d, n::Int, α) = partialsort(rand(d, n), floor(Int, α*n))
quantile_sampled2(d, n::Int, α) = quantile(rand(d, n), α)</code></pre><p>We defined the vectorized version. This is not efficient because for every <span>$n$</span>, the samples will be randomly generated again.</p><pre><code class="language-julia hljs">quantile_sampled1(d, ns::AbstractVector, α) = quantile_sampled1.(d, ns, α)
quantile_sampled2(d, ns::AbstractVector, α) = quantile_sampled2.(d, ns, α)</code></pre><p>We generate the quantile for <span>$\alpha = 0.99$</span> and repeat it 20 times.</p><pre><code class="language-julia hljs">α = 0.99
n_rep = 20
ns = round.(Int, 10 .^ (1:0.05:5))

Random.seed!(666)
qs1 = hcat([quantile_sampled1(d1, ns, α) for _ in 1:n_rep]...)
Random.seed!(666)
qs2 = hcat([quantile_sampled2(d1, ns, α) for _ in 1:n_rep]...)</code></pre><p>We initialize the plot with the line for the true quantile. Since this will be part of both plots, we create just one and use <code>deepcopy</code> to create the other one. </p><pre><code class="language-julia hljs">plt1 = plot([0.9*minimum(ns); 1.1*maximum(ns)], quantile(d1, α)*ones(2);
    xlabel=&quot;n: log scale&quot;,
    ylabel=&quot;sampled quantile&quot;,
    xscale=:log10,
    label=&quot;True quantile&quot;,
    line=(4,:black),
    ylims=(0,3.5),
)
plt2 = deepcopy(plt1)</code></pre><p>Now we add the sampled quantiles and the mean over all repetitions. Since we work with two plots, we specify into which plot we want to add the new data. It would be better to create a function for plotting and call it for <code>qs1</code> and <code>qs2</code>, but we wanted to show how to work two plots simultaneously.</p><pre><code class="language-julia hljs">for i in 1:size(qs1,1)
    scatter!(plt1, ns[i]*ones(size(qs1,2)), qs1[i,:];
        label=&quot;&quot;,
        color=:blue,
        markersize = 2,
    )
    scatter!(plt2, ns[i]*ones(size(qs2,2)), qs2[i,:];
        label=&quot;&quot;,
        color=:blue,
        markersize = 2,
    )
end

plot!(plt1, ns, mean(qs1; dims=2);
    label=&quot;Sampled mean&quot;,
    line=(4,:red),
)
plot!(plt2, ns, mean(qs2; dims=2);
    label=&quot;Sampled mean&quot;,
    line=(4,:red),
)

display(plt1)
display(plt2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_12/quantile2.svg&quot;</code></pre><p><img src="../quantile1.svg" alt/> <img src="../quantile2.svg" alt/></p><p>Both sampled estimates give a lower estimate than the true quantile. In statistical methodology, these estimates are biased. We observe that the interpolated estimate is closer to the true value and that computing the quantile even on <span>$10000$</span> points gives an uncertainty interval of approximately <span>$0.25$</span>.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../sparse/">« Linear regression with sparse constraints</a><a class="docs-footer-nextpage" href="../glm/">Linear regression revisited »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Sunday 27 October 2024 13:07">Sunday 27 October 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
