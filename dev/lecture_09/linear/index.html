<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear regression · Julia for Optimization and Learning</title><meta name="title" content="Linear regression · Julia for Optimization and Learning"/><meta property="og:title" content="Linear regression · Julia for Optimization and Learning"/><meta property="twitter:title" content="Linear regression · Julia for Optimization and Learning"/><meta name="description" content="Documentation for Julia for Optimization and Learning."/><meta property="og:description" content="Documentation for Julia for Optimization and Learning."/><meta property="twitter:description" content="Documentation for Julia for Optimization and Learning."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Optimization and Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Optimization and Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Julia for Optimization and Learning</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Setup guide</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/installation/">Install</a></li><li><a class="tocitem" href="../../installation/tutorial/">Project setup</a></li><li><a class="tocitem" href="../../installation/running/">Running Julia</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Basics I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Basics II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/functions/">Function basics</a></li><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_03/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_03/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_03/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_03/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_03/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_04/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_04/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_04/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization I</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/structure/">Package structure</a></li><li><a class="tocitem" href="../../lecture_06/workflow/">Development workflow</a></li><li><a class="tocitem" href="../../lecture_06/compatibility/">Package dependencies</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Code organization II</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/tests/">Tests</a></li><li><a class="tocitem" href="../../lecture_07/documentation/">Documentation</a></li><li><a class="tocitem" href="../../lecture_07/extensions/">Extensions</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_08/gradients/">Gradients</a></li><li><a class="tocitem" href="../../lecture_08/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox" checked/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Introduction to regression and classification</a></li><li class="is-active"><a class="tocitem" href>Linear regression</a><ul class="internal"><li><a class="tocitem" href="#Theory-of-linear-regression"><span>Theory of linear regression</span></a></li><li><a class="tocitem" href="#UCI-repository"><span>UCI repository</span></a></li><li><a class="tocitem" href="#Loading-and-preparing-data"><span>Loading and preparing data</span></a></li><li><a class="tocitem" href="#Training-the-classifier"><span>Training the classifier</span></a></li></ul></li><li><a class="tocitem" href="../logistic/">Logistic regression</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">11: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_11/iris/">Introduction to Flux</a></li><li><a class="tocitem" href="../../lecture_11/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_11/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">12: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/sparse/">Linear regression with sparse constraints</a></li><li><a class="tocitem" href="../../lecture_12/monte/">Monte Carlo sampling</a></li><li><a class="tocitem" href="../../lecture_12/glm/">Linear regression revisited</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">9: Regression and classification</a></li><li class="is-active"><a href>Linear regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear regression</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/blob/master/docs/src/lecture_09/linear.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-regression"><a class="docs-heading-anchor" href="#Linear-regression">Linear regression</a><a id="Linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression" title="Permalink"></a></h1><p>We start with linear regression, where the labels are continuous variables.</p><h2 id="Theory-of-linear-regression"><a class="docs-heading-anchor" href="#Theory-of-linear-regression">Theory of linear regression</a><a id="Theory-of-linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Theory-of-linear-regression" title="Permalink"></a></h2><p>Linear regression uses the linear prediction function <span>$\operatorname{predict}(w;x) = w^\top x$</span> and the loss function <span>$\operatorname{loss}(y, \hat y) = (y - \hat y)^2$</span>. When we have a dataset with <span>$n$</span> data points (samples) <span>$x_i$</span> and labels <span>$y_i$</span>, linear regression may be written as the following optimization problem:</p><p class="math-container">\[\operatorname{minimize}_w\qquad \frac 1n\sum_{i=1}^n (w^\top x_i - y_i)^2.\]</p><p>The objective function is minimal if the predictions <span>$w^\top x_i$</span> are equal to the labels <span>$y_i$</span> for all samples <span>$i=1,\dots,n$</span>.</p><p>Some algorithms use the sum instead of the mean in the objective function. These approaches are equivalent. For the former case, it is simpler to work in the matrix notation, where we form a matrix <span>$X$</span> whose rows are the samples <span>$x_i$</span>. It is not difficult to show that the previous problem is equivalent to</p><p class="math-container">\[\operatorname{minimize}_w\qquad \|Xw - y\|^2,\]</p><p>where the norm is the <span>$l_2$</span> norm. Since this is a convex quadratic problem, it is equivalent to its optimality conditions. Setting the derivative to zero yields</p><p class="math-container">\[2X^\top (Xw-y) = 0.\]</p><p>From here, we obtain the closed-form solution to the linear regression</p><p class="math-container">\[w = (X^\top X)^{-1}X^\top y.\]</p><div class="admonition is-info" id="Closed-form-solution:-adf2512395aafe2a"><header class="admonition-header">Closed-form solution:<a class="admonition-anchor" href="#Closed-form-solution:-adf2512395aafe2a" title="Permalink"></a></header><div class="admonition-body"><p>Linear regression is probably the only machine learning model with a closed-form solution. All other models must be solved by iterative algorithms such as gradient descent. In some cases, it may be advantageous to use iterative algorithms even for linear regression. For example, this includes the case of a large number of features <span>$m$</span> because then <span>$X^\top X$</span> is an <span>$m\times m$</span> matrix that may be difficult to invert.</p></div></div><h2 id="UCI-repository"><a class="docs-heading-anchor" href="#UCI-repository">UCI repository</a><a id="UCI-repository-1"></a><a class="docs-heading-anchor-permalink" href="#UCI-repository" title="Permalink"></a></h2><p>Training a machine learning model requires data. Neural networks require lots of data. Since collecting data is difficult, there are many datasets at the <a href="http://archive.ics.uci.edu/ml/index.php">UCI Machine Learning Repository</a>. We will use the iris (kosatec in Czech) dataset which predicts one of the three types of iris based on sepal (kališní lístek in Czech) and petal (okvětní lístek in Czech) widths and lengths.</p><p><img src="../iris.png" alt/></p><p>If you do not see any differences between these three species, machine learning to the rescue!</p><h2 id="Loading-and-preparing-data"><a class="docs-heading-anchor" href="#Loading-and-preparing-data">Loading and preparing data</a><a id="Loading-and-preparing-data-1"></a><a class="docs-heading-anchor-permalink" href="#Loading-and-preparing-data" title="Permalink"></a></h2><p>To experiment with machine learning models, we use the <code>RDatasets</code> package, which stores many machine learning datasets, and we load the data by</p><pre><code class="language-julia hljs">using Plots
using StatsPlots
using RDatasets

iris = dataset(&quot;datasets&quot;, &quot;iris&quot;)</code></pre><div><div style = "float: left;"><span>5×5 DataFrame</span></div><div style = "clear: both;"></div></div><div class = "data-frame" style = "overflow-x: scroll;"><table class = "data-frame" style = "margin-bottom: 6px;"><thead><tr class = "columnLabelRow"><th class = "stubheadLabel" style = "font-weight: bold; text-align: right;">Row</th><th style = "text-align: left;">SepalLength</th><th style = "text-align: left;">SepalWidth</th><th style = "text-align: left;">PetalLength</th><th style = "text-align: left;">PetalWidth</th><th style = "text-align: left;">Species</th></tr><tr class = "columnLabelRow"><th class = "stubheadLabel" style = "font-weight: bold; text-align: right;"></th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "Float64" style = "text-align: left;">Float64</th><th title = "CategoricalArrays.CategoricalValue{String, UInt8}" style = "text-align: left;">Cat…</th></tr></thead><tbody><tr class = "dataRow"><td class = "rowLabel" style = "font-weight: bold; text-align: right;">1</td><td style = "text-align: right;">5.1</td><td style = "text-align: right;">3.5</td><td style = "text-align: right;">1.4</td><td style = "text-align: right;">0.2</td><td style = "text-align: left;">setosa</td></tr><tr class = "dataRow"><td class = "rowLabel" style = "font-weight: bold; text-align: right;">2</td><td style = "text-align: right;">4.9</td><td style = "text-align: right;">3.0</td><td style = "text-align: right;">1.4</td><td style = "text-align: right;">0.2</td><td style = "text-align: left;">setosa</td></tr><tr class = "dataRow"><td class = "rowLabel" style = "font-weight: bold; text-align: right;">3</td><td style = "text-align: right;">4.7</td><td style = "text-align: right;">3.2</td><td style = "text-align: right;">1.3</td><td style = "text-align: right;">0.2</td><td style = "text-align: left;">setosa</td></tr><tr class = "dataRow"><td class = "rowLabel" style = "font-weight: bold; text-align: right;">4</td><td style = "text-align: right;">4.6</td><td style = "text-align: right;">3.1</td><td style = "text-align: right;">1.5</td><td style = "text-align: right;">0.2</td><td style = "text-align: left;">setosa</td></tr><tr class = "dataRow"><td class = "rowLabel" style = "font-weight: bold; text-align: right;">5</td><td style = "text-align: right;">5.0</td><td style = "text-align: right;">3.6</td><td style = "text-align: right;">1.4</td><td style = "text-align: right;">0.2</td><td style = "text-align: left;">setosa</td></tr></tbody></table></div><p>Printing the first five entries of the data shows that they are saved in DataFrame, and the columns (features) are sepal length, sepal width, petal length and petal width.</p><p>When designing a classification method, a good practice is to perform at least a basic analysis of the data. That may include checking for NaNs, infinite values, obvious errors, standard deviations of features or others. Here, we only plot the data. </p><div class="admonition is-warning" id="Exercise:-1d6fbff78661b006"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-1d6fbff78661b006" title="Permalink"></a></header><div class="admonition-body"><p>We will simplify the goal and estimate the dependence of petal width on petal length. Create the data <span>$X$</span> (do not forget to add the bias) and the labels <span>$y$</span>.</p><p>Make a graph of the dependence of petal width on petal length.</p></div></div><details class="admonition is-details" id="Solution:-355135a7b169ce0b"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-355135a7b169ce0b" title="Permalink"></a></summary><div class="admonition-body"><p>Since the petal length and width are the third and fourth columns, we assign them to <code>X</code> and <code>y</code>, respectively. We can use <code>iris[:, 4]</code> or <code>iris[:, :PetalWidth]</code> instead of <code>iris.PetalWidth</code>, but the first possibility is vulnerable to errors. We need to concatenate <code>X</code> it with a vector of ones to add the bias.</p><pre><code class="language-julia hljs">y = iris.PetalWidth
X = hcat(iris.PetalLength, ones(length(y)))</code></pre><p>The best visualization is by the scatter plot. We use the version from the <code>StatsPlots</code> package but the one from the <code>Plots</code> package would be naturally sufficient.</p><pre><code class="language-julia hljs">@df iris scatter(
    :PetalLength,
    :PetalWidth;
    label=&quot;&quot;,
    xlabel = &quot;Petal length&quot;,
    ylabel = &quot;Petal width&quot;
)</code></pre></div></details><p><img src="../iris_lin1.svg" alt/></p><p>The figure shows a positive correlation between length and width. This is natural as bigger petals mean both longer and wider petals. We will quantify this dependence by linear regression.</p><h2 id="Training-the-classifier"><a class="docs-heading-anchor" href="#Training-the-classifier">Training the classifier</a><a id="Training-the-classifier-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-classifier" title="Permalink"></a></h2><div class="admonition is-warning" id="Exercise:-2d038407131b6c3"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-2d038407131b6c3" title="Permalink"></a></header><div class="admonition-body"><p>Use the closed-form formula to get the coefficients <span>$w$</span> for the linear regression. Then use the <code>optim</code> method derived in the previous lecture to solve the optimization problem via gradient descent. The results should be identical.</p></div></div><details class="admonition is-details" id="Solution:-702f753aabcc0371"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-702f753aabcc0371" title="Permalink"></a></summary><div class="admonition-body"><p>The closed-form expression is <span>$(X^\top X)^{-1}X^\top y$</span>. In the <a href="../../lecture_08/exercises/#l7-exercises">exercises</a> to the previous lecture, we explained that writing <code>(X&#39;*X) \ (X&#39;*y)</code> is better than <code>inv(X&#39;*X)*X&#39;*y</code> because the former does not compute the matrix inverse. As a side-note, can you guess the difference between <code>inv(X&#39;*X)*X&#39;*y</code> and <code>inv(X&#39;*X)*(X&#39;*y)</code>?</p><pre><code class="language-julia hljs">w = (X&#39;*X) \ (X&#39;*y)</code></pre><p>For the gradient descent, we first realize that the formula for the derivative is <span>$X^\top (Xw-y)$</span>. Defining the derivative function in <code>g</code>, we call the <code>optim</code> method in the same way as in the last lecture. Since we use the sum and not mean in the objective, we need to use a much smaller step size.</p><pre><code class="language-julia hljs">g(w) = X&#39;*(X*w-y)
w2 = optim([], g, zeros(size(X,2)), GD(1e-4); max_iter=10000)</code></pre><p>The difference between the solutions is</p><pre><code class="language-julia hljs">using LinearAlgebra

norm(w-w2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2.8849444094365703e-12</code></pre><p>which is acceptable.</p></div></details><p>The correct solution is</p><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
  0.4157554163524131
 -0.3630755213190365</code></pre><p>Now we can estimate the petal width if only petal length is known.</p><div class="admonition is-warning" id="Exercise:-7f41f9c5c858fed6"><header class="admonition-header">Exercise:<a class="admonition-anchor" href="#Exercise:-7f41f9c5c858fed6" title="Permalink"></a></header><div class="admonition-body"><p>Write the dependence of the petal width on the petal length. Plot it in the previous graph.</p></div></div><details class="admonition is-details" id="Solution:-648ed5f76a776863"><summary class="admonition-header">Solution:<a class="admonition-anchor" href="#Solution:-648ed5f76a776863" title="Permalink"></a></summary><div class="admonition-body"><p>The desired dependence is</p><p class="math-container">\[\text{width} \approx -0.36 + 0.42*\text{length}.\]</p><p>Before plotting the prediction, we save it into <code>f_pred</code>.</p><pre><code class="language-julia hljs">f_pred(x::Real, w) = w[1]*x + w[2]</code></pre><p>Then we create the limits <code>x_lim</code> and finally plot the prediction function.</p><pre><code class="language-julia hljs">x_lims = extrema(iris.PetalLength) .+ [-0.1, 0.1]

@df iris scatter(
    :PetalLength,
    :PetalWidth;
    xlabel = &quot;Petal length&quot;,
    ylabel = &quot;Petal width&quot;,
    label = &quot;&quot;,
    legend = :topleft,
)

plot!(x_lims, x -&gt; f_pred(x,w); label = &quot;Prediction&quot;, line = (:black,3))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;/home/runner/work/Julia-for-Optimization-and-Learning/Julia-for-Optimization-and-Learning/docs/build/lecture_09/iris_lin2.svg&quot;</code></pre></div></details><p><img src="../iris_lin2.svg" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../theory/">« Introduction to regression and classification</a><a class="docs-footer-nextpage" href="../logistic/">Logistic regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 10 December 2025 09:48">Wednesday 10 December 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
