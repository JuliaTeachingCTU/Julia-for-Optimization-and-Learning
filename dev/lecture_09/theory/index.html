<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theory of neural networks · Numerical computing in Julia</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Numerical computing in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Numerical computing in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit">Numerical computing in Julia</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why_julia/">Why Julia?</a></li><li><a class="tocitem" href="../../howto/">How to...</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Mathematical operations and Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluation</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of Variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception Handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/basics/">Package management</a></li><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">5: Composite types and constructors</span></li><li><span class="tocitem">6: Modules and enviroments</span></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Theory of optimization</a></li><li><a class="tocitem" href="../../lecture_07/gradients/">Visualization of gradients</a></li><li><a class="tocitem" href="../../lecture_07/numerical_methods/">Numerical methods</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox" checked/><label class="tocitem" for="menuitem-12"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Theory of neural networks</a><ul class="internal"><li><a class="tocitem" href="#Neural-networks"><span>Neural networks</span></a></li><li><a class="tocitem" href="#Layers"><span>Layers</span></a></li><li><a class="tocitem" href="#Approximation-quality"><span>Approximation quality</span></a></li><li><a class="tocitem" href="#Gradient-computation"><span>Gradient computation</span></a></li></ul></li><li><a class="tocitem" href="../nn/">Neural networks</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">11: Ordinary differential equations</span></li><li><span class="tocitem">12: Statistics I.</span></li><li><span class="tocitem">13: Statistics II.</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">9: Neural networks I.</a></li><li class="is-active"><a href>Theory of neural networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theory of neural networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_09/theory.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Theory-of-neural-networks"><a class="docs-heading-anchor" href="#Theory-of-neural-networks">Theory of neural networks</a><a id="Theory-of-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Theory-of-neural-networks" title="Permalink"></a></h1><p>Neural network appeared for the first time decades ago but were almost forgotten after a few years. Their resurgence in the last one or two decades is mainly due to the available computational power and their impressive list of applications which include</p><ul><li>One of the first application was an automatic reading of postal codes to automatize the sorting of postcards. Since only ten black and white digits can appear at five predetermined locations, simple networks could be used.</li><li>A similar type of neural networks (convolutional) is used in autonomous vehicles to provide information about cars, pedestrians or traffic signs. These networks also use bounding boxes to specify the position of the desired object.</li><li>While the previous techniques used 2D structure of the input (image), recurrent neural networks are used for series (text, sound). The major application are automatic translations.</li><li>Another applications include generating new content. While there may exist useful applications such as artistic composition (music or writing scripts), these networks are often used to generate fake content (news, images).</li></ul><h2 id="Neural-networks"><a class="docs-heading-anchor" href="#Neural-networks">Neural networks</a><a id="Neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-networks" title="Permalink"></a></h2><p>The first three bullets from the previous paragraph are all used for classification. Even though neural networks can be similarly used for regression, this usage is not so developed. The idea is the same as for linear networks. For an input <span>$x$</span> with a label <span>$y$</span> and a classifier <span>$f$</span>, it minimizes the loss between the prediction <span>$f(x)$</span> and label <span>$y$</span>. Having <span>$n$</span> samples, this results in</p><p class="math-container">\[\operatorname{minimize}\qquad \sum_{i=1}^n \operatorname{loss}(f(x_i), y_i).\]</p><p>The previous lecture used the linear classifier <span>$f(x)=w^\top x$</span> and the cross-entropy loss for classification and the squared <span>$l_2$</span> norm <span>$\operatorname{loss}(\hat y, y) = (\hat y - y)^2$</span> for regression.</p><p>The main ideas of neural networks is to use more complex function <span>$f$</span> with a certain structure such that:</p><ul><li>It has a good approximative quality.</li><li>It does not contain many parameters to learn (train).</li><li>The computation of derivatives (training) is simple.</li></ul><h2 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h2><p>The previous three bullets were achieved in an ellegant way by representing the neural network via  a layered structure. The input goes into the first layers, the output of the first layer goes into the second layer and so on. Mathematically speaking, a network <span>$f$</span> with <span>$M$</span> layers has the structure</p><p class="math-container">\[f(x) = (f_M \circ \dots \circ f_1)(x),\]</p><p>where <span>$f_1,\dots,f_M$</span> are individual layers. Since two layers which are not next to each other (such as the first and the third layer) are never directly connected (with the exception of skip connections), the function value can be propagrated through the network in a simple way. The same holds true for the gradients which can be easily computed via chain rules.</p><p><img src="../nn.png" alt/></p><h4 id="Dense-layer"><a class="docs-heading-anchor" href="#Dense-layer">Dense layer</a><a id="Dense-layer-1"></a><a class="docs-heading-anchor-permalink" href="#Dense-layer" title="Permalink"></a></h4><p>Dense layer is the simplest layer which has the form</p><p class="math-container">\[f_m(z) = l(W_mz + b_m),\]</p><p>where <span>$W_m$</span> is a matrix of appropriate dimensions, <span>$b_m$</span> is the bias (shift) and <span>$l$</span> is an activation function. The activation function is usually written as <span>$l:\mathbb{R}\to\mathbb{R}$</span> and its operation on a vector <span>$W_mz + b_m$</span> is understood componentwise. Examples of the activation functions include:</p><p class="math-container">\[\begin{aligned}
&amp;\text{Sigmoid:}&amp;l(x) &amp;= \frac{1}{1+e^{-x}} ,\\
&amp;\text{ReLU:}&amp;l(x) &amp;= \operatorname{max}\{0,x\}, \\
&amp;\text{Softplus:}&amp;l(x) &amp;= \log(1+e^x), \\
&amp;\text{Swish:}&amp;l(x) &amp;= \frac{x}{1+e^{-x}} ,\\
\end{aligned}\]</p><p><img src="../Activation.svg" alt/></p><h4 id="Other-layers"><a class="docs-heading-anchor" href="#Other-layers">Other layers</a><a id="Other-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Other-layers" title="Permalink"></a></h4><p>There are many other layers (convolutional, recurrent, pooling, ...) which we will go through in the next lesson.</p><h2 id="Approximation-quality"><a class="docs-heading-anchor" href="#Approximation-quality">Approximation quality</a><a id="Approximation-quality-1"></a><a class="docs-heading-anchor-permalink" href="#Approximation-quality" title="Permalink"></a></h2><p>Even shallow neural networks (not many layers) are able to approximate any continuous function as the next theorem states.</p><div class = "theorem-body">
<header class = "theorem-header">Theorem: Universal approximation of neural networks</header><p><p>Let <span>$g:[a,b]\to \mathbb{R}$</span> be a continuous function defined on an interval. Then for every <span>$\varepsilon&gt;0$</span>, there is a neural network <span>$f$</span> such that <span>$\|f-g\|_{\infty}\le \varepsilon$</span>.</p><p>Moreover, this network can be chosen as a chain of the following two layers:</p><ul><li>Dense layer with ReLU activation function.</li><li>Dense layer with identity activation function.</li></ul></p></div><p>As the proof suggest (Exercise 1), the price to pay is that the network needs to be extremely wide (lots of hidden neurons). x</p><h2 id="Gradient-computation"><a class="docs-heading-anchor" href="#Gradient-computation">Gradient computation</a><a id="Gradient-computation-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-computation" title="Permalink"></a></h2></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_08/exercises/">« Exercises</a><a class="docs-footer-nextpage" href="../nn/">Neural networks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 4 January 2021 14:12">Monday 4 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
