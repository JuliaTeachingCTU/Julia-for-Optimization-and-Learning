<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Exercises · Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Machine Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Machine Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox" checked/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../nn/">Neural networks</a></li><li class="is-active"><a class="tocitem" href>Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Statistics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">9: Neural networks I.</a></li><li class="is-active"><a href>Exercises</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Exercises</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_09/exercises.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="l9-exercises"><a class="docs-heading-anchor" href="#l9-exercises">Exercises</a><a id="l9-exercises-1"></a><a class="docs-heading-anchor-permalink" href="#l9-exercises" title="Permalink"></a></h1><div class = "homework-body">
<header class = "homework-header">Homework: Optimal setting</header><p><p>Perform an analysis of hyperparameters of the neural network from this lecture. Examples may include network architecture, learning rate (stepsize), activation functions or normalization.</p><p>Write a short summary (in LaTeX) of your suggestions.</p></p></div><div class = "exercise-body">
<header class = "exercise-header">Exercise 1: Universal approximation of neural networks</header><p><p>Proof the theorem about universal approximation of neural networks.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Since piecewise linear functions are dense in the set of continuous functions, there is a piecewise linear function <span>$h$</span> such that <span>$\|h-g\|_{\infty}\le \varepsilon$</span>. Assume that <span>$h$</span> has kinks at <span>$x_1&lt;\dots&lt;x_n$</span> with function values <span>$h(x_i)=y_i$</span> for <span>$i=1,\dots,n$</span>. Defining</p><p class="math-container">\[d_i = \frac{y_{i+1}-y_i}{x_{i+1}-x_i},\]</p><p>then <span>$h$</span> has the form</p><p class="math-container">\[h(x) = y_i + d_i(x-x_i) \qquad\text{ for }x\in [x_i,x_{i+1}].\]</p><p>It is not difficult to show that</p><p class="math-container">\[h(x) = y_1 + \sum_{i=1}^n(d_i-d_{i-1})\operatorname{max}\{x-x_i,0\},\]</p><p>where we defined <span>$d_0=0$</span>.</p><p>Then <span>$h$</span> can be represented as the following network with two layers:</p><ul><li>Dense layer with <span>$n$</span> hidden neurons and ReLU activation function. Neuron <span>$i$</span> has weight <span>$1$</span> and bias <span>$-x_i$</span>.</li><li>Dense layer with <span>$1$</span> ouput neurons and identity activation function. Connection <span>$i$</span> has weight <span>$d_i-d_{i-1}$</span> and the joint bias is <span>$y_1$</span>.</li></ul><p>This finishes the proof.</p></p></details><div class = "exercise-body">
<header class = "exercise-header">Exercise 2: Keyword arguments</header><p><p>Keyword arguments (often denoted as <code>kwargs...</code> but any name may be used) specify additional arguments which do not need to be used when the function is called. We recall the <code>prepare_data</code> function written earlier.</p><pre><code class="language-julia">function prepare_data(X, y; do_normal=true, kwargs...)
    X_train, y_train, X_test, y_test = split(X, y; kwargs...)

    if do_normal
        X_train, X_test = normalize(X_train, X_test)
    end

    X_train = Matrix(X_train&#39;)
    X_test = Matrix(X_test&#39;)

    classes = unique(y)

    y_train = onehot(y_train, classes)
    y_test = onehot(y_test, classes)

    return X_train, y_train, X_test, y_test, classes
end</code></pre><p>All keyword arguments <code>kwargs</code> will be passed to the <code>split</code> function. They could also be passed to <code>normalize</code> or any other function. The benefit is that we do not need to specify the keyword arguments for <code>split</code> in <code>prepare_data</code>.</p><p>Recall that <code>split</code> takes <code>ratio_split</code> as an optional argument. Write an one-line function <code>ratio_train</code> which gets the training and testing sets and computes the ratio of samples in the training set. Then call the <code>prepare_data</code> with:</p><ul><li>no normalization and default split ratio;</li><li>normalization and split ratio of 50/50;</li><li>no normalization and split ratio of 50/50.</li></ul></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The <code>ratio_train</code> function reads</p><pre><code class="language-julia">ratio_train(X_train, X_test) = size(X_train, 2) / (size(X_train,2) + size(X_test,2))</code></pre><p>For the first call, we want to use the default ratio, hence we do not pass <code>ratio_split</code>. Since we do not want to use normalization, we need to pass <code>do_normal=false</code>.</p><pre><code class="language-julia">X_train, y_train, X_test, y_test, classes = prepare_data(X, y; do_normal=false)
println(&quot;Ratio train/test = &quot; * string(ratio_train(X_train, X_test)))</code></pre><pre class="documenter-example-output">Ratio train/test = 0.8</pre><p>For the second call, it is the other way round. We use the default normalization, thus we do not need to specify <code>do_normal=true</code> (even though it may be a good idea to do so). We need to pass <code>ratio_train=0.5</code>.</p><pre><code class="language-julia">X_train, y_train, X_test, y_test, classes = prepare_data(X, y; ratio_train=0.5)
println(&quot;Ratio train/test = &quot; * string(ratio_train(X_train, X_test)))</code></pre><pre class="documenter-example-output">Ratio train/test = 0.5</pre><p>For the final call, we need to use both arguments. Note that  <code>do_normal</code> is passed as an optional argument and therefore, its default value will be overwritten while <code>ratio_train</code> is passed in <code>kwargs</code> and goes into the <code>split</code> function.</p><pre><code class="language-julia">X_train, y_train, X_test, y_test, classes = prepare_data(X, y; do_normal=false, ratio_train=0.5)
println(&quot;Ratio train/test = &quot; * string(ratio_train(X_train, X_test)))</code></pre><pre class="documenter-example-output">Ratio train/test = 0.5</pre></p></details><div class = "exercise-body">
<header class = "exercise-header">Exercise 3: Showing the contours</header><p><p>The goal of this exercise will be to show the separation graphically. For this reason, we need to consider only two features. The description may be a bit unclear. If you are uncertain, check the correct answer and try to reproduce the graph.</p><p>First, use the same data and the same training procedure as during lecture with the exception that we will consider only the last two features of <span>$X$</span>. Train the network with five hidden neurons.</p><p>Second, write a function <code>rectangles(x, y, r)</code> where <code>x</code> and <code>y</code> are vectors of the same size and <code>r</code> is a positive number. It should return two outputs of size <code>(5,n)</code> where <code>n</code> is length of <code>x</code> and <code>y</code>. Column <code>i</code> of the outputs forms the edges of the rectangle with center <code>(x[i],y[i])</code> and length <code>2r</code>. The first and second outputs correspond to <span>$x$</span> and <span>$y$</span> coordinates, respectively. For plotting, the outputs need to have five edges (instead of four), where the first and the last one are the same.</p><p>Thirs, create a uniform discretization of <span>$[-2,2]\times [-2,2]$</span> with grid <span>$0.1$</span> and convert it into rectangles using the <code>rectangle</code> function. Assign one of three colours (blue, red, green) to each rectangle based on the prediction of its center. Plot all rectangle using the corresponding colour. Use <code>fill=(0,0.2,???)</code>, where <code>???</code> is the colour. Finally, using the scatter plot, show the testing data in the same colours.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The procedure for training the network is the same as during the lecture</p><pre><code class="language-julia">X_train, y_train, X_test, y_test, classes = prepare_data(X[:,3:4], y)

W1, b1, W2, b2 = initialize(size(X_train,1), 5, size(y_train,1))

α = 1e-1
max_iter = 1000
for iter in 1:max_iter
    grad_all = [grad(X_train[:,k], y_train[:,k], W1, b1, W2, b2) for k in 1:size(X_train,2)]
    grad_mean = mean_tuple(grad_all)

    W1 .-= α*grad_mean[2]
    b1 .-= α*grad_mean[3]
    W2 .-= α*grad_mean[4]
    b2 .-= α*grad_mean[5]
end</code></pre><p>For the <code>rectangles</code> function, we first create realize that if a rectangle is centered at zero, its x and y coordinates are <code>[-r; r; r; -r; -r]</code> and <code>[-r; -r; r; r; -r]</code>, respectively. Then we reshape the input <code>x</code> vector into a row vector and move the rectangle derived above into the proper center. Note that the implementation adds a column vector <code>reshape(x, 1, :)</code> and a row vector <code>[-r; r; r; -r; -r]</code>. The result is a matrix with an appropriate dimension.</p><pre><code class="language-julia">function rectangles(x::AbstractVector, y::AbstractVector, r::Number)
    xs = reshape(x, 1, :) .+ [-r; r; r; -r; -r]
    ys = reshape(y, 1, :) .+ [-r; -r; r; r; -r]
    return xs, ys
end</code></pre><p>To create the discretization, we first discretize the <span>$x$</span> axis via <code>x = collect(-2:x_diff:2)</code>. To get the grid, we then create all combinations of <span>$x$</span> with <span>$x$</span>. To have a proper dimension for the neural network, it should be dimension <code>(1,n^2)</code>. Finally, we evaluate all these points via the model <code>m</code> and convert the one-hot into the one-cold representation.</p><pre><code class="language-julia">x_diff = 0.1
x = collect(-2:x_diff:2)
n = length(x)
xy = hcat(repeat(x, n, 1), repeat(x&#39;, n, 1)[:])&#39;
z = m(xy, W1, b1, W2, b2)
z = onecold(z, classes)</code></pre><p>Finally, for plotting, we first define the three colours as three symbols. Then we create a loop over all three classes. In the first plot, we need to call <code>plot</code> while in the remaining ones, we call <code>plot!</code>. For each predicted class, we find the indices with the predicted class <code>z.==classes[i]</code> and plot the corresponding rectangles. Besides the <code>fill</code> parameter, we specify the line as well via <code>line</code>. For the prediction on the testing set, we repeat the same procedure. This time the indices are found by <code>onecold(y_test, classes) .== classes[i]</code> and we use the scatter plot. We specify the marker for a better visualization.</p><pre><code class="language-julia">using Plots

colours = (:blue, :red, :green)

for i in 1:length(classes)
    i == 1 ? p = plot : p = plot!
    ii1 = z .== classes[i]
    ii2 = onecold(y_test, classes) .== classes[i]
    p(rectangles(xy[1,ii1], xy[2,ii1], x_diff/2)..., line=(0, 0.2, colours[i]), fill=(0, 0.2, colours[i]), label=&quot;&quot;)
    scatter!(X_test[1,ii2][:], X_test[2,ii2][:], marker=(8, 0.8, colours[i]), label=y_name[classes[i]], legend=:topleft)
end</code></pre></p></details><p><img src="../Separation.svg" alt/></p><div class = "exercise-body">
<header class = "exercise-header">Exercise 4: Overfitting</header><p><p>This exercise will show the well-known effect of overfitting. Since the model sees only the testing set, it may happen that it fits it too perfectly (overfits it) and generalizes poorly to unseen examples (testing set).</p><p>As in the previous exercise, consider only the last two features and train the network with 50 hidden neurons for 5000 iterations. Plot the evolution of the objective (loss) function on the training and testing sets.</p><p>Make two countour plots as in the previous exercise. The first one depicts the scatter plot for the testing set while the second one depicts it for the training set. Describe what went wrong.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The first part of the exercise is the same as before. The only change is that we need to save the training and testing objective. Note that the training loss could be extracted from <code>grad_all</code>. </p><pre><code class="language-julia">X_train, y_train, X_test, y_test, classes = prepare_data(X[:,3:4], y)

W1, b1, W2, b2 = initialize(size(X_train,1), 50, size(y_train,1))

α = 1e-1
max_iter = 5000
L_train = zeros(max_iter)
L_test = zeros(max_iter)
for iter in 1:max_iter
    grad_all = [grad(X_train[:,k], y_train[:,k], W1, b1, W2, b2) for k in 1:size(X_train,2)]
    grad_mean = mean_tuple(grad_all)

    W1 .-= α*grad_mean[2]
    b1 .-= α*grad_mean[3]
    W2 .-= α*grad_mean[4]
    b2 .-= α*grad_mean[5]

    L_train[iter] = mean(loss(m(X_train, W1, b1, W2, b2), y_train)[:])
    L_test[iter] = mean(loss(m(X_test,  W1, b1, W2, b2), y_test)[:])
end</code></pre><p>Then we plot it. We ignore the first nine iterations. </p><pre><code class="language-julia">plot(L_train[10:end], xlabel=&quot;Iteration&quot;, label=&quot;Training loss&quot;, legend=:topleft)
plot!(L_test[10:end], label=&quot;Testing loss&quot;)</code></pre><p><img src="../Train_test.svg" alt/></p><p>We see the classical procedure of overfitting. While the loss function on the training set decreases steadily, on the testing set, it decreases first and after approximately 100 iterations, it starts increasing. This behaviour may be prevented by several techniques which we discuss in the next lecture. </p><p>We create the contour plot in the same way as in the previous exercise.</p><pre><code class="language-julia">z = m(xy, W1, b1, W2, b2)
z = onecold(z, classes)

for i in 1:length(classes)
    i == 1 ? p = plot : p = plot!
    ii1 = z .== classes[i]
    ii2 = onecold(y_test, classes) .== classes[i]
    p(rectangles(xy[1,ii1], xy[2,ii1], x_diff/2)..., line=(0, 0.2, colours[i]), fill=(0, 0.2, colours[i]), label=&quot;&quot;, title=&quot;Testing set&quot;)
    scatter!(X_test[1,ii2][:], X_test[2,ii2][:], marker=(8, 0.8, colours[i]), label=y_name[classes[i]], legend=:topleft)
end

for i in 1:length(classes)
    i == 1 ? p = plot : p = plot!
    ii1 = z .== classes[i]
    ii2 = onecold(y_train, classes) .== classes[i]
    p(rectangles(xy[1,ii1], xy[2,ii1], x_diff/2)..., line=(0, 0.2, colours[i]), fill=(0, 0.2, colours[i]), label=&quot;&quot;, title=&quot;Training set&quot;)
    scatter!(X_train[1,ii2][:], X_train[2,ii2][:], marker=(8, 0.8, colours[i]), label=y_name[classes[i]], legend=:topleft)
end</code></pre><p><img src="../Over1.svg" alt/></p><p><img src="../Over2.svg" alt/></p><p>We see that the separation on the testing set is very good but it could be better for the two bottommost green circles (iris virginica). The model predicted (in background) the red color (iris versicolor) there. This is wrong. The reason is clear from the picture depicting the training set. The classifier tried to fit perfectly the boundary between the green and red points, making a outward-pointing tip there from otherwise a rather flat boundary. This is precisely overfitting and the reason of the misclassification on the testing set.</p></p></details><p><img src="../Train_test0.svg" alt/></p><p><img src="../Over1.svg" alt/></p><p><img src="../Over2.svg" alt/></p><div class = "exercise-body">
<header class = "exercise-header">Exercise 5: Generalization</header><p><p>Compare the contour plots from Exercises 2 and 3. They are strikingly different, especially in the top-left and bottom-right corners. Why is that?</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Since the dataset does not contain any data in the top-left or bottom-right corners, it does not know what to put there. From its perspective, both separations are very good. This raises an important take-away message.</p><div class = "info-body">
<header class = "info-header">Generalization</header><p><p>If a classifier does not have any data in some region, it may predict anything there. Including predictions with no sense.</p></p></div></p></details></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../nn/">« Neural networks</a><a class="docs-footer-nextpage" href="../../lecture_10/theory/">Theory of neural networks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 13 April 2021 11:16">Tuesday 13 April 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
