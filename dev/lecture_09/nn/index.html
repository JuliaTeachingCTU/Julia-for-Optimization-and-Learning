<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Neural networks Â· Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Machine Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Machine Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composit types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox" checked/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li class="is-active"><a class="tocitem" href>Neural networks</a><ul class="internal"><li><a class="tocitem" href="#Prepare-data"><span>Prepare data</span></a></li><li><a class="tocitem" href="#Create-the-network"><span>Create the network</span></a></li><li><a class="tocitem" href="#Train-the-network"><span>Train the network</span></a></li><li><a class="tocitem" href="#Prediction"><span>Prediction</span></a></li></ul></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Statistics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">9: Neural networks I.</a></li><li class="is-active"><a href>Neural networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Neural networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_09/nn.md" title="Edit on GitHub"><span class="docs-icon fab">ï</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-networks"><a class="docs-heading-anchor" href="#Neural-networks">Neural networks</a><a id="Neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-networks" title="Permalink"></a></h1><p>During this lecture, we will train a better classifier for the iris dataset. From the previous lecture, it will differ in several points:</p><ul><li>It will use a neural network instead of the linear classifier.</li><li>It will use all features and not only two.</li><li>It will use all classes and not only two.</li></ul><h2 id="Prepare-data"><a class="docs-heading-anchor" href="#Prepare-data">Prepare data</a><a id="Prepare-data-1"></a><a class="docs-heading-anchor-permalink" href="#Prepare-data" title="Permalink"></a></h2><p>We start with loading the iris dataset in the same way as in the last lecture.</p><pre><code class="language-julia">using BSON: @load

file_name = joinpath(&quot;data&quot;, &quot;iris.bson&quot;)
@load file_name X y y_name</code></pre><p>Since we will use random functions, it is a good idea to fix the seed. When the code is run multiple times, the results will always be the same. Since Julia uses one global seed (unlike Python, which uses different seeds in each package), we all should see the same results. However, if you obtain slightly different results, it may have happened due to a different Julia version or operating system.</p><pre><code class="language-julia">using Random

Random.seed!(666)</code></pre><p>The first exercise splits the dataset into the training and testing set.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Write the <code>split</code> function, which splits the dataset and the labels into training and testing set. Its input should be the dataset <code>X</code> and the labels <code>y</code>. It should have four outputs. Include 80% of data in the training set and 20% of data in the testing set.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Since the input may have different forms, we assume that <code>y</code> is a vector and the samples of  <code>X</code> and <code>y</code> are across the first dimension. If this is not satisfied, the <code>@assert</code> statement returns an error. This is not necessary to include but it makes the code much more error-prone.</p><p>To split the dataset, we first determine the number of samples <code>n_train</code> in the training set. We need to round it and convert it to integer. For the split, we create a random permumation of indices and then select the first <code>n_train</code> indices as the indices of the training set and the remaining as the indices of the testing set.</p><pre><code class="language-julia">function split(X::AbstractMatrix, y::AbstractVector; ratio_train=0.8)
    @assert size(X,1) == size(y,1)

    n = size(X,1)
    n_train = round(Int, ratio_train*n)
    i_rand = randperm(n)
    i_train = i_rand[1:n_train]
    i_test = i_rand[n_train+1:end]

    return X[i_train,:], y[i_train], X[i_test,:], y[i_test]
end</code></pre><p>Then we split the dataset by calling the split function</p><pre><code class="language-julia">X_train, y_train, X_test, y_test = split(X, y)</code></pre></p></details><p>The next exercises normalizes the data. In the previous lecture, we have already normalized the training set. We compute the normalizing constants (mean and standard deviation) for each feature and then apply them to the data. Since the normalization needs to be done before training, and since the testing set is not available during training, the normalizing constants can be computed only from the testing set. This also means that the features on the training set have zero mean and unit variance but features on the testing set may have different mean and variance.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Write the <code>normalize</code> functions as described above. It should have two inputs and two outputs.</p><p>Normalize the data and print the first feature of the first sample in the testing set.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Since the features are in columns, we compute the mean and the standard deviation of each column. Since <code>[??? for ??? in ???]</code> creates a column vector, and since we want to apply this vector to all columns, we need to transpose it to a row vector. Otherwise it could not be broadcasted. Then we normalize the columns. Due to the reason mentioned above, we need to use the same normalizing constant for the training and testing sets.</p><pre><code class="language-julia">using Statistics

function normalize(X_train, X_test)
    col_means = [mean(X_col) for X_col in eachcol(X_train)]&#39;
    col_std = [std(X_col) for X_col in eachcol(X_train)]&#39;

    return (X_train .- col_means) ./ col_std, (X_test .- col_means) ./ col_std
end</code></pre><p>Now we run the <code>normalize</code> function.</p><pre><code class="language-julia">X_train, X_test = normalize(X_train, X_test)</code></pre></p></details><p>The correct answer is</p><pre class="documenter-example-output">-0.9838</pre><p>The standard representation of data in linear and logistic regression is that each row (first dimension) is one sample. However, neural networks work with more-dimensional data (each image is represented by three dimensions). The convention changed and the samples are represented in the last dimension.</p><p>The next exercise modifies the data into a standard form for machine learning.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Modify the data so that the first dimension of <code>X</code> are features and the second one the samples.</p><p>Write <code>onehot</code> function which converts <code>y</code> into the one-hot representation. Write <code>onecold</code> function which converts the one-hot representation into the one-cold (original) representation. Both these functions need to have two arguments, the second one will be <code>classes</code> which will equal to <code>unique(y)</code>.</p><p>Write a one-line check that both work correctly.</p><p>Finally, convert <code>y</code> into its one-hot representation.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We need to transpose <code>X</code> by <code>X&#39;</code>. Since this creates an <code>Adjoint</code> type (check <code>typeof(X&#39;)</code>), we convert it to a standard matrix by calling <code>Matrix(X&#39;)</code>.</p><pre><code class="language-julia">X_train = Matrix(X_train&#39;)
X_test = Matrix(X_test&#39;)</code></pre><p>The <code>onehot</code> function first creates an array <code>y_onehot</code>, where the first dimension is the number of classes. Since all but one entries of each column will be zeros, we initialize it by zeros. Then we run a for loop to fill one into each column. We perform the for loop over all classes but it is also possible to perform it over all columns.</p><pre><code class="language-julia">function onehot(y, classes)
    y_onehot = zeros(length(classes), length(y))
    for i in 1:length(classes)
        y_onehot[i,y.==classes[i]] .= 1
    end
    return y_onehot
end</code></pre><p>The <code>onecold</code> function finds the index of its maximum value via the <code>findmax</code> function. This is repeated for every column  <code>y_col</code>.</p><pre><code class="language-julia">onecold(y, classes) = [classes[findmax(y_col)[2]] for y_col in eachcol(y)]</code></pre><p>Functions <code>onehot</code> and <code>onecold</code> should be inverse to each other. That means that if we call them in succession, we obtain the original input. We could manually check</p><pre><code class="language-julia">classes = unique(y)

isequal(onecold(onehot(y, classes), classes), y)</code></pre><p>but it is better to perform this check automatically by including the error message</p><pre><code class="language-julia">!isequal(onecold(onehot(y, classes), classes), y) &amp;&amp; error(&quot;Function onehot or onecold is wrong.&quot;)</code></pre><p>Now, the modification of  the labels is straigforward. As in the case of the matrix, we need to modify the split data.</p><pre><code class="language-julia">y_train = onehot(y_train, classes)
y_test = onehot(y_test, classes)</code></pre></p></details><p>Preparing the data is spread over many lines. It is better to combine them into one function</p><pre><code class="language-julia">function prepare_data(X, y; do_normal=true, kwargs...)
    X_train, y_train, X_test, y_test = split(X, y; kwargs...)

    if do_normal
        X_train, X_test = normalize(X_train, X_test)
    end

    X_train = Matrix(X_train&#39;)
    X_test = Matrix(X_test&#39;)

    classes = unique(y)

    y_train = onehot(y_train, classes)
    y_test = onehot(y_test, classes)

    return X_train, y_train, X_test, y_test, classes
end</code></pre><p>Then the whole code to load and preprocess the data can be summarized in just three lines.</p><pre><code class="language-julia">file_name = joinpath(&quot;data&quot;, &quot;iris.bson&quot;)
@load file_name X y y_name

X_train, y_train, X_test, y_test, classes = prepare_data(X, y)</code></pre><p>Writing function <code>prepare_data</code> as above has other advantages, we will get back to them in the exercises.</p><h2 id="Create-the-network"><a class="docs-heading-anchor" href="#Create-the-network">Create the network</a><a id="Create-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Create-the-network" title="Permalink"></a></h2><p>We will now construct a simple neural network.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Construct the following network:</p><ul><li>The first layer is a dense layer with the ReLU activation function.</li><li>The second layer is a dense layer with the identity activation function.</li><li>The third layer is the softmax.</li></ul><p>Write is as <code>m(x, ???)</code>, where <code>x</code> is the input and <code>???</code> stands for all weights (parameters to optimize).</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The dense layer is a linear function <code>z1 = W1*x .+ b1</code> followed by an activation function. If we assume that <code>x</code> is a vector, then <code>+</code> would work the same as <code>.+</code> because both <code>W1*x</code> and <code>b</code> are of the same dimension. However, if we want <code>x</code> to be a matrix (each column corresponds to one sample), then we need to write <code>.+</code> because <code>W1*x</code> is a matrix and the vector <code>b</code> needs to be broadcasted to be of the same size. The activation function is the ReLU function which needs to be applied componentwise. The second layer is the same but this time, we need to finish it with the softmax function. If <code>x</code> is a matrix, then <code>z2</code> is a matrix, we  specify that we want to normalize along the first dimension because the first dimension are classes and the second samples. If we assume only vector inputs, then specifying the dimension is not necessary.</p><pre><code class="language-julia">function m(x, W1, b1, W2, b2)
    z1 = W1*x .+ b1
    a1 = max.(z1, 0)
    z2 = W2*a1 .+ b2
    a2 = exp.(z2) ./ sum(exp.(z2), dims=1)
end</code></pre></p></details><p>Before we can use one of the numerical methods from the previous lectures to train the neural network, we need to initialize the weights.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Initialize all the weights randomly following the standard normal distribution. The first layer should have 5 hidden (output) neurons. You need to specify the number of neurons for the other layers correctly.</p><p>Evaluate the model <code>m</code> for the first datum from the training set.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>We write a simple <code>initialize</code> function which takes the number of neurons in each layer as inputs, and randomly generates the matrices.</p><pre><code class="language-julia">function initialize(n1, n2, n3)
    W1 = randn(n2,n1)
    b1 = randn(n2)
    W2 = randn(n3,n2)
    b2 = randn(n3)
    return W1, b1, W2, b2
end</code></pre><p>To initialize, we need to provide <code>n1</code>, <code>n2</code> and <code>n3</code>. The first one is the number of features, the second one is specified to be 5 and the last one must equal to the number of classes (the length of the labels in the one-hot representation).</p><pre><code class="language-julia">W1, b1, W2, b2 = initialize(size(X_train,1), 5, size(y_train,1))</code></pre><p>To evaluate the model, we call the <code>m</code> function with the first sample in the training set</p><pre><code class="language-julia">m(X_train[:,1], W1, b1, W2, b2)</code></pre></p></details><p>The correct answer is</p><pre class="documenter-example-output">[0.1503, 0.0423, 0.8074]</pre><p>Due to the softmax layer, they sum to one and form a probability distribution describing the probability of each classes.</p><h2 id="Train-the-network"><a class="docs-heading-anchor" href="#Train-the-network">Train the network</a><a id="Train-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-network" title="Permalink"></a></h2><p>To train the network, we need to compute the gradients. It is rather complicated, it can be written as follows. When going through the code, it becomes clear that it is just a different form of the chain rule derived in the theoretical part.</p><pre><code class="language-julia">function grad(x::AbstractVector, y, W1, b1, W2, b2; Ïµ=1e-10)
    z1 = W1*x .+ b1
    a1 = max.(z1, 0)
    z2 = W2*a1 .+ b2
    a2 = exp.(z2) ./ sum(exp.(z2))
    l = -sum(y .* log.(a2 .+ Ïµ))

    e_z2 = exp.(z2)
    l_part = (- e_z2 * e_z2&#39; + Diagonal(e_z2 .* sum(e_z2))) / sum(e_z2)^2

    l_a2 = - y ./ (a2 .+ Ïµ)
    l_z2 = l_part * l_a2
    l_a1 = W2&#39; * l_z2
    l_z1 = l_a1 .* (a1 .&gt; 0)
    l_x = W1&#39; * l_z1

    l_W2 = l_z2 * a1&#39;
    l_b2 = l_z2
    l_W1 = l_z1 * x&#39;
    l_b1 = l_z1

    return l, l_W1, l_b1, l_W2, l_b2
end</code></pre><p>The function returns the function value <code>l</code> and derivatives with respect to all four variables.</p><div class = "info-body">
<header class = "info-header">That's it? I thought neural networks are magic...</header><p><p>Well, for a network with two layers and a loss, you can compute the function value and its derivative in only 16 lines of code. And it could be even shorter :)</p></p></div><div class = "info-body">
<header class = "info-header">Simple implementation</header><p><p>The previous function <code>grad</code> can compute the gradient for only one sample. Since the objective in training neural network is a mean over all samples, this mean needs to be included externally. This is NOT the correct way of writing function. However, we decided to present it in the current way to keep the presentation (relatively) simple.</p><p>Any time when a simplication like this is included in the code, a check such as <code>x::AbstractVector</code> or an <code>@assert</code> statement should be included to prevent unexpected errors.</p></p></div><p>Having the gradient at hand, we can finally train the network.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Train the network with a gradient descent with stepsize <span>$\alpha=0.1$</span> for <span>$1000$</span> iterations. Save the objective value at each iteration and plot the results.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>Due to the simplicity of the <code>grad</code> function, <code>grad(X_train[:,k], y_train[:,k], W1, b1, W2, b2)</code> returns a tuple of five objects. We can use the standard trick to create an array of such tuples by going through all columns <code>[grad(X_train[:,k], y_train[:,k], W1, b1, W2, b2) for k in 1:size(X_train,2)]</code>. To obtain a mean from this array, we write the <code>mean_tuple</code> function. To make sure that everything is correct, we specify the input type <code>d::AbstractArray{&lt;:Tuple}</code>. If <code>d</code> is the input data, then <code>d[k]</code> is an element of the array (thefore a tuple) while <code>d[i][k]</code> is an element of the tuple. Since we want to compute the mean over the array, the inner loop needs to be with respect to <code>k</code> while the outer one with respect to <code>i</code>.</p><pre><code class="language-julia">using LinearAlgebra
using Statistics

mean_tuple(d::AbstractArray{&lt;:Tuple}) = [mean([d[k][i] for k in 1:length(d)]) for i in 1:length(d[1])]</code></pre><p>Now the process is simple. We compute the gradient <code>grad_all</code>, then its mean <code>grad_mean</code> via the already written function <code>mean_tuple</code>. The first value of the tuple <code>grad_mean</code> is the objective, the remaining are the gradients. Thus, we save the first value to an array and use the remaining one to update the weights.</p><pre><code class="language-julia">Î± = 1e-1
max_iter = 1000
L = zeros(max_iter)
for iter in 1:max_iter
    grad_all = [grad(X_train[:,k], y_train[:,k], W1, b1, W2, b2) for k in 1:size(X_train,2)]
    grad_mean = mean_tuple(grad_all)

    L[iter] = grad_mean[1]

    W1 .-= Î±*grad_mean[2]
    b1 .-= Î±*grad_mean[3]
    W2 .-= Î±*grad_mean[4]
    b2 .-= Î±*grad_mean[5]
end</code></pre></p></details><p><img src="../loss.svg" alt/></p><h2 id="Prediction"><a class="docs-heading-anchor" href="#Prediction">Prediction</a><a id="Prediction-1"></a><a class="docs-heading-anchor-permalink" href="#Prediction" title="Permalink"></a></h2><p>We have trained our first network. We saw that the loss function keeps decreasing, which is a good sign of a good training procedure. Now we will evaluate the performance.</p><div class = "exercise-body">
<header class = "exercise-header">Exercise:</header><p><p>Write a function which predict the labels for samples. Show the accuracy on both training and testing sets.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The predicted probabilities are obtained by using the model <code>m</code>. The prediction (highest predicted probability) is obtained by converting the one-hot into the one-cold representation. Finally, the accuracy computes in how many cases the prediction equals to the label.</p><pre><code class="language-julia">predict(X) = m(X, W1, b1, W2, b2)
accuracy(X, y) = mean(onecold(predict(X), classes) .== onecold(y, classes))

println(&quot;Train accuracy = $(accuracy(X_train, y_train))&quot;)
println(&quot;Test accuracy = $(accuracy(X_test, y_test))&quot;)</code></pre><pre class="documenter-example-output">Train accuracy = 0.9916666666666667
Test accuracy = 0.9333333333333333</pre></p></details><p>The correct answer is</p><pre class="documenter-example-output">Train accuracy = 0.9917
Test accuracy = 0.9333</pre><p>We see that the testing accuracy is smaller than the training one. This is quite a common phenomenon which is named overfitting. The problem is that the algorithm sees only the data from the training set. If it fits this data &quot;too perfectly&quot;, it is not able to generalize into unseen samples (the testing set).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../theory/">Â« Theory of neural networks</a><a class="docs-footer-nextpage" href="../exercises/">Exercises Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 8 March 2021 18:09">Monday 8 March 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
