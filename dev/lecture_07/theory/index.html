<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Theory of optimization · Numerical computing in Julia</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Numerical computing in Julia logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Numerical computing in Julia logo"/></a><div class="docs-package-name"><span class="docs-autofit">Numerical computing in Julia</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why_julia/">Why Julia?</a></li><li><a class="tocitem" href="../../howto/">How to...</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Mathematical operations and Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/control_flow/">Iteration</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and multiple-dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">4: Composite types and constructors</span></li><li><span class="tocitem">5: Modules and enviroments</span></li><li><span class="tocitem">6: Useful packages</span></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox" checked/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Theory of optimization</a><ul class="internal"><li><a class="tocitem" href="#Gradients"><span>Gradients</span></a></li><li><a class="tocitem" href="#Unconstrained-optimization"><span>Unconstrained optimization</span></a></li><li><a class="tocitem" href="#Constrained-optimization"><span>Constrained optimization</span></a></li></ul></li><li><a class="tocitem" href="../gradients/">Visualization of gradients</a></li><li><a class="tocitem" href="../numerical_methods/">Numerical methods</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">9: Neural networks I.</span></li><li><span class="tocitem">10: Neural networks II.</span></li><li><span class="tocitem">11: Ordinary differential equations</span></li><li><span class="tocitem">12: Statistics I.</span></li><li><span class="tocitem">13: Statistics II.</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">7: Optimization</a></li><li class="is-active"><a href>Theory of optimization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Theory of optimization</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_07/theory.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Theory-of-optimization"><a class="docs-heading-anchor" href="#Theory-of-optimization">Theory of optimization</a><a id="Theory-of-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Theory-of-optimization" title="Permalink"></a></h1><p>The goal of an optimization problem is to minimize of maximize a function <span>$f$</span> over a set <span>$X$</span>. Namely</p><p class="math-container">\[    \begin{aligned}
    \text{minimize}\qquad &amp;f(x) \\
    \text{subject to}\qquad &amp;x\in X.
    \end{aligned}\]</p><p>Should we consider both minimization and maximization problems? No. Because</p><p class="math-container">\[    \text{maximize}\qquad f(x)\]</p><p>is equivalent to </p><p class="math-container">\[    -\text{minimize}\qquad -f(x).\]</p><p>Therefore, it suffices to consider minimization problems.</p><h2 id="Gradients"><a class="docs-heading-anchor" href="#Gradients">Gradients</a><a id="Gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Gradients" title="Permalink"></a></h2><p>As we will see later, gradients are crucial to optimization. For a function <span>$f:\mathbb{R}\to \mathbb{R}$</span> it is defined by</p><p class="math-container">\[f&#39;(x) = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}.\]</p><p>For a mapping  <span>$f:\mathbb{R}^n\to \mathbb{R}^m$</span>, the Jacobian is a matrix <span>$\nabla f(x)$</span> of size <span>$m\times n$</span> which satisfies</p><p class="math-container">\[    \lim_{h\to 0}\frac{\|f(x+h)-f(x) - \nabla f(x)h \|}{\|h\|} = 0.\]</p><p>If a function <span>$f=(f_1,\dots,f_m)$</span> is differentiable, then</p><p class="math-container">\[(\nabla f(x))_{i,j} = \frac{\partial f_i}{\partial x_j}(x) = \lim_{h\to 0}\frac{f_i(x_1,\dots,x_{j-1},x_j+h,x_{j+1},\dots,x_n)-f(x_1,\dots,x_n)}{h}\]</p><div class = "info-body">
<header class = "info-header">Confusion</header><p><p>Gradient <span>$\nabla f(x)$</span> of a function <span>$f:\mathbb{R}^n\to\mathbb{R}$</span> should be of size  <span>$1\times n$</span> but it is commonly considered as <span>$n\times 1$</span>.</p></p></div><p>To compute the gradient, the chain rule is crucial.</p><div class = "theorem-body">
<header class = "theorem-header">Theorem: Chain</header><p><p>Consider two differentiable functions <span>$f:\mathbb{R}^m\to\mathbb{R}^s$</span> and <span>$g:\mathbb{R}^n\to\mathbb{R}^m$</span>. Then its composition <span>$h(x) := f(g(x))$</span> is differentiable with Jacobian</p><p class="math-container">\[\nabla h(x) = \nabla f(g(x))\nabla g(x).\]</p></p></div><h2 id="Unconstrained-optimization"><a class="docs-heading-anchor" href="#Unconstrained-optimization">Unconstrained optimization</a><a id="Unconstrained-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Unconstrained-optimization" title="Permalink"></a></h2><p>What do we look for when we minimize a function <span>$f$</span> over some <span>$X$</span>? The optimal point would be a global minimum, which is a point <span>$x\in X$</span> which satisfies</p><p class="math-container">\[f(x) \le f(y) \text{ for all }y\in X.\]</p><p>This point is often very difficult to find. Sometimes we are able to find a local minimum, which is a global minimum on some small neighborhood of <span>$x$</span>.</p><div class = "theorem-body">
<header class = "theorem-header">Theorem: Connection between optimization problems and gradients</header><p><p>Consider a differentiable function <span>$f$</span> over <span>$X=\mathbb{R}^n$</span>. If <span>$x$</span> is its local minimum, then <span>$\nabla f(x)=0$</span>. Conversely, if <span>$f$</span> is convex, then every point <span>$x$</span> with <span>$\nabla f(x)=0$</span> is a global minimum of <span>$f$</span>.</p></p></div><p>Points with <span>$\nabla f(x)=0$</span> are known as stationary points. Optimization algorithms often try to find them with the hope that they minimize the function <span>$f$</span>.</p><p><img src="../minmax.svg" alt/></p><div class = "info-body">
<header class = "info-header">Take care</header><p><p>This theorem does not hold if <span>$X$</span> is not the whole space.</p></p></div><h2 id="Constrained-optimization"><a class="docs-heading-anchor" href="#Constrained-optimization">Constrained optimization</a><a id="Constrained-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Constrained-optimization" title="Permalink"></a></h2><p>The usual formulation of constrained optimization is in the form</p><p class="math-container">\[\tag{P}
\begin{aligned}
\text{minimize}\qquad &amp;f(x) \\
\text{subject to}\qquad &amp;g_i(x) \le 0,\ i=1,\dots,I, \\
&amp;h_j(x) = 0,\ j=1,\dots,J.
\end{aligned}\]</p><p>This optimization problem is also called the primal formulation. It is closely connected with the Lagrangian</p><p class="math-container">\[L(x;\lambda,\mu) = f(x)  + \sum_{i=1}^I \lambda_i g_i(x) + \sum_{j=1}^J \mu_j h_j(x).\]</p><p>Namely, it is simple to show that the primal formulation (P) is equivalent to</p><p class="math-container">\[\operatorname*{minimize}_x\quad \operatorname*{maximize}_{\lambda\ge 0,\mu}\quad L(x;\lambda,\mu).\]</p><p>The dual problem then switches the minimize and maximize operators to arrive at</p><p class="math-container">\[\tag{D} \operatorname*{maximize}_{\lambda\ge 0,\mu} \quad\operatorname*{minimize}_x\quad L(x;\lambda,\mu).\]</p><div class = "info-body">
<header class = "info-header">Linear programming</header><p><p>The linear program</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;c^\top x \\
\text{subject to}\qquad &amp;Ax=b, \\
&amp;x\ge 0
\end{aligned}\]</p><p>is equivalent to</p><p class="math-container">\[\begin{aligned}
\text{maximize}\qquad &amp;b^\top \mu \\
\text{subject to}\qquad &amp;A^\top \mu\le c.
\end{aligned}\]</p><p>We can observe several things:</p><ol><li>Primal and dual problems switch minimization and maximization.</li><li>Primal and dual problems switch variables and constraints.</li></ol></p></div><p>The optimality conditions for constrained optimization take a more complex form.</p><div class = "theorem-body">
<header class = "theorem-header">Theorem: Karush-Kuhn-Tucker conditions</header><p><p>Let <span>$f$</span>, <span>$g_i$</span> and <span>$h_j$</span> be differentiable function and let a constraint qualification hold. If <span>$x$</span> is a local minimum of the primal problem (P), then there are <span>$\lambda\ge 0$</span> and <span>$\mu$</span> such that</p><p class="math-container">\[    \begin{aligned}
    &amp;\text{Optimality:} &amp;&amp; \nabla_x L(x;\lambda,\mu) = 0, \\
    &amp;\text{Feasibility:} &amp;&amp; \nabla_\lambda L(x;\lambda,\mu)\le 0,\ \nabla_\mu L(x;\lambda,\mu) = 0, \\
    &amp;\text{Complementarity:} &amp;&amp; \lambda^\top g(x) = 0.
    \end{aligned}\]</p><p>If <span>$f$</span> and <span>$g$</span> are convex and <span>$h$</span> is linear, then every stationary point is a global minimum of (P).</p></p></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../lecture_03/exercises/">« Exercises</a><a class="docs-footer-nextpage" href="../gradients/">Visualization of gradients »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 28 December 2020 16:42">Monday 28 December 2020</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
