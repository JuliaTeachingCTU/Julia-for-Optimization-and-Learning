<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Exercises · Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="Julia for Machine Learning logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="Julia for Machine Learning logo"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quick Start Guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary Functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data Structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional Evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and Iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft Local Scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of Variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception Handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard Library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other Useful Packages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composit types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic Programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Modules and packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Modules</a></li><li><a class="tocitem" href="../../lecture_06/tests/">Unit Testing</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package Management</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package Development</a></li><li><a class="tocitem" href="../../lecture_06/research/">Research project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox" checked/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../constrained/">Constrained optimization</a></li><li class="is-active"><a class="tocitem" href>Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_10/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_10/nn/">More complex networks</a></li><li><a class="tocitem" href="../../lecture_10/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Statistics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">7: Optimization</a></li><li class="is-active"><a href>Exercises</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Exercises</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_07/exercises.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="l7-exercises"><a class="docs-heading-anchor" href="#l7-exercises">Exercises</a><a id="l7-exercises-1"></a><a class="docs-heading-anchor-permalink" href="#l7-exercises" title="Permalink"></a></h1><div class = "homework-body">
<header class = "homework-header">Homework: Newton's method</header><p><p>Newton&#39;s method for solving equation <span>$g(x)=0$</span> is an iterative procedure which at every iteration <span>$x^k$</span> approximates the function <span>$g(x)$</span> by its first-order (linear) expansion <span>$g(x) \approx g(x^k) + \nabla g(x^k)(x-x^k)$</span> and finds the zero point of this approximation.</p><p>Newton&#39;s method for unconstrained optimization replaces the optimization problem by its optimality condition and solves the resulting equation.</p><p>Implement Newton&#39;s method to minimize</p><p class="math-container">\[f(x) = e^{x_1^2 + x_2^2 - 1} + (x_1-1)^2\]</p><p>with the starting point <span>$x^0=(0,0)$</span>.</p></p></div><div class = "exercise-body">
<header class = "exercise-header">Exercise 1: Bisection method</header><p><p>Similarly to Newton&#39;s method, the bisection method is primarily designed to solve equations by finding their zero points. It is only able to solve equations <span>$f(x)=0$</span> where <span>$f:\mathbb{R}\to\mathbb{R}$</span>. It starts with an interval <span>$[a,b]$</span> where <span>$f$</span> has opposite values <span>$f(a)f(b)&lt;0$</span>. Then it selects the middle point on <span>$[a,b]$</span> and halves the interval so that the new interval again satisfies the constraint on opposite signs <span>$f(a)f(b)&lt;0$</span>. This is repeated until the function value is small or until the interval has a small length.</p><p>Implement the bisection method and use it to minimize <span>$f(x) = x^2 - x$</span> on <span>$[-1,1]$</span>. During the implementation, do not evaluate <span>$f$</span> unless necessary.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>First, we write the bisection method. We initialize it with arguments <span>$f$</span> and the initial interval <span>$[a,b]$</span>. We also specify the optional tolerance. First, we save the function value <code>fa = f(a)</code> to not need to recompute it every time. The syntax <code>fa == 0 &amp;&amp; return a</code> is a bit complex. Since <code>&amp;&amp;</code> is the &quot;and&quot; operator, this first checks whether <code>fa == 0</code> is satisfied and if so, it evaluates the second part. However, the second part exits the function and returns <code>a</code>. Since we need to have <span>$f(a)f(b)&lt;0$</span>, we check this condition, and if it is not satisfied, we return an error message. Finally, we run the while loop, where every iteration halves the interval. The condition on opposite signs is enforced in the if condition inside the loop.</p><pre><code class="language-julia">function bisection(f, a, b; tol=1e-6)
    fa = f(a)
    fb = f(b)
    fa == 0 &amp;&amp; return a
    fb == 0 &amp;&amp; return b
    fa*fb &gt; 0 &amp;&amp; error(&quot;Wrong initial values for bisection&quot;)
    while b-a &gt; tol
        c = (a+b)/2
        fc = f(c)
        fc == 0 &amp;&amp; return c
        if fa*fc &gt; 0
            a = c
            fa = fc
        else
            b = c
            fb = fc
        end
    end
    return (a+b)/2
end</code></pre><p>This implementation is efficient in the way that only one function evaluation is neededper iteration. The price to pay are additional variables <code>fa</code>, <code>fb</code> and <code>fc</code>.</p><p>To use the bisection method to minimize a function <span>$f(x)$</span>, we use it find the solution of the optimality condition <span>$f&#39;(x)=0$</span>.</p><pre><code class="language-julia">f(x) = x^2 - x
g(x) = 2*x - 1
x_opt = bisection(g, -1, 1)</code></pre></p></details><p>The correct solution is</p><pre class="documenter-example-output">0.5</pre><div class = "exercise-body">
<header class = "exercise-header">Exercise 2: JuMP</header><p><p>The library to perform optimization is called <code>JuMP</code>. Install it, go briefly through its documentation, and use it to solve the linear optimization problem</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;x_1 + x_2 + x_5 \\
\text{subject to}\qquad &amp;x_1+2x_2+3x_3+4x_4+5x_5 = 8, \\
&amp;x_3+x_4+x_5 = 2, \\
&amp;x_1+x_2 = 2.
\end{aligned}\]</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The best start is the official documentation of the <a href="https://jump.dev/JuMP.jl/stable/quickstart/">JuMP package</a>. Since <code>JuMP</code> is only an interface for solvers, we need to include an actual solver as well. For linear programs, we can use <code>using GLPK</code>, for non-linear ones, we would need to use <code>using Ipopt</code>. We specify the constraints in a matrix form. It is possible to write them directly via <code>@constraint(model, x[1] + x[2] == 2)</code>. This second way is more pleasant for complex constraints. Since <code>x</code> is a vector, we need to use <code>value.(x)</code> instead of the wrong <code>value(x)</code>.</p><pre><code class="language-julia">using JuMP
using GLPK

A = [1 2 3 4 5; 0 0 1 1 1; 1 1 0 0 0]
b = [8; 2; 2]
c = [1; 1; 0; 0; 1]
n = size(A, 2)

model = Model(GLPK.Optimizer)

@variable(model, x[1:n] &gt;= 0)

@objective(model, Min, c&#39;*x)
@constraint(model, A*x .== b)
optimize!(model)

x_val = value.(x)</code></pre></p></details><p>The correct solution is</p><pre class="documenter-example-output">[2.0, 0.0, 2.0, 0.0, 0.0]</pre><div class = "exercise-body">
<header class = "exercise-header">Exercise 3: SQP method</header><p><p>Derive the SQP method for optimization problem with only equality constraints</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;f(x) \\
\text{subject to}\qquad &amp;h_j(x) = 0, j=1,\dots,J.
\end{aligned}\]</p><p>SQP writes the optimality (KKT) conditions and then applies Newton&#39;s method to solve the resulting system of equations. </p><p>Apply the obtained algorithm to</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;\sum_{i=1}^{10} ix_i^4 \\
\text{subject to}\qquad &amp;\sum_{i=1}^{10} x_i = 1.
\end{aligned}\]</p><p>Verify that the numerically obtained solution is correct.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The Lagrangian reads</p><p class="math-container">\[L(x,\mu) = f(x) + \sum_{j=1}^J\mu_j h_j(x).\]</p><p>Since there are no inequality constraints, the optimality conditions contain no complementarity and read</p><p class="math-container">\[\begin{aligned}
\nabla f(x) + \sum_{j=1}^J\mu_j \nabla h_j(x) &amp;= 0,\\
h_j(x) &amp;= 0,
\end{aligned}\]</p><p>The Newton method&#39;s at iteration <span>$k$</span> has some pair <span>$(x^k,\mu^k)$</span> and performs the update</p><p class="math-container">\[\begin{pmatrix} x^{k+1} \\ \mu^{k+1} \end{pmatrix} = \begin{pmatrix} x^{k} \\ \mu^{k} \end{pmatrix} - \begin{pmatrix} \nabla^2 f(x^k) + \sum_{j=1}^J \mu_j^k \nabla^2 h_j(x^k) &amp; \nabla h(x^k) \\ \nabla h(x^k)^\top &amp; 0 \end{pmatrix}^{-1} \begin{pmatrix} \nabla f(x^k) + \sum_{j=1}^J\mu_j^k \nabla h_j(x^k) \\ h(x^k) \end{pmatrix}. \]</p><p>We define functions <span>$f$</span> and <span>$h$</span> and their derivates and Hessians for the numerical implementation. The simplest way to create a diagonal matrix is <code>Diagonal</code> from the <code>LinearAlgebra</code> package. It can be, of course, done manually as well. </p><pre><code class="language-julia">using LinearAlgebra

n = 10
f(x) = sum((1:n) .* x.^4)
f_grad(x) = 4*(1:n)[:].*x.^3
f_hess(x) = 12*Diagonal((1:n)[:].*x.^2)
h(x) = sum(x) - 1
h_grad(x) = ones(n)
h_hess(x) = zeros(n,n)</code></pre><p>To implement SQP, we first randomly generate initial <span>$x$</span> and <span>$\mu$</span> and then write the procedure derived above. Since we update <span>$x$</span> in a for loop, we need to define it as a <code>global</code> variables; otherwise, it will be a local variable, and the global (outside of the loop) will not update. We can write <code>inv(A)*b</code> or the more efficient <code>A\b</code>. To subtract from <span>$x$</span>, we use the shortened notation <code>x -= ?</code>, which is the same as <code>x = x - ?</code>.</p><pre><code class="language-julia">x = randn(n)
μ = randn()
for i in 1:100
    global x, μ
    A = [f_hess(x) + μ*h_hess(x) h_grad(x); h_grad(x)&#39; 0]
    b = [f_grad(x) + μ*h_grad(x); h(x)]
    step = A \ b
    x -= step[1:n]
    μ -= step[n+1]
end</code></pre><p>The need to differentiate global and local variables in scripts are one of the reasons why functions should be used as much as possible.</p><p>To validate, we need to verify the optimality and the feasibility; both need to equal to zero. These are the same as the <code>b</code> variable. However, we cannot call <code>b</code> directly, as it is inside the for loop and therefore local only.</p><pre><code class="language-julia-repl">julia&gt; f_grad(x) + μ*h_grad(x)
10-element Array{Float64,1}:
 3.469446951953614e-18
 3.469446951953614e-18
 0.0
 0.0
 0.0
 6.938893903907228e-18
 3.469446951953614e-18
 3.469446951953614e-18
 0.0
 3.469446951953614e-18

julia&gt; h(x)
0.0</code></pre></p></details><p>The correct solution is</p><pre class="documenter-example-output">[0.1608, 0.1276, 0.1115, 0.1013, 0.094, 0.0885, 0.084, 0.0804, 0.0773, 0.0746]</pre><div class = "exercise-body">
<header class = "exercise-header">Exercise 4 (theory)</header><p><p>Show that the primal formulation for a problem with no inequalities is equivalent to the min-max formulation.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The primal problem with no inequalities reads</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;f(x) \\
\text{subject to}\qquad &amp;h_j(x) = 0,\ j=1,\dots,J.
\end{aligned}\]</p><p>The Lagrangian has form</p><p class="math-container">\[L(x;\lambda,\mu) = f(x) + \sum_{j=1}^J \mu_j h_j(x).\]</p><p>Now consider the min-max formulation</p><p class="math-container">\[\operatorname*{minimize}_x\quad \operatorname*{maximize}_{\mu}\quad f(x) + \sum_{j=1}^J \mu_j h_j(x).\]</p><p>If <span>$h_j(x)\neq 0$</span>, then it is simple to choose <span>$\mu_j$</span>so that the inner maximization problem has the optimal value <span>$+\infty$</span>. However, since the outer problem minimizes the objective, the value of <span>$+\infty$</span> is irrelevant. Therefore, we can ignore all points with <span>$h_j(x)\neq 0$</span> and prescribe <span>$h_j(x)=0$</span> as a hard constraint. That is precisely the primal formulation.</p></p></details><div class = "exercise-body">
<header class = "exercise-header">Exercise 5 (theory)</header><p><p>Derive the dual formulation for the linear programming.</p></p></div>
<details class = "solution-body">
<summary class = "solution-header">Solution:</summary><p><p>The linear program</p><p class="math-container">\[\begin{aligned}
\text{minimize}\qquad &amp;c^\top x \\
\text{subject to}\qquad &amp;Ax=b, \\
&amp;x\ge 0
\end{aligned}\]</p><p>has the Lagrangian</p><p class="math-container">\[L(x;\lambda,\mu) = c^\top x - \lambda^\top x + \mu^\top (b-Ax) = (c - \lambda - A^\top\mu)^\top x + b^\top \mu.\]</p><p>We need to have <span>$- \lambda^\top x$</span> because we require constraints <span>$g(x)\le 0$</span> or in other words <span>$-x\le 0$</span>. The dual problem from its definition reads</p><p class="math-container">\[\operatorname*{maximize}_{\lambda\ge0, \mu} \quad \operatorname*{minimize}_x \quad (c - \lambda - A^\top\mu)^\top x + b^\top \mu.\]</p><p>Since the minimization with respect to <span>$x$</span> is unconstrained, the same arguments as the previous exercise imply the hard constraint <span>$c - \lambda - A^\top\mu=0$</span>. Then we may simplify the dual problem into</p><p class="math-container">\[\begin{aligned}
\text{maximize}\qquad &amp;b^\top \mu \\
\text{subject to}\qquad &amp;c - \lambda - A^\top\mu = 0, \\
&amp;\lambda\ge 0.
\end{aligned}\]</p><p>From this formulation, we may remove <span>$\lambda$</span> and obtain <span>$A^\top \mu\le c$</span>. This is the desired dual formulation.</p></p></details></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../constrained/">« Constrained optimization</a><a class="docs-footer-nextpage" href="../../lecture_08/theory/">Theory of regression and classification »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 23 February 2021 13:21">Tuesday 23 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
