<!DOCTYPE html><HTML lang="en"><head><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Numerical methods · Numerical computing in Julia</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><script data-outdated-warner="">function maybeAddWarning () {
    const head = document.getElementsByTagName('head')[0];

    // Add a noindex meta tag (unless one exists) so that search engines don't index this version of the docs.
    if (document.body.querySelector('meta[name="robots"]') === null) {
        const meta = document.createElement('meta');
        meta.name = 'robots';
        meta.content = 'noindex';

        head.appendChild(meta);
    };

    // Add a stylesheet to avoid inline styling
    const style = document.createElement('style');
    style.type = 'text/css';
    style.appendChild(document.createTextNode('.outdated-warning-overlay {  position: fixed;  top: 0;  left: 0;  right: 0;  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);  z-index: 999;  background-color: #ffaba7;  color: rgba(0, 0, 0, 0.7);  border-bottom: 3px solid #da0b00;  padding: 10px 35px;  text-align: center;  font-size: 15px; }  .outdated-warning-overlay .outdated-warning-closer {    position: absolute;    top: calc(50% - 10px);    right: 18px;    cursor: pointer;    width: 12px; }  .outdated-warning-overlay a {    color: #2e63b8; }    .outdated-warning-overlay a:hover {      color: #363636; }'));
    head.appendChild(style);

    const div = document.createElement('div');
    div.classList.add('outdated-warning-overlay');
    const closer = document.createElement('div');
    closer.classList.add('outdated-warning-closer');

    // Icon by font-awesome (license: https://fontawesome.com/license, link: https://fontawesome.com/icons/times?style=solid)
    closer.innerHTML = '<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>';
    closer.addEventListener('click', function () {
        document.body.removeChild(div);
    });
    let href = '/stable';
    if (window.documenterBaseURL) {
        href = window.documenterBaseURL + '/../stable';
    }
    div.innerHTML = 'This documentation is not for the latest version. <br> <a href="' + href + '">Go to the latest documentation</a>.';
    div.appendChild(closer);
    document.body.appendChild(div);
};

if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', maybeAddWarning);
} else {
    maybeAddWarning();
};
</script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="Numerical computing in Julia logo" class="docs-light-only" src="../../assets/logo.svg"/><img alt="Numerical computing in Julia logo" class="docs-dark-only" src="../../assets/logo-dark.svg"/></a><div class="docs-package-name"><span class="docs-autofit">Numerical computing in Julia</span></div><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why_julia/">Why Julia?</a></li><li><a class="tocitem" href="../../howto/">How to...</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Mathematical operations and Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/control_flow/">Iteration</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and multiple-dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">4: Composite types and constructors</span></li><li><span class="tocitem">5: Modules and enviroments</span></li><li><span class="tocitem">6: Useful packages</span></li><li><input checked="" class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of optimization</a></li><li><a class="tocitem" href="../gradients/">Visualization of gradients</a></li><li class="is-active"><a class="tocitem" href="">Numerical methods</a><ul class="internal"><li><a class="tocitem" href="#Gradient-descent"><span>Gradient descent</span></a></li><li><a class="tocitem" href="#Adaptive-stepsize"><span>Adaptive stepsize</span></a></li></ul></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Theory of regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><span class="tocitem">9: Neural networks I.</span></li><li><span class="tocitem">10: Neural networks II.</span></li><li><span class="tocitem">11: Ordinary differential equations</span></li><li><span class="tocitem">12: Statistics I.</span></li><li><span class="tocitem">13: Statistics II.</span></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">7: Optimization</a></li><li class="is-active"><a href="">Numerical methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Numerical methods</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_07/numerical_methods.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="Numerical-methods"><a class="docs-heading-anchor" href="#Numerical-methods">Numerical methods</a><a id="Numerical-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-methods" title="Permalink"></a></h1><p>This part introduces the most basic optimization algorithm called gradient (or steepest) descent.</p><h2 id="Gradient-descent"><a class="docs-heading-anchor" href="#Gradient-descent">Gradient descent</a><a id="Gradient-descent-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-descent" title="Permalink"></a></h2><p>We learnt that the gradient is the direction of the steepest descent. The straightforward idea is to move in the opposite direction. This gives rise to the gradient descent algorithm</p><p class="math-container">\[x^{k+1} = x^k - \alpha^k\nabla f(x^k).\]</p><p>The stepsize <span>$\alpha^k&gt;0$</span> can be tuned as a hyperparameter.</p><div class="exercise-body"><header class="exercise-header">Gradient descent</header><p></p><p>Implement function <code>optim</code> which takes function <span>$f$</span>, its gradient <code>g</code>, starting point <span>$x^0$</span> and fixed stepsize <span>$\alpha$</span> and runs the gradient descent. It should return first 100 iterations of the algorithm.</p><p>This example is rather artificial because often only the last iteration is returned and some stopping criterion is employed instead of the fixed number of iterations.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>First we need to create an empty array into which we store the iterates. Then at every iteration we compute the gradient <code>g(x)</code>, perform the update and save the new value of <span>$x$</span>. </p><pre><code class="language-julia">function optim(f, g, x, α; max_iter=100)
    xs = zeros(length(x), max_iter+1)
    xs[:,1] = x
    for i in 1:max_iter
        x -= α*g(x)
        xs[:,i+1] = x
    end
    return xs
end</code></pre><p></p></details><p>Note that the implementation does not use the values of <span>$f$</span> but only of the gradient <span>$\nabla f$</span>. Moreover, if the algorithm converges <span>$x^k \to \bar x$</span>, then passing the the limit in the gradient update results in <span>$\nabla f(\bar x)=0$</span>. Therefore, as most optimization methods, gradient descent looks for stationary points.</p><div class="exercise-body"><header class="exercise-header">Gradient descent</header><p></p><p>Use the implementation of the gradient descent to minimize the function</p><p class="math-container">\[f(x) = \sin(x_1 + x_2) + \cos(x_1)^2\]</p><p>from the starting point <span>$x^0=(0,-1)$</span> and constant stepsize <span>$\alpha=0.1$</span>. Store all iterations into variable <code>xs</code>.</p><p>Plot again the contours pf <span>$f$</span> and all iterations <code>xs</code>.</p><p>Use one line of code to evaluate the function values for all iterations <code>xs</code> (hint: you need to iterate via <code>eachcol(xs)</code> or <code>eachrow(xs)</code> depending on how you repserent <code>xs</code>). Plot these values. </p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>We call <code>optim</code> written in the previous exercise. Then we plot the contours as before. Since <code>x_gd[1,:]</code> stores the <span>$x$</span> coordinate of all iterations and similarly <code>x_gd[2,:]</code>, we plot them. Again, we need to use <code>plot!</code> instead of <code>plot</code> to add the line to the contour plot.</p><pre><code class="language-julia">x_gd = optim([], g, [0; -1], 0.1)

contourf(xs, ys, f_mod, color = :jet)

plot!(x_gd[1,:], x_gd[2,:], line=(4,:black), label = "")</code></pre><pre class="documenter-example-output">/home/runner/.julia/packages/GR/RlE5Y/src/../deps/gr/bin/gksqt: error while loading shared libraries: libQt5Widgets.so.5: cannot open shared object file: No such file or directory
connect: Connection refused
GKS: can't connect to GKS socket application

GKS: Open failed in routine OPEN_WS
GKS: GKS not in proper state. GKS must be either in the state WSOP or WSAC in routine ACTIVATE_WS</pre><p>This example is similar to <code>[? for h in hs]</code> encountered earlier. To iterate over all columns, we use <code>[? for x in eachcol(x_gd)]</code> and apply <code>f(x)</code> instead of <code>?</code>. Another way is to iterate over indices instead of vectors and write <code>[f(x_gs[:,i]) for i in 1:size(x_gd,2)]</code>.</p><pre><code class="language-julia">f_gd = [f(x) for x in eachcol(x_gd)]

plot(f_gd, label="", xlabel="Iteration", ylabel="Function value")</code></pre><pre class="documenter-example-output">/home/runner/.julia/packages/GR/RlE5Y/src/../deps/gr/bin/gksqt: error while loading shared libraries: libQt5Widgets.so.5: cannot open shared object file: No such file or directory
connect: Connection refused
GKS: can't connect to GKS socket application

GKS: Open failed in routine OPEN_WS
GKS: GKS not in proper state. GKS must be either in the state WSOP or WSAC in routine ACTIVATE_WS</pre><p></p></details><p><img alt="" src="../numer1.svg"/></p><p><img alt="" src="../numer2.svg"/></p><p>The convergence looks very nice and the function value decreases. First, the decrease is faster but when the iterations get closer to the minimum, it slows down.</p><p>What happens if we choose a different stepsize though? Let us try with two different values. First let us try <span>$\alpha=0.01$</span>.</p><pre><code class="language-julia">x_gd = optim([], g, [0; -1], 0.01)

contourf(xs, ys, f_mod, color = :jet)

plot!(x_gd[1,:], x_gd[2,:], line=(4,:black), label = "")</code></pre><pre class="documenter-example-output">/home/runner/.julia/packages/GR/RlE5Y/src/../deps/gr/bin/gksqt: error while loading shared libraries: libQt5Widgets.so.5: cannot open shared object file: No such file or directory
connect: Connection refused
GKS: can't connect to GKS socket application

GKS: Open failed in routine OPEN_WS
GKS: GKS not in proper state. GKS must be either in the state WSOP or WSAC in routine ACTIVATE_WS</pre><p><img alt="" src="../numer3.svg"/></p><p>We see that when the stepsize is reduced, the steps are shorter and we would need to increase the number of iterations (and thus time) to converge. When the stepsize is larger, say <span>$\alpha=1$</span>, the situation is different. </p><pre><code class="language-julia">x_gd = optim([], g, [0; -1], 1)

contourf(xs, ys, f_mod, color = :jet)

plot!(x_gd[1,:], x_gd[2,:], line=(4,:black), label = "")</code></pre><pre class="documenter-example-output">/home/runner/.julia/packages/GR/RlE5Y/src/../deps/gr/bin/gksqt: error while loading shared libraries: libQt5Widgets.so.5: cannot open shared object file: No such file or directory
connect: Connection refused
GKS: can't connect to GKS socket application

GKS: Open failed in routine OPEN_WS
GKS: GKS not in proper state. GKS must be either in the state WSOP or WSAC in routine ACTIVATE_WS</pre><p><img alt="" src="../numer4.svg"/></p><p>For a large stepsize, the algorithm gets close to the solution and then starts jumping around. If you further increase the stepsize, it will even diverge to infinite. Try it :)</p><h2 id="Adaptive-stepsize"><a class="docs-heading-anchor" href="#Adaptive-stepsize">Adaptive stepsize</a><a id="Adaptive-stepsize-1"></a><a class="docs-heading-anchor-permalink" href="#Adaptive-stepsize" title="Permalink"></a></h2><p>To handle this numerical instability, safeguards were introduced. One of the possibilities is the Armijo condition which automatically select the stepsize. It looks for <span>$\alpha^k$</span> which satisfies</p><p class="math-container">\[f(x^k - \alpha^k\nabla f(x^k)) \le f(x^k) - c \alpha^k \|\nabla f(x^k)\|^2.\]</p><p>Here  <span>$c\in(0,1)$</span> is a small contant, usually <span>$c=10^{-4}$</span>. Since the left-hand side is the function value at the new iterate <span>$x^{k+1}$</span>, the Armijo condition ensures that the sequence of function values is strictly decreasing. This prevents oscillations.</p><p>The implementation of <code>optim(f, g, x, α; max_iter=100)</code> from the exercise above is rather stupid because it does not allow to modify the selection of the step. The simplest solution to this problem would be to include if conditions inside the function. However, this would result in a long function, which may be difficult to debug and modify. More elegant solution is to create an abstract class</p><pre><code class="language-julia">abstract type Step end</code></pre><p>and for each possible step selection method implement a <code>optim_step</code> method, which selects the step. First, we create the gradeint descent class <code>GD</code> as a subclass of <code>Step</code> by</p><pre><code class="language-julia">struct GD &lt;: Step
    α::Real
end</code></pre><p>It is a structure with parameter <code>α</code>. Then we create the <code>optim_step</code> function by</p><pre><code class="language-julia">optim_step(s::GD, f, g, x) = -s.α*g(x)</code></pre><p>Due to the first input argument, it will be called only for the  <code>GD</code> stepsize. To access the parameter <code>α</code>, we need to retrieve it from the structure by <code>s.α</code>. Now we can modify the <code>optim</code> function by</p><pre><code class="language-julia">function optim(f, g, x, s::Step; max_iter=100)
    for i in 1:max_iter
        x += optim_step(s, f, g, x)
    end
    return x
end</code></pre><p>Note that the input is <code>s::Step</code> which allows for any subclass of the abstract class <code>Step</code>. Using this implentation results in</p><pre><code class="language-julia">gd = GD(0.1)
x_opt = optim(f, g, [-1;0], gd)</code></pre><pre class="documenter-example-output">[-1.570582871855513, -0.0005153258007101915]</pre><p>We obtained the same results as in the previous case. This is not surprising as the code does exactly the same things; it is only written differently. The next exercise shows the power of defining the <code>Step</code> class.</p><div class="exercise-body"><header class="exercise-header">Armijo condition</header><p></p><p>Implement the <code>Armijo</code> subclass of the <code>Step</code> class. It should have two parameters <code>c</code> from the definition and <code>α_max</code> which will be the initial value of <span>$\alpha$</span>. The value <span>$\alpha$</span> should be divided by two until the Armijo condition is satisfied.</p><p>Then run the optimization with the Armijo selection of the stepsize.</p><p></p></div><details class="solution-body"><summary class="solution-header">Solution:</summary><p></p><p>We define the class in the same way as with <code>GD</code></p><pre><code class="language-julia">struct Armijo &lt;: Step
    c::Real
    α_max::Real
end</code></pre><p>For the search for the stepsize, we first save the values for the function value <span>$f(x)$</span> and the gradient <span>$\nabla f(x)$</span>. If we do not do this, it will be recomputed at every step. Then we initialize the value of <span>$\alpha$</span> and run the while loop until the Armijo condition is satisfied. We added a termination condition (also a safe check) <code>α &lt;= 1e-6</code> to prevent the loop for continuing indefinitely.</p><pre><code class="language-julia">function optim_step(s::Armijo, f, g, x)
    fun = f(x)
    grad = g(x)
    α = s.α_max
    while f(x .- α*grad) &gt; fun - s.c*α*(grad'*grad)
        α /= 2
        if α &lt;= 1e-6
            warning("Armijo line search failed.")
            break
        end
    end
    return -α*grad
end</code></pre><p>Then we create the <code>Armijo</code> object and run the optimization again.</p><pre><code class="language-julia">gd = Armijo(1e-4, 1)
x_opt = optim(f, g, [-1;0], gd)</code></pre><p></p></details><p>The correct solution is</p><pre class="documenter-example-output">[-1.5708, 0.0]</pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../gradients/">« Visualization of gradients</a><a class="docs-footer-nextpage" href="../exercises/">Exercises »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 28 December 2020 16:58">Monday 28 December 2020</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>