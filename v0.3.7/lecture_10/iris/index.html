<!DOCTYPE html><HTML lang="en"><head><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Introduction to Flux Â· Julia for Machine Learning</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><script data-outdated-warner="">function maybeAddWarning () {
    const head = document.getElementsByTagName('head')[0];

    // Add a noindex meta tag (unless one exists) so that search engines don't index this version of the docs.
    if (document.body.querySelector('meta[name="robots"]') === null) {
        const meta = document.createElement('meta');
        meta.name = 'robots';
        meta.content = 'noindex';

        head.appendChild(meta);
    };

    // Add a stylesheet to avoid inline styling
    const style = document.createElement('style');
    style.type = 'text/css';
    style.appendChild(document.createTextNode('.outdated-warning-overlay {  position: fixed;  top: 0;  left: 0;  right: 0;  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);  z-index: 999;  background-color: #ffaba7;  color: rgba(0, 0, 0, 0.7);  border-bottom: 3px solid #da0b00;  padding: 10px 35px;  text-align: center;  font-size: 15px; }  .outdated-warning-overlay .outdated-warning-closer {    position: absolute;    top: calc(50% - 10px);    right: 18px;    cursor: pointer;    width: 12px; }  .outdated-warning-overlay a {    color: #2e63b8; }    .outdated-warning-overlay a:hover {      color: #363636; }'));
    head.appendChild(style);

    const div = document.createElement('div');
    div.classList.add('outdated-warning-overlay');
    const closer = document.createElement('div');
    closer.classList.add('outdated-warning-closer');

    // Icon by font-awesome (license: https://fontawesome.com/license, link: https://fontawesome.com/icons/times?style=solid)
    closer.innerHTML = '<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>';
    closer.addEventListener('click', function () {
        document.body.removeChild(div);
    });
    let href = '/stable';
    if (window.documenterBaseURL) {
        href = window.documenterBaseURL + '/../stable';
    }
    div.innerHTML = 'This documentation is not for the latest version. <br> <a href="' + href + '">Go to the latest documentation</a>.';
    div.appendChild(closer);
    document.body.appendChild(div);
};

if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', maybeAddWarning);
} else {
    maybeAddWarning();
};
</script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="Julia for Machine Learning logo" class="docs-light-only" src="../../assets/logo.svg"/><img alt="Julia for Machine Learning logo" class="docs-dark-only" src="../../assets/logo-dark.svg"/></a><div class="docs-package-name"><span class="docs-autofit">Julia for Machine Learning</span></div><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../why/">Why Julia?</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Installation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../installation/julia/">Julia</a></li><li><a class="tocitem" href="../../installation/vscode/">Visual Studio Code</a></li><li><a class="tocitem" href="../../installation/git/">Git</a></li><li><a class="tocitem" href="../../installation/tutorial/">Quickstart guide</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">1: Variables and basic operators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_01/variables/">Variables</a></li><li><a class="tocitem" href="../../lecture_01/operators/">Elementary functions</a></li><li><a class="tocitem" href="../../lecture_01/strings/">Strings</a></li><li><a class="tocitem" href="../../lecture_01/arrays/">Arrays</a></li><li><a class="tocitem" href="../../lecture_01/data_structures/">Data structures</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">2: Control flow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_02/conditions/">Conditional evaluations</a></li><li><a class="tocitem" href="../../lecture_02/loops/">Loops and iterators</a></li><li><a class="tocitem" href="../../lecture_02/scope/">Soft local scope</a></li><li><a class="tocitem" href="../../lecture_02/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">3: Functions and methods</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_03/functions/">Functions</a></li><li><a class="tocitem" href="../../lecture_03/methods/">Methods</a></li><li><a class="tocitem" href="../../lecture_03/scope/">Scope of variables</a></li><li><a class="tocitem" href="../../lecture_03/exceptions/">Exception handling</a></li><li><a class="tocitem" href="../../lecture_03/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">4: Packages</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_04/standardlibrary/">Standard library</a></li><li><a class="tocitem" href="../../lecture_04/Plots/">Plots.jl</a></li><li><a class="tocitem" href="../../lecture_04/DataFrames/">DataFrames.jl</a></li><li><a class="tocitem" href="../../lecture_04/otherpackages/">Other useful packages</a></li><li><a class="tocitem" href="../../lecture_04/interaction/">Interaction with other languages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">5: Type system and generic programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_05/compositetypes/">Abstract and composite types</a></li><li><a class="tocitem" href="../../lecture_05/currencies/">Generic programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">6: Code organization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_06/modules/">Files and modules</a></li><li><a class="tocitem" href="../../lecture_06/pkg/">Package manager</a></li><li><a class="tocitem" href="../../lecture_06/develop/">Package development</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Course requirements</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../final_project/homeworks/">Homework</a></li><li><a class="tocitem" href="../../final_project/project/">Final project</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">7: Optimization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_07/theory/">Introduction to continuous optimization</a></li><li><a class="tocitem" href="../../lecture_07/unconstrained/">Unconstrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/constrained/">Constrained optimization</a></li><li><a class="tocitem" href="../../lecture_07/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">8: Regression and classification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_08/theory/">Introduction to regression and classification</a></li><li><a class="tocitem" href="../../lecture_08/linear/">Linear regression</a></li><li><a class="tocitem" href="../../lecture_08/logistic/">Logistic regression</a></li><li><a class="tocitem" href="../../lecture_08/exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-13" type="checkbox"/><label class="tocitem" for="menuitem-13"><span class="docs-label">9: Neural networks I.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_09/theory/">Theory of neural networks</a></li><li><a class="tocitem" href="../../lecture_09/nn/">Neural networks</a></li><li><a class="tocitem" href="../../lecture_09/exercises/">Exercises</a></li></ul></li><li><input checked="" class="collapse-toggle" id="menuitem-14" type="checkbox"/><label class="tocitem" for="menuitem-14"><span class="docs-label">10: Neural networks II.</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../theory/">Theory of neural networks</a></li><li class="is-active"><a class="tocitem" href="">Introduction to Flux</a><ul class="internal"><li><a class="tocitem" href="#Creating-the-network"><span>Creating the network</span></a></li><li><a class="tocitem" href="#Training-the-network"><span>Training the network</span></a></li></ul></li><li><a class="tocitem" href="../nn/">More complex networks</a></li><li><a class="tocitem" href="../exercises/">Exercises</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-15" type="checkbox"/><label class="tocitem" for="menuitem-15"><span class="docs-label">11: Statistics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_11/theory/">Statistics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-16" type="checkbox"/><label class="tocitem" for="menuitem-16"><span class="docs-label">12: Ordinary differential equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../lecture_12/theory/">Differential equations</a></li><li><a class="tocitem" href="../../lecture_12/ode/">Wave equation</a></li><li><a class="tocitem" href="../../lecture_12/diff_eq/">Julia package</a></li><li><a class="tocitem" href="../../lecture_12/optimal_control/">Optimal control</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">10: Neural networks II.</a></li><li class="is-active"><a href="">Introduction to Flux</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Introduction to Flux</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/VaclavMacha/JuliaCourse/blob/master/docs/src/lecture_10/iris.md" title="Edit on GitHub"><span class="docs-icon fab">ï</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="Introduction-to-Flux"><a class="docs-heading-anchor" href="#Introduction-to-Flux">Introduction to Flux</a><a id="Introduction-to-Flux-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction-to-Flux" title="Permalink"></a></h1><p>Flux is a library for using neural networks. This part will present the basics of Flux on the Iris dataset from the previous lecture. We include the auxiliary functions from the previous lesson into the <code>utilities.jl</code> file, which we include by</p><pre><code class="language-julia">include("utilities.jl")</code></pre><p>We set the seed and load the data in the same way as during the last lecture.</p><pre><code class="language-julia">using RDatasets
using Random

Random.seed!(666)

iris = dataset("datasets", "iris")

X = Matrix(iris[:, 1:4])
y = iris.Species

X_train, y_train, X_test, y_test, classes = prepare_data(X', y; dims=2)</code></pre><h2 id="Creating-the-network"><a class="docs-heading-anchor" href="#Creating-the-network">Creating the network</a><a id="Creating-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-the-network" title="Permalink"></a></h2><p>We recall that machine learning minimizes the discrepancy between the predictions <span>$\operatorname{predict}(w; x_i)$</span> and labels <span>$y_i$</span>. Mathematically, this amount to minimizing the following objective function.  </p><p class="math-container">\[L(w) = \frac1n\sum_{i=1}^n \operatorname{loss}(y_i, \operatorname{predict}(w; x_i)).\]</p><p>To build the objective <span>$L$</span>, we first specify the prediction function <span>$\operatorname{predict}$</span>, which we denote by model <code>m</code>.  We start by creating the same network by the function <code>Chain</code>. Its inputs are the individual layers. Dense layers are created by <code>Dense</code> with the correct number of input and output neurons. We also need to specify the activation functions.</p><pre><code class="language-julia">using Flux

n_hidden = 5
m = Chain(
    Dense(size(X_train,1), n_hidden, relu),
    Dense(n_hidden, size(y_train,1), identity),
    softmax,
)</code></pre><p>Since <code>identity</code> is the default argument, it is possible to remove it in the second layer. However, we recommend keeping it for clarity.</p><p>We can evaluate the whole dataset.</p><pre><code class="language-julia">m(X_train)</code></pre><pre class="documenter-example-output">3Ã120 Array{Float32,2}:
 0.303607  0.309745  0.367206  0.137385  â¦  0.0179704  0.3824    0.318091
 0.467098  0.25595   0.408589  0.170671     0.163688   0.372205  0.350239
 0.229295  0.434306  0.224205  0.691944     0.818342   0.245395  0.33167</pre><p>Because there are <span>$3$</span> classes and <span>$120$</span> samples in the training set, it returns an array of size <span>$3\times 120$</span>. Each column corresponds to one sample and forms a vector of probabilities due to the last layer of softmax.</p><p>We access the neural network parameters by using <code>params(m)</code>. We can select the second layer of <code>m</code> by <code>m[2]</code>. Since the second layer has <span>$5$</span> input and <span>$3$</span> output neurons, its parameters are a matrix of size <span>$3\times 5$</span> and a vector of length <span>$3$</span>. The parameters <code>params(m[2])</code> are a tuple of the matrix and the vector. This also implies that the parameters are initialized randomly, and we do not need to take care of it. We can easily modify any parameters.</p><pre><code class="language-julia">params(m[2])[2] .= [-1;0;1]</code></pre><h2 id="Training-the-network"><a class="docs-heading-anchor" href="#Training-the-network">Training the network</a><a id="Training-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-network" title="Permalink"></a></h2><p>To train the network, we need to define the objective function <span>$L$</span>. Since we already defined <span>$\operatorname{predict}$</span>, it suffices to define the loss function <span>$\operatorname{loss}$</span>. Since we work with a multi-class problem, the loss function is usually the cross-entropy.</p><pre><code class="language-julia">using Flux: crossentropy

L(x,y) = crossentropy(m(x), y)</code></pre><p>The <code>loss</code> function does not have <code>m</code> as input. Even though there could be an additional input parameter, it is customary to write it without it. We can evaluate the objective function by</p><pre><code class="language-julia">L(X_train, y_train)</code></pre><pre class="documenter-example-output">1.4863017f0</pre><p>This computes the objective function on the whole training set. Since Flux is (unlike our implementation from the last lecture) smart, there is no need to take care of individual samples.</p><div class="info-body"><header class="info-header">Notation</header><p></p><p>While the <a href="https://en.wikipedia.org/wiki/Cross_entropy">standard definition</a> of cross-entropy is <span>$\operatorname{loss}(y,\hat y)$</span>, <a href="https://fluxml.ai/Flux.jl/stable/models/losses/">Flux</a> uses <span>$\operatorname{loss}(\hat y,y)$</span>.</p><p></p></div><p>Since we have the model and the loss function, the only remaining thing is the gradient. Flux again provides a smart way to compute it.</p><pre><code class="language-julia">ps = params(m)
grad = gradient(() -&gt; L(X_train, y_train), ps)</code></pre><p>The function <code>gradient</code> takes two inputs. The first one is the function we want to differentiate, and the second one are the parameters. The <code>L</code> function needs to be evaluated at the correct points <code>X_train</code> and <code>y_train</code>. In some applications, we may need to differentiate with respect to other parameters such as <code>X_train</code>. This can be achieved by changing the second parameters of the <code>gradient</code> function.</p><pre><code class="language-julia">grad = gradient(() -&gt; L(X_train, y_train), params(X_train))

size(grad[X_train])</code></pre><pre class="documenter-example-output">(4, 120)</pre><p>Since <code>X_train</code> has shape <span>$4\times 120$</span>, the gradient needs to have the same size.</p><p>We train the classifiers for 250 iterations. In each iteration, we compute the gradient with respect to all network parameters and perform the gradient descent with stepsize <span>$0.1$</span>.</p><pre><code class="language-julia">opt = Descent(0.1)
max_iter = 250

acc_test = zeros(max_iter)
for i in 1:max_iter
    gs = gradient(() -&gt; L(X_train, y_train), ps)
    Flux.Optimise.update!(opt, ps, gs)
    acc_test[i] = accuracy(X_test, y_test)
end</code></pre><p>The accuracy on the testing set keeps increasing as the training progresses.</p><pre><code class="language-julia">using Plots

plot(acc_test, xlabel="Iteration", ylabel="Test accuracy", label="", ylim=(-0.01,1.01))</code></pre><p><img alt="" src="../Iris_acc.svg"/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../theory/">Â« Theory of neural networks</a><a class="docs-footer-nextpage" href="../nn/">More complex networks Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 26 April 2021 22:24">Monday 26 April 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>